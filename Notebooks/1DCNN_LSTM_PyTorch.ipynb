{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import talib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.trial import TrialState\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau \n",
    "import shap\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n",
      "2.1.2+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8902\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"gpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA version:', torch.version.cuda)\n",
    "print('cuDNN version:', torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>17.869831</td>\n",
       "      <td>1.698451</td>\n",
       "      <td>2.6718</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>0.209015</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062957</td>\n",
       "      <td>2.970550</td>\n",
       "      <td>4.378340</td>\n",
       "      <td>-5.517243</td>\n",
       "      <td>-6.345335</td>\n",
       "      <td>7.308964</td>\n",
       "      <td>-1.141483</td>\n",
       "      <td>-1.381423</td>\n",
       "      <td>-1.624192</td>\n",
       "      <td>5.051769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>17.834875</td>\n",
       "      <td>3.131120</td>\n",
       "      <td>2.9872</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>1645.000000</td>\n",
       "      <td>0.258659</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120922</td>\n",
       "      <td>-13.305144</td>\n",
       "      <td>-5.027217</td>\n",
       "      <td>-22.349038</td>\n",
       "      <td>-23.061887</td>\n",
       "      <td>-10.695185</td>\n",
       "      <td>-21.535988</td>\n",
       "      <td>-8.171207</td>\n",
       "      <td>-25.425447</td>\n",
       "      <td>0.418158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>21.771714</td>\n",
       "      <td>3.087308</td>\n",
       "      <td>3.4500</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>0.249095</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087046</td>\n",
       "      <td>11.933447</td>\n",
       "      <td>9.541476</td>\n",
       "      <td>18.150742</td>\n",
       "      <td>10.076208</td>\n",
       "      <td>9.328708</td>\n",
       "      <td>15.503083</td>\n",
       "      <td>7.838985</td>\n",
       "      <td>14.100815</td>\n",
       "      <td>7.019633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>26.426070</td>\n",
       "      <td>2.380069</td>\n",
       "      <td>3.9154</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>0.153558</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191850</td>\n",
       "      <td>15.556124</td>\n",
       "      <td>4.893814</td>\n",
       "      <td>3.789965</td>\n",
       "      <td>21.538450</td>\n",
       "      <td>8.417417</td>\n",
       "      <td>10.874069</td>\n",
       "      <td>18.506872</td>\n",
       "      <td>10.358213</td>\n",
       "      <td>-2.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>27.652509</td>\n",
       "      <td>3.109565</td>\n",
       "      <td>4.2582</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>2056.000000</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484614</td>\n",
       "      <td>-2.905304</td>\n",
       "      <td>2.024644</td>\n",
       "      <td>-7.498254</td>\n",
       "      <td>-7.341768</td>\n",
       "      <td>1.063541</td>\n",
       "      <td>-4.676644</td>\n",
       "      <td>-4.741380</td>\n",
       "      <td>-4.544226</td>\n",
       "      <td>5.565071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.600140</td>\n",
       "      <td>37.3900</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>45.779944</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355782</td>\n",
       "      <td>-15.534891</td>\n",
       "      <td>-5.840908</td>\n",
       "      <td>-24.280433</td>\n",
       "      <td>-13.633068</td>\n",
       "      <td>-9.080399</td>\n",
       "      <td>-17.844385</td>\n",
       "      <td>-17.722026</td>\n",
       "      <td>-12.791300</td>\n",
       "      <td>0.512814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>52.800000</td>\n",
       "      <td>3.001754</td>\n",
       "      <td>41.5751</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>41.655270</td>\n",
       "      <td>0.300673</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178828</td>\n",
       "      <td>14.988381</td>\n",
       "      <td>10.496262</td>\n",
       "      <td>15.292074</td>\n",
       "      <td>7.934506</td>\n",
       "      <td>6.057101</td>\n",
       "      <td>16.488114</td>\n",
       "      <td>19.393354</td>\n",
       "      <td>9.857481</td>\n",
       "      <td>9.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>49.574468</td>\n",
       "      <td>4.422004</td>\n",
       "      <td>42.9480</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>44.643278</td>\n",
       "      <td>0.516266</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>4.699164</td>\n",
       "      <td>3.493137</td>\n",
       "      <td>-3.644893</td>\n",
       "      <td>7.351231</td>\n",
       "      <td>0.970027</td>\n",
       "      <td>3.185392</td>\n",
       "      <td>5.459461</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>2.509890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>51.193548</td>\n",
       "      <td>3.780823</td>\n",
       "      <td>40.7500</td>\n",
       "      <td>1.583422</td>\n",
       "      <td>44.768947</td>\n",
       "      <td>0.615809</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>1.258389</td>\n",
       "      <td>5.768897</td>\n",
       "      <td>-7.078949</td>\n",
       "      <td>1.449274</td>\n",
       "      <td>-2.709415</td>\n",
       "      <td>0.271247</td>\n",
       "      <td>3.190669</td>\n",
       "      <td>-0.512819</td>\n",
       "      <td>8.569506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>47.115385</td>\n",
       "      <td>9.213489</td>\n",
       "      <td>41.1200</td>\n",
       "      <td>1.832289</td>\n",
       "      <td>46.407565</td>\n",
       "      <td>0.484964</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063856</td>\n",
       "      <td>3.910523</td>\n",
       "      <td>2.539892</td>\n",
       "      <td>1.418919</td>\n",
       "      <td>9.928574</td>\n",
       "      <td>13.014539</td>\n",
       "      <td>4.946547</td>\n",
       "      <td>13.833353</td>\n",
       "      <td>5.532642</td>\n",
       "      <td>-0.185349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9660 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date OFTIC       PENDS       MEAN     STDEV      BPS       CPS  \\\n",
       "0      2011-06-30  AAPL  2011-06-30  17.869831  1.698451   2.6718  0.422900   \n",
       "1      2011-09-30  AAPL  2011-09-30  17.834875  3.131120   2.9872  0.397600   \n",
       "2      2011-12-31  AAPL  2011-12-31  21.771714  3.087308   3.4500  0.665700   \n",
       "3      2012-03-31  AAPL  2012-03-31  26.426070  2.380069   3.9154  0.528200   \n",
       "4      2012-06-30  AAPL  2012-06-30  27.652509  3.109565   4.2582  0.384300   \n",
       "...           ...   ...         ...        ...       ...      ...       ...   \n",
       "14055  2018-12-31  ZION  2018-12-31  53.000000  4.600140  37.3900  1.760000   \n",
       "14056  2019-03-31  ZION  2019-03-31  52.800000  3.001754  41.5751 -0.210000   \n",
       "14057  2019-06-30  ZION  2019-06-30  49.574468  4.422004  42.9480  1.380000   \n",
       "14058  2019-09-30  ZION  2019-09-30  51.193548  3.780823  40.7500  1.583422   \n",
       "14059  2019-12-31  ZION  2019-12-31  47.115385  9.213489  41.1200  1.832289   \n",
       "\n",
       "               CPX       CSH   DPS  ...  \\\n",
       "0       777.000000  0.209015  0.00  ...   \n",
       "1      1645.000000  0.258659  0.00  ...   \n",
       "2      1321.000000  0.249095  0.00  ...   \n",
       "3      1457.000000  0.153558  0.00  ...   \n",
       "4      2056.000000  0.204900  0.00  ...   \n",
       "...            ...       ...   ...  ...   \n",
       "14055    45.779944  0.504142  0.30  ...   \n",
       "14056    41.655270  0.300673  0.30  ...   \n",
       "14057    44.643278  0.516266  0.30  ...   \n",
       "14058    44.768947  0.615809  0.34  ...   \n",
       "14059    46.407565  0.484964  0.34  ...   \n",
       "\n",
       "       commodity_trade_Close_quarterly_return  \\\n",
       "0                                    0.062957   \n",
       "1                                    0.120922   \n",
       "2                                   -0.087046   \n",
       "3                                   -0.191850   \n",
       "4                                    0.484614   \n",
       "...                                       ...   \n",
       "14055                                0.355782   \n",
       "14056                               -0.178828   \n",
       "14057                                0.081598   \n",
       "14058                               -0.036101   \n",
       "14059                               -0.063856   \n",
       "\n",
       "       C_Discretionary_Close_quarterly_return  \\\n",
       "0                                    2.970550   \n",
       "1                                  -13.305144   \n",
       "2                                   11.933447   \n",
       "3                                   15.556124   \n",
       "4                                   -2.905304   \n",
       "...                                       ...   \n",
       "14055                              -15.534891   \n",
       "14056                               14.988381   \n",
       "14057                                4.699164   \n",
       "14058                                1.258389   \n",
       "14059                                3.910523   \n",
       "\n",
       "       C_Staples_Close_quarterly_return  Energy_Close_quarterly_return  \\\n",
       "0                              4.378340                      -5.517243   \n",
       "1                             -5.027217                     -22.349038   \n",
       "2                              9.541476                      18.150742   \n",
       "3                              4.893814                       3.789965   \n",
       "4                              2.024644                      -7.498254   \n",
       "...                                 ...                            ...   \n",
       "14055                         -5.840908                     -24.280433   \n",
       "14056                         10.496262                      15.292074   \n",
       "14057                          3.493137                      -3.644893   \n",
       "14058                          5.768897                      -7.078949   \n",
       "14059                          2.539892                       1.418919   \n",
       "\n",
       "       Financials_Close_quarterly_return  Health_care_Close_quarterly_return  \\\n",
       "0                              -6.345335                            7.308964   \n",
       "1                             -23.061887                          -10.695185   \n",
       "2                              10.076208                            9.328708   \n",
       "3                              21.538450                            8.417417   \n",
       "4                              -7.341768                            1.063541   \n",
       "...                                  ...                                 ...   \n",
       "14055                         -13.633068                           -9.080399   \n",
       "14056                           7.934506                            6.057101   \n",
       "14057                           7.351231                            0.970027   \n",
       "14058                           1.449274                           -2.709415   \n",
       "14059                           9.928574                           13.014539   \n",
       "\n",
       "       industrials_Close_quarterly_return  information_Close_quarterly_return  \\\n",
       "0                               -1.141483                           -1.381423   \n",
       "1                              -21.535988                           -8.171207   \n",
       "2                               15.503083                            7.838985   \n",
       "3                               10.874069                           18.506872   \n",
       "4                               -4.676644                           -4.741380   \n",
       "...                                   ...                                 ...   \n",
       "14055                          -17.844385                          -17.722026   \n",
       "14056                           16.488114                           19.393354   \n",
       "14057                            3.185392                            5.459461   \n",
       "14058                            0.271247                            3.190669   \n",
       "14059                            4.946547                           13.833353   \n",
       "\n",
       "       materials_Close_quarterly_return  utilities_Close_quarterly_return  \n",
       "0                             -1.624192                          5.051769  \n",
       "1                            -25.425447                          0.418158  \n",
       "2                             14.100815                          7.019633  \n",
       "3                             10.358213                         -2.612559  \n",
       "4                             -4.544226                          5.565071  \n",
       "...                                 ...                               ...  \n",
       "14055                        -12.791300                          0.512814  \n",
       "14056                          9.857481                          9.920635  \n",
       "14057                          5.405405                          2.509890  \n",
       "14058                         -0.512819                          8.569506  \n",
       "14059                          5.532642                         -0.185349  \n",
       "\n",
       "[9660 rows x 61 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data = pd.read_csv(\"../Data/longmerged_deneme_51.csv\")\n",
    "pivoted_data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "#pivoted_data.set_index(\"PENDS\", inplace=True)\n",
    "len(pivoted_data[\"OFTIC\"].unique())\n",
    "# pivoted_data[pivoted_data[\"OFTIC\"] == \"AAPL\"]\n",
    "pivoted_data = pivoted_data[pivoted_data['PENDS'] > '2011-03-31']\n",
    "pivoted_data = pivoted_data[pivoted_data['PENDS'] < '2020-01-01']\n",
    "\n",
    "pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sector\n",
       "Communication Services     5\n",
       "Consumer Discretionary    32\n",
       "Consumer Staples          20\n",
       "Energy                    14\n",
       "Financials                38\n",
       "Health Care               38\n",
       "Industrials               36\n",
       "Information Technology    34\n",
       "Materials                 19\n",
       "Real Estate               20\n",
       "Utilities                 20\n",
       "Name: OFTIC, dtype: int64"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = pivoted_data.groupby('Sector')['OFTIC'].nunique()\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'OFTIC', 'PENDS', 'MEAN', 'STDEV', 'BPS', 'CPS', 'CPX', 'CSH',\n",
       "       'DPS', 'EBG', 'EBI', 'EBS', 'EBT', 'ENT', 'EPS', 'FFO', 'GPS', 'GRM',\n",
       "       'NAV', 'NDT', 'NET', 'OPR', 'PRE', 'ROA', 'ROE', 'SAL',\n",
       "       'Real_Estate_Index_Price', 'VIX_Close', 'Gold_Close',\n",
       "       'Three_Month_Yield', 'Brent_Close', 'Hrc_close',\n",
       "       'commodity_trade_Close', 'C_Discretionary_Close', 'C_Staples_Close',\n",
       "       'Energy_Close', 'Financials_Close', 'Health_care_Close',\n",
       "       'industrials_Close', 'information_Close', 'materials_Close',\n",
       "       'utilities_Close', 'Sector', 'numeric_sector',\n",
       "       'Real_Estate_Index_Price_quarterly_return',\n",
       "       'VIX_Close_quarterly_return', 'Gold_Close_quarterly_return',\n",
       "       'Three_Month_Yield_quarterly_return', 'Brent_Close_quarterly_return',\n",
       "       'Hrc_close_quarterly_return', 'commodity_trade_Close_quarterly_return',\n",
       "       'C_Discretionary_Close_quarterly_return',\n",
       "       'C_Staples_Close_quarterly_return', 'Energy_Close_quarterly_return',\n",
       "       'Financials_Close_quarterly_return',\n",
       "       'Health_care_Close_quarterly_return',\n",
       "       'industrials_Close_quarterly_return',\n",
       "       'information_Close_quarterly_return',\n",
       "       'materials_Close_quarterly_return', 'utilities_Close_quarterly_return'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = pivoted_data[~pivoted_data[\"OFTIC\"].isin([\"DFS\", \"ICE\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>27.546538</td>\n",
       "      <td>3.568238</td>\n",
       "      <td>12.8250</td>\n",
       "      <td>1.587282</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062957</td>\n",
       "      <td>2.970550</td>\n",
       "      <td>4.378340</td>\n",
       "      <td>-5.517243</td>\n",
       "      <td>-6.345335</td>\n",
       "      <td>7.308964</td>\n",
       "      <td>-1.141483</td>\n",
       "      <td>-1.381423</td>\n",
       "      <td>-1.624192</td>\n",
       "      <td>5.051769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>26.911765</td>\n",
       "      <td>2.682322</td>\n",
       "      <td>13.6250</td>\n",
       "      <td>1.763514</td>\n",
       "      <td>0.324273</td>\n",
       "      <td>0.310576</td>\n",
       "      <td>0.150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120922</td>\n",
       "      <td>-13.305144</td>\n",
       "      <td>-5.027217</td>\n",
       "      <td>-22.349038</td>\n",
       "      <td>-23.061887</td>\n",
       "      <td>-10.695185</td>\n",
       "      <td>-21.535988</td>\n",
       "      <td>-8.171207</td>\n",
       "      <td>-25.425447</td>\n",
       "      <td>0.418158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>1.982881</td>\n",
       "      <td>14.4800</td>\n",
       "      <td>1.621515</td>\n",
       "      <td>0.320066</td>\n",
       "      <td>0.322786</td>\n",
       "      <td>0.165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087046</td>\n",
       "      <td>11.933447</td>\n",
       "      <td>9.541476</td>\n",
       "      <td>18.150742</td>\n",
       "      <td>10.076208</td>\n",
       "      <td>9.328708</td>\n",
       "      <td>15.503083</td>\n",
       "      <td>7.838985</td>\n",
       "      <td>14.100815</td>\n",
       "      <td>7.019633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>28.115385</td>\n",
       "      <td>2.550767</td>\n",
       "      <td>14.5950</td>\n",
       "      <td>1.660416</td>\n",
       "      <td>0.201505</td>\n",
       "      <td>0.197983</td>\n",
       "      <td>0.165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191850</td>\n",
       "      <td>15.556124</td>\n",
       "      <td>4.893814</td>\n",
       "      <td>3.789965</td>\n",
       "      <td>21.538450</td>\n",
       "      <td>8.417417</td>\n",
       "      <td>10.874069</td>\n",
       "      <td>18.506872</td>\n",
       "      <td>10.358213</td>\n",
       "      <td>-2.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>2.592189</td>\n",
       "      <td>15.1450</td>\n",
       "      <td>1.823892</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>0.257621</td>\n",
       "      <td>0.165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484614</td>\n",
       "      <td>-2.905304</td>\n",
       "      <td>2.024644</td>\n",
       "      <td>-7.498254</td>\n",
       "      <td>-7.341768</td>\n",
       "      <td>1.063541</td>\n",
       "      <td>-4.676644</td>\n",
       "      <td>-4.741380</td>\n",
       "      <td>-4.544226</td>\n",
       "      <td>5.565071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.600140</td>\n",
       "      <td>37.3900</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>45.779944</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355782</td>\n",
       "      <td>-15.534891</td>\n",
       "      <td>-5.840908</td>\n",
       "      <td>-24.280433</td>\n",
       "      <td>-13.633068</td>\n",
       "      <td>-9.080399</td>\n",
       "      <td>-17.844385</td>\n",
       "      <td>-17.722026</td>\n",
       "      <td>-12.791300</td>\n",
       "      <td>0.512814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>52.800000</td>\n",
       "      <td>3.001754</td>\n",
       "      <td>41.5751</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>41.655270</td>\n",
       "      <td>0.300673</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178828</td>\n",
       "      <td>14.988381</td>\n",
       "      <td>10.496262</td>\n",
       "      <td>15.292074</td>\n",
       "      <td>7.934506</td>\n",
       "      <td>6.057101</td>\n",
       "      <td>16.488114</td>\n",
       "      <td>19.393354</td>\n",
       "      <td>9.857481</td>\n",
       "      <td>9.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>49.574468</td>\n",
       "      <td>4.422004</td>\n",
       "      <td>42.9480</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>44.643278</td>\n",
       "      <td>0.516266</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>4.699164</td>\n",
       "      <td>3.493137</td>\n",
       "      <td>-3.644893</td>\n",
       "      <td>7.351231</td>\n",
       "      <td>0.970027</td>\n",
       "      <td>3.185392</td>\n",
       "      <td>5.459461</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>2.509890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>51.193548</td>\n",
       "      <td>3.780823</td>\n",
       "      <td>40.7500</td>\n",
       "      <td>1.583422</td>\n",
       "      <td>44.768947</td>\n",
       "      <td>0.615809</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>1.258389</td>\n",
       "      <td>5.768897</td>\n",
       "      <td>-7.078949</td>\n",
       "      <td>1.449274</td>\n",
       "      <td>-2.709415</td>\n",
       "      <td>0.271247</td>\n",
       "      <td>3.190669</td>\n",
       "      <td>-0.512819</td>\n",
       "      <td>8.569506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>47.115385</td>\n",
       "      <td>9.213489</td>\n",
       "      <td>41.1200</td>\n",
       "      <td>1.832289</td>\n",
       "      <td>46.407565</td>\n",
       "      <td>0.484964</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063856</td>\n",
       "      <td>3.910523</td>\n",
       "      <td>2.539892</td>\n",
       "      <td>1.418919</td>\n",
       "      <td>9.928574</td>\n",
       "      <td>13.014539</td>\n",
       "      <td>4.946547</td>\n",
       "      <td>13.833353</td>\n",
       "      <td>5.532642</td>\n",
       "      <td>-0.185349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date OFTIC       PENDS       MEAN     STDEV      BPS       CPS  \\\n",
       "51     2011-06-30   AFL  2011-06-30  27.546538  3.568238  12.8250  1.587282   \n",
       "52     2011-09-30   AFL  2011-09-30  26.911765  2.682322  13.6250  1.763514   \n",
       "53     2011-12-31   AFL  2011-12-31  27.750000  1.982881  14.4800  1.621515   \n",
       "54     2012-03-31   AFL  2012-03-31  28.115385  2.550767  14.5950  1.660416   \n",
       "55     2012-06-30   AFL  2012-06-30  28.200000  2.592189  15.1450  1.823892   \n",
       "...           ...   ...         ...        ...       ...      ...       ...   \n",
       "14055  2018-12-31  ZION  2018-12-31  53.000000  4.600140  37.3900  1.760000   \n",
       "14056  2019-03-31  ZION  2019-03-31  52.800000  3.001754  41.5751 -0.210000   \n",
       "14057  2019-06-30  ZION  2019-06-30  49.574468  4.422004  42.9480  1.380000   \n",
       "14058  2019-09-30  ZION  2019-09-30  51.193548  3.780823  40.7500  1.583422   \n",
       "14059  2019-12-31  ZION  2019-12-31  47.115385  9.213489  41.1200  1.832289   \n",
       "\n",
       "             CPX       CSH    DPS  ...  \\\n",
       "51      0.259769  0.235599  0.150  ...   \n",
       "52      0.324273  0.310576  0.150  ...   \n",
       "53      0.320066  0.322786  0.165  ...   \n",
       "54      0.201505  0.197983  0.165  ...   \n",
       "55      0.289115  0.257621  0.165  ...   \n",
       "...          ...       ...    ...  ...   \n",
       "14055  45.779944  0.504142  0.300  ...   \n",
       "14056  41.655270  0.300673  0.300  ...   \n",
       "14057  44.643278  0.516266  0.300  ...   \n",
       "14058  44.768947  0.615809  0.340  ...   \n",
       "14059  46.407565  0.484964  0.340  ...   \n",
       "\n",
       "       commodity_trade_Close_quarterly_return  \\\n",
       "51                                   0.062957   \n",
       "52                                   0.120922   \n",
       "53                                  -0.087046   \n",
       "54                                  -0.191850   \n",
       "55                                   0.484614   \n",
       "...                                       ...   \n",
       "14055                                0.355782   \n",
       "14056                               -0.178828   \n",
       "14057                                0.081598   \n",
       "14058                               -0.036101   \n",
       "14059                               -0.063856   \n",
       "\n",
       "       C_Discretionary_Close_quarterly_return  \\\n",
       "51                                   2.970550   \n",
       "52                                 -13.305144   \n",
       "53                                  11.933447   \n",
       "54                                  15.556124   \n",
       "55                                  -2.905304   \n",
       "...                                       ...   \n",
       "14055                              -15.534891   \n",
       "14056                               14.988381   \n",
       "14057                                4.699164   \n",
       "14058                                1.258389   \n",
       "14059                                3.910523   \n",
       "\n",
       "       C_Staples_Close_quarterly_return  Energy_Close_quarterly_return  \\\n",
       "51                             4.378340                      -5.517243   \n",
       "52                            -5.027217                     -22.349038   \n",
       "53                             9.541476                      18.150742   \n",
       "54                             4.893814                       3.789965   \n",
       "55                             2.024644                      -7.498254   \n",
       "...                                 ...                            ...   \n",
       "14055                         -5.840908                     -24.280433   \n",
       "14056                         10.496262                      15.292074   \n",
       "14057                          3.493137                      -3.644893   \n",
       "14058                          5.768897                      -7.078949   \n",
       "14059                          2.539892                       1.418919   \n",
       "\n",
       "       Financials_Close_quarterly_return  Health_care_Close_quarterly_return  \\\n",
       "51                             -6.345335                            7.308964   \n",
       "52                            -23.061887                          -10.695185   \n",
       "53                             10.076208                            9.328708   \n",
       "54                             21.538450                            8.417417   \n",
       "55                             -7.341768                            1.063541   \n",
       "...                                  ...                                 ...   \n",
       "14055                         -13.633068                           -9.080399   \n",
       "14056                           7.934506                            6.057101   \n",
       "14057                           7.351231                            0.970027   \n",
       "14058                           1.449274                           -2.709415   \n",
       "14059                           9.928574                           13.014539   \n",
       "\n",
       "       industrials_Close_quarterly_return  information_Close_quarterly_return  \\\n",
       "51                              -1.141483                           -1.381423   \n",
       "52                             -21.535988                           -8.171207   \n",
       "53                              15.503083                            7.838985   \n",
       "54                              10.874069                           18.506872   \n",
       "55                              -4.676644                           -4.741380   \n",
       "...                                   ...                                 ...   \n",
       "14055                          -17.844385                          -17.722026   \n",
       "14056                           16.488114                           19.393354   \n",
       "14057                            3.185392                            5.459461   \n",
       "14058                            0.271247                            3.190669   \n",
       "14059                            4.946547                           13.833353   \n",
       "\n",
       "       materials_Close_quarterly_return  utilities_Close_quarterly_return  \n",
       "51                            -1.624192                          5.051769  \n",
       "52                           -25.425447                          0.418158  \n",
       "53                            14.100815                          7.019633  \n",
       "54                            10.358213                         -2.612559  \n",
       "55                            -4.544226                          5.565071  \n",
       "...                                 ...                               ...  \n",
       "14055                        -12.791300                          0.512814  \n",
       "14056                          9.857481                          9.920635  \n",
       "14057                          5.405405                          2.509890  \n",
       "14058                         -0.512819                          8.569506  \n",
       "14059                          5.532642                         -0.185349  \n",
       "\n",
       "[1260 rows x 61 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data = pivoted_data[pivoted_data[\"Sector\"] == \"Financials\"]\n",
    "pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, OFTIC, PENDS, MEAN, STDEV, BPS, CPS, CPX, CSH, DPS, EBG, EBI, EBS, EBT, ENT, EPS, FFO, GPS, GRM, NAV, NDT, NET, OPR, PRE, ROA, ROE, SAL, Real_Estate_Index_Price, VIX_Close, Gold_Close, Three_Month_Yield, Brent_Close, Hrc_close, commodity_trade_Close, C_Discretionary_Close, C_Staples_Close, Energy_Close, Financials_Close, Health_care_Close, industrials_Close, information_Close, materials_Close, utilities_Close, Sector, numeric_sector, Real_Estate_Index_Price_quarterly_return, VIX_Close_quarterly_return, Gold_Close_quarterly_return, Three_Month_Yield_quarterly_return, Brent_Close_quarterly_return, Hrc_close_quarterly_return, commodity_trade_Close_quarterly_return, C_Discretionary_Close_quarterly_return, C_Staples_Close_quarterly_return, Energy_Close_quarterly_return, Financials_Close_quarterly_return, Health_care_Close_quarterly_return, industrials_Close_quarterly_return, information_Close_quarterly_return, materials_Close_quarterly_return, utilities_Close_quarterly_return]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 61 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data[pivoted_data[\"OFTIC\"] == \"ACN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = pivoted_data.copy()\n",
    "\n",
    "pivoted_data['Hrc_close'] = pd.to_numeric(pivoted_data['Hrc_close'].astype(str).str.split(\",\").str[0], errors='coerce')\n",
    "pivoted_data['Gold_Close'] = pd.to_numeric(pivoted_data['Gold_Close'].astype(str).str.replace(\",\", \"\"), errors='coerce').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AFL', 'AON', 'AXP', 'BEN', 'BK', 'BLK', 'BX', 'CBOE', 'CMA',\n",
       "       'CME', 'COF', 'GS', 'HIG', 'IVZ', 'KEY', 'MA', 'MCO', 'MKTX',\n",
       "       'MMC', 'MSCI', 'MTB', 'NDAQ', 'NTRS', 'PFG', 'PGR', 'PNC', 'PRU',\n",
       "       'RF', 'RJF', 'SCHW', 'STT', 'TROW', 'TRV', 'WFC', 'WRB', 'ZION'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_list = pivoted_data[\"OFTIC\"].unique()\n",
    "company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1260, 61)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AFL', 'AON', 'AXP', 'BEN', 'BK', 'BLK', 'BX', 'CBOE', 'CMA',\n",
       "       'CME', 'COF', 'GS', 'HIG', 'IVZ', 'KEY', 'MA', 'MCO', 'MKTX',\n",
       "       'MMC', 'MSCI', 'MTB', 'NDAQ', 'NTRS', 'PFG', 'PGR', 'PNC', 'PRU',\n",
       "       'RF', 'RJF', 'SCHW', 'STT', 'TROW', 'TRV', 'WFC', 'WRB', 'ZION'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_list = pivoted_data[\"OFTIC\"].unique()\n",
    "company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, OFTIC, PENDS, MEAN, STDEV, BPS, CPS, CPX, CSH, DPS, EBG, EBI, EBS, EBT, ENT, EPS, FFO, GPS, GRM, NAV, NDT, NET, OPR, PRE, ROA, ROE, SAL, Real_Estate_Index_Price, VIX_Close, Gold_Close, Three_Month_Yield, Brent_Close, Hrc_close, commodity_trade_Close, C_Discretionary_Close, C_Staples_Close, Energy_Close, Financials_Close, Health_care_Close, industrials_Close, information_Close, materials_Close, utilities_Close, Sector, numeric_sector, Real_Estate_Index_Price_quarterly_return, VIX_Close_quarterly_return, Gold_Close_quarterly_return, Three_Month_Yield_quarterly_return, Brent_Close_quarterly_return, Hrc_close_quarterly_return, commodity_trade_Close_quarterly_return, C_Discretionary_Close_quarterly_return, C_Staples_Close_quarterly_return, Energy_Close_quarterly_return, Financials_Close_quarterly_return, Health_care_Close_quarterly_return, industrials_Close_quarterly_return, information_Close_quarterly_return, materials_Close_quarterly_return, utilities_Close_quarterly_return]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 61 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data[pivoted_data[\"OFTIC\"] == \"AAPL\"].sort_values(by=\"PENDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>EBG</th>\n",
       "      <th>EBI</th>\n",
       "      <th>EBS</th>\n",
       "      <th>...</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sector</th>\n",
       "      <th>numeric_sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>27.546538</td>\n",
       "      <td>3.568238</td>\n",
       "      <td>12.8250</td>\n",
       "      <td>1.587282</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.227910</td>\n",
       "      <td>1367.020851</td>\n",
       "      <td>0.251326</td>\n",
       "      <td>...</td>\n",
       "      <td>7.308964</td>\n",
       "      <td>-1.141483</td>\n",
       "      <td>-1.381423</td>\n",
       "      <td>-1.624192</td>\n",
       "      <td>5.051769</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>26.911765</td>\n",
       "      <td>2.682322</td>\n",
       "      <td>13.6250</td>\n",
       "      <td>1.763514</td>\n",
       "      <td>0.324273</td>\n",
       "      <td>0.310576</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.319596</td>\n",
       "      <td>1389.471671</td>\n",
       "      <td>0.315255</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.695185</td>\n",
       "      <td>-21.535988</td>\n",
       "      <td>-8.171207</td>\n",
       "      <td>-25.425447</td>\n",
       "      <td>0.418158</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>27.750000</td>\n",
       "      <td>1.982881</td>\n",
       "      <td>14.4800</td>\n",
       "      <td>1.621515</td>\n",
       "      <td>0.320066</td>\n",
       "      <td>0.322786</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.269770</td>\n",
       "      <td>1109.000000</td>\n",
       "      <td>0.361496</td>\n",
       "      <td>...</td>\n",
       "      <td>9.328708</td>\n",
       "      <td>15.503083</td>\n",
       "      <td>7.838985</td>\n",
       "      <td>14.100815</td>\n",
       "      <td>7.019633</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>28.115385</td>\n",
       "      <td>2.550767</td>\n",
       "      <td>14.5950</td>\n",
       "      <td>1.660416</td>\n",
       "      <td>0.201505</td>\n",
       "      <td>0.197983</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.183399</td>\n",
       "      <td>1382.000000</td>\n",
       "      <td>0.199331</td>\n",
       "      <td>...</td>\n",
       "      <td>8.417417</td>\n",
       "      <td>10.874069</td>\n",
       "      <td>18.506872</td>\n",
       "      <td>10.358213</td>\n",
       "      <td>-2.612559</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>28.200000</td>\n",
       "      <td>2.592189</td>\n",
       "      <td>15.1450</td>\n",
       "      <td>1.823892</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>0.257621</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.248831</td>\n",
       "      <td>1221.000000</td>\n",
       "      <td>0.282720</td>\n",
       "      <td>...</td>\n",
       "      <td>1.063541</td>\n",
       "      <td>-4.676644</td>\n",
       "      <td>-4.741380</td>\n",
       "      <td>-4.544226</td>\n",
       "      <td>5.565071</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.600140</td>\n",
       "      <td>37.3900</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>45.779944</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.519314</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>0.587800</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.080399</td>\n",
       "      <td>-17.844385</td>\n",
       "      <td>-17.722026</td>\n",
       "      <td>-12.791300</td>\n",
       "      <td>0.512814</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>52.800000</td>\n",
       "      <td>3.001754</td>\n",
       "      <td>41.5751</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>41.655270</td>\n",
       "      <td>0.300673</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.245076</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>0.327376</td>\n",
       "      <td>...</td>\n",
       "      <td>6.057101</td>\n",
       "      <td>16.488114</td>\n",
       "      <td>19.393354</td>\n",
       "      <td>9.857481</td>\n",
       "      <td>9.920635</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>49.574468</td>\n",
       "      <td>4.422004</td>\n",
       "      <td>42.9480</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>44.643278</td>\n",
       "      <td>0.516266</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.489922</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970027</td>\n",
       "      <td>3.185392</td>\n",
       "      <td>5.459461</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>2.509890</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>51.193548</td>\n",
       "      <td>3.780823</td>\n",
       "      <td>40.7500</td>\n",
       "      <td>1.583422</td>\n",
       "      <td>44.768947</td>\n",
       "      <td>0.615809</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.638738</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>0.654236</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.709415</td>\n",
       "      <td>0.271247</td>\n",
       "      <td>3.190669</td>\n",
       "      <td>-0.512819</td>\n",
       "      <td>8.569506</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>47.115385</td>\n",
       "      <td>9.213489</td>\n",
       "      <td>41.1200</td>\n",
       "      <td>1.832289</td>\n",
       "      <td>46.407565</td>\n",
       "      <td>0.484964</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.488651</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>0.522177</td>\n",
       "      <td>...</td>\n",
       "      <td>13.014539</td>\n",
       "      <td>4.946547</td>\n",
       "      <td>13.833353</td>\n",
       "      <td>5.532642</td>\n",
       "      <td>-0.185349</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            MEAN     STDEV      BPS       CPS        CPX       CSH    DPS  \\\n",
       "51     27.546538  3.568238  12.8250  1.587282   0.259769  0.235599  0.150   \n",
       "52     26.911765  2.682322  13.6250  1.763514   0.324273  0.310576  0.150   \n",
       "53     27.750000  1.982881  14.4800  1.621515   0.320066  0.322786  0.165   \n",
       "54     28.115385  2.550767  14.5950  1.660416   0.201505  0.197983  0.165   \n",
       "55     28.200000  2.592189  15.1450  1.823892   0.289115  0.257621  0.165   \n",
       "...          ...       ...      ...       ...        ...       ...    ...   \n",
       "14055  53.000000  4.600140  37.3900  1.760000  45.779944  0.504142  0.300   \n",
       "14056  52.800000  3.001754  41.5751 -0.210000  41.655270  0.300673  0.300   \n",
       "14057  49.574468  4.422004  42.9480  1.380000  44.643278  0.516266  0.300   \n",
       "14058  51.193548  3.780823  40.7500  1.583422  44.768947  0.615809  0.340   \n",
       "14059  47.115385  9.213489  41.1200  1.832289  46.407565  0.484964  0.340   \n",
       "\n",
       "            EBG          EBI       EBS  ...  \\\n",
       "51     0.227910  1367.020851  0.251326  ...   \n",
       "52     0.319596  1389.471671  0.315255  ...   \n",
       "53     0.269770  1109.000000  0.361496  ...   \n",
       "54     0.183399  1382.000000  0.199331  ...   \n",
       "55     0.248831  1221.000000  0.282720  ...   \n",
       "...         ...          ...       ...  ...   \n",
       "14055  0.519314   297.000000  0.587800  ...   \n",
       "14056  0.245076   278.000000  0.327376  ...   \n",
       "14057  0.489922   277.000000  0.540476  ...   \n",
       "14058  0.638738   298.000000  0.654236  ...   \n",
       "14059  0.488651   242.000000  0.522177  ...   \n",
       "\n",
       "       Health_care_Close_quarterly_return  industrials_Close_quarterly_return  \\\n",
       "51                               7.308964                           -1.141483   \n",
       "52                             -10.695185                          -21.535988   \n",
       "53                               9.328708                           15.503083   \n",
       "54                               8.417417                           10.874069   \n",
       "55                               1.063541                           -4.676644   \n",
       "...                                   ...                                 ...   \n",
       "14055                           -9.080399                          -17.844385   \n",
       "14056                            6.057101                           16.488114   \n",
       "14057                            0.970027                            3.185392   \n",
       "14058                           -2.709415                            0.271247   \n",
       "14059                           13.014539                            4.946547   \n",
       "\n",
       "       information_Close_quarterly_return  materials_Close_quarterly_return  \\\n",
       "51                              -1.381423                         -1.624192   \n",
       "52                              -8.171207                        -25.425447   \n",
       "53                               7.838985                         14.100815   \n",
       "54                              18.506872                         10.358213   \n",
       "55                              -4.741380                         -4.544226   \n",
       "...                                   ...                               ...   \n",
       "14055                          -17.722026                        -12.791300   \n",
       "14056                           19.393354                          9.857481   \n",
       "14057                            5.459461                          5.405405   \n",
       "14058                            3.190669                         -0.512819   \n",
       "14059                           13.833353                          5.532642   \n",
       "\n",
       "       utilities_Close_quarterly_return  OFTIC       PENDS        Date  \\\n",
       "51                             5.051769    AFL  2011-06-30  2011-06-30   \n",
       "52                             0.418158    AFL  2011-09-30  2011-09-30   \n",
       "53                             7.019633    AFL  2011-12-31  2011-12-31   \n",
       "54                            -2.612559    AFL  2012-03-31  2012-03-31   \n",
       "55                             5.565071    AFL  2012-06-30  2012-06-30   \n",
       "...                                 ...    ...         ...         ...   \n",
       "14055                          0.512814   ZION  2018-12-31  2018-12-31   \n",
       "14056                          9.920635   ZION  2019-03-31  2019-03-31   \n",
       "14057                          2.509890   ZION  2019-06-30  2019-06-30   \n",
       "14058                          8.569506   ZION  2019-09-30  2019-09-30   \n",
       "14059                         -0.185349   ZION  2019-12-31  2019-12-31   \n",
       "\n",
       "           Sector  numeric_sector  \n",
       "51     Financials               5  \n",
       "52     Financials               5  \n",
       "53     Financials               5  \n",
       "54     Financials               5  \n",
       "55     Financials               5  \n",
       "...           ...             ...  \n",
       "14055  Financials               5  \n",
       "14056  Financials               5  \n",
       "14057  Financials               5  \n",
       "14058  Financials               5  \n",
       "14059  Financials               5  \n",
       "\n",
       "[1260 rows x 61 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_move = ['OFTIC', 'PENDS', 'Date', 'Sector', 'numeric_sector']\n",
    "pivoted_data = pivoted_data[[col for col in pivoted_data if col not in cols_to_move] + cols_to_move]\n",
    "pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "AFL: EPS - PENDS",
         "type": "scatter",
         "x": [
          "2011-06-30",
          "2011-09-30",
          "2011-12-31",
          "2012-03-31",
          "2012-06-30",
          "2012-09-30",
          "2012-12-31",
          "2013-03-31",
          "2013-06-30",
          "2013-09-30",
          "2013-12-31",
          "2014-03-31",
          "2014-06-30",
          "2014-09-30",
          "2014-12-31",
          "2015-03-31",
          "2015-06-30",
          "2015-09-30",
          "2015-12-31",
          "2016-03-31",
          "2016-06-30",
          "2016-09-30",
          "2016-12-31",
          "2017-03-31",
          "2017-06-30",
          "2017-09-30",
          "2017-12-31",
          "2018-03-31",
          "2018-06-30",
          "2018-09-30",
          "2018-12-31",
          "2019-03-31",
          "2019-06-30",
          "2019-09-30",
          "2019-12-31"
         ],
         "y": [
          0.78,
          0.83,
          0.74,
          0.825,
          0.805,
          0.885,
          0.74,
          0.845,
          0.81,
          0.735,
          0.7,
          0.845,
          0.83,
          0.7549999999999999,
          0.645,
          0.77,
          0.75,
          0.78,
          0.78,
          0.865,
          0.855,
          0.91,
          0.73,
          0.8350000000000001,
          0.915,
          0.85,
          0.815,
          1.02,
          1.06,
          1.03,
          1.02,
          1.12,
          1.13,
          1.16,
          1.03
         ]
        }
       ],
       "layout": {
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "AFL: EPS - PENDS"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "rangemode": "tozero",
         "side": "left",
         "title": {
          "text": "EPS"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_symbol = \"AFL\"\n",
    "stock = pivoted_data[pivoted_data[\"OFTIC\"] == stock_symbol].sort_values(by=\"PENDS\")\n",
    "\n",
    "stock_features_dict = {}\n",
    "for column in stock.columns:\n",
    "    stock_features_dict[column] = stock[column]\n",
    "\n",
    "trace = go.Scatter(x=stock_features_dict[\"PENDS\"], y=stock_features_dict[\"EPS\"], mode=\"lines+markers\", name=f\"{stock_symbol}: EPS - PENDS\")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = f\"{stock_symbol}: EPS - PENDS\",\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='EPS', side='left', rangemode='tozero'),\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51       0.780\n",
       "52       0.830\n",
       "53       0.740\n",
       "54       0.825\n",
       "55       0.805\n",
       "         ...  \n",
       "14055    1.080\n",
       "14056    1.040\n",
       "14057    0.990\n",
       "14058    1.170\n",
       "14059    0.970\n",
       "Name: EPS, Length: 1260, dtype: float64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = pivoted_data.drop(columns=[\"EPS\"])\n",
    "y_ = pivoted_data['EPS']\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>EBG</th>\n",
       "      <th>EBI</th>\n",
       "      <th>EBS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>27.546538</td>\n",
       "      <td>3.568238</td>\n",
       "      <td>12.8250</td>\n",
       "      <td>1.587282</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.227910</td>\n",
       "      <td>1367.020851</td>\n",
       "      <td>0.251326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062957</td>\n",
       "      <td>2.970550</td>\n",
       "      <td>4.378340</td>\n",
       "      <td>-5.517243</td>\n",
       "      <td>-6.345335</td>\n",
       "      <td>7.308964</td>\n",
       "      <td>-1.141483</td>\n",
       "      <td>-1.381423</td>\n",
       "      <td>-1.624192</td>\n",
       "      <td>5.051769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>26.911765</td>\n",
       "      <td>2.682322</td>\n",
       "      <td>13.6250</td>\n",
       "      <td>1.763514</td>\n",
       "      <td>0.324273</td>\n",
       "      <td>0.310576</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.319596</td>\n",
       "      <td>1389.471671</td>\n",
       "      <td>0.315255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120922</td>\n",
       "      <td>-13.305144</td>\n",
       "      <td>-5.027217</td>\n",
       "      <td>-22.349038</td>\n",
       "      <td>-23.061887</td>\n",
       "      <td>-10.695185</td>\n",
       "      <td>-21.535988</td>\n",
       "      <td>-8.171207</td>\n",
       "      <td>-25.425447</td>\n",
       "      <td>0.418158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>27.750000</td>\n",
       "      <td>1.982881</td>\n",
       "      <td>14.4800</td>\n",
       "      <td>1.621515</td>\n",
       "      <td>0.320066</td>\n",
       "      <td>0.322786</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.269770</td>\n",
       "      <td>1109.000000</td>\n",
       "      <td>0.361496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087046</td>\n",
       "      <td>11.933447</td>\n",
       "      <td>9.541476</td>\n",
       "      <td>18.150742</td>\n",
       "      <td>10.076208</td>\n",
       "      <td>9.328708</td>\n",
       "      <td>15.503083</td>\n",
       "      <td>7.838985</td>\n",
       "      <td>14.100815</td>\n",
       "      <td>7.019633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>28.115385</td>\n",
       "      <td>2.550767</td>\n",
       "      <td>14.5950</td>\n",
       "      <td>1.660416</td>\n",
       "      <td>0.201505</td>\n",
       "      <td>0.197983</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.183399</td>\n",
       "      <td>1382.000000</td>\n",
       "      <td>0.199331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191850</td>\n",
       "      <td>15.556124</td>\n",
       "      <td>4.893814</td>\n",
       "      <td>3.789965</td>\n",
       "      <td>21.538450</td>\n",
       "      <td>8.417417</td>\n",
       "      <td>10.874069</td>\n",
       "      <td>18.506872</td>\n",
       "      <td>10.358213</td>\n",
       "      <td>-2.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>28.200000</td>\n",
       "      <td>2.592189</td>\n",
       "      <td>15.1450</td>\n",
       "      <td>1.823892</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>0.257621</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.248831</td>\n",
       "      <td>1221.000000</td>\n",
       "      <td>0.282720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484614</td>\n",
       "      <td>-2.905304</td>\n",
       "      <td>2.024644</td>\n",
       "      <td>-7.498254</td>\n",
       "      <td>-7.341768</td>\n",
       "      <td>1.063541</td>\n",
       "      <td>-4.676644</td>\n",
       "      <td>-4.741380</td>\n",
       "      <td>-4.544226</td>\n",
       "      <td>5.565071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.600140</td>\n",
       "      <td>37.3900</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>45.779944</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.519314</td>\n",
       "      <td>297.000000</td>\n",
       "      <td>0.587800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355782</td>\n",
       "      <td>-15.534891</td>\n",
       "      <td>-5.840908</td>\n",
       "      <td>-24.280433</td>\n",
       "      <td>-13.633068</td>\n",
       "      <td>-9.080399</td>\n",
       "      <td>-17.844385</td>\n",
       "      <td>-17.722026</td>\n",
       "      <td>-12.791300</td>\n",
       "      <td>0.512814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>52.800000</td>\n",
       "      <td>3.001754</td>\n",
       "      <td>41.5751</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>41.655270</td>\n",
       "      <td>0.300673</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.245076</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>0.327376</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178828</td>\n",
       "      <td>14.988381</td>\n",
       "      <td>10.496262</td>\n",
       "      <td>15.292074</td>\n",
       "      <td>7.934506</td>\n",
       "      <td>6.057101</td>\n",
       "      <td>16.488114</td>\n",
       "      <td>19.393354</td>\n",
       "      <td>9.857481</td>\n",
       "      <td>9.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>49.574468</td>\n",
       "      <td>4.422004</td>\n",
       "      <td>42.9480</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>44.643278</td>\n",
       "      <td>0.516266</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.489922</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>4.699164</td>\n",
       "      <td>3.493137</td>\n",
       "      <td>-3.644893</td>\n",
       "      <td>7.351231</td>\n",
       "      <td>0.970027</td>\n",
       "      <td>3.185392</td>\n",
       "      <td>5.459461</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>2.509890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>51.193548</td>\n",
       "      <td>3.780823</td>\n",
       "      <td>40.7500</td>\n",
       "      <td>1.583422</td>\n",
       "      <td>44.768947</td>\n",
       "      <td>0.615809</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.638738</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>0.654236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>1.258389</td>\n",
       "      <td>5.768897</td>\n",
       "      <td>-7.078949</td>\n",
       "      <td>1.449274</td>\n",
       "      <td>-2.709415</td>\n",
       "      <td>0.271247</td>\n",
       "      <td>3.190669</td>\n",
       "      <td>-0.512819</td>\n",
       "      <td>8.569506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>47.115385</td>\n",
       "      <td>9.213489</td>\n",
       "      <td>41.1200</td>\n",
       "      <td>1.832289</td>\n",
       "      <td>46.407565</td>\n",
       "      <td>0.484964</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.488651</td>\n",
       "      <td>242.000000</td>\n",
       "      <td>0.522177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063856</td>\n",
       "      <td>3.910523</td>\n",
       "      <td>2.539892</td>\n",
       "      <td>1.418919</td>\n",
       "      <td>9.928574</td>\n",
       "      <td>13.014539</td>\n",
       "      <td>4.946547</td>\n",
       "      <td>13.833353</td>\n",
       "      <td>5.532642</td>\n",
       "      <td>-0.185349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            MEAN     STDEV      BPS       CPS        CPX       CSH    DPS  \\\n",
       "51     27.546538  3.568238  12.8250  1.587282   0.259769  0.235599  0.150   \n",
       "52     26.911765  2.682322  13.6250  1.763514   0.324273  0.310576  0.150   \n",
       "53     27.750000  1.982881  14.4800  1.621515   0.320066  0.322786  0.165   \n",
       "54     28.115385  2.550767  14.5950  1.660416   0.201505  0.197983  0.165   \n",
       "55     28.200000  2.592189  15.1450  1.823892   0.289115  0.257621  0.165   \n",
       "...          ...       ...      ...       ...        ...       ...    ...   \n",
       "14055  53.000000  4.600140  37.3900  1.760000  45.779944  0.504142  0.300   \n",
       "14056  52.800000  3.001754  41.5751 -0.210000  41.655270  0.300673  0.300   \n",
       "14057  49.574468  4.422004  42.9480  1.380000  44.643278  0.516266  0.300   \n",
       "14058  51.193548  3.780823  40.7500  1.583422  44.768947  0.615809  0.340   \n",
       "14059  47.115385  9.213489  41.1200  1.832289  46.407565  0.484964  0.340   \n",
       "\n",
       "            EBG          EBI       EBS  ...  \\\n",
       "51     0.227910  1367.020851  0.251326  ...   \n",
       "52     0.319596  1389.471671  0.315255  ...   \n",
       "53     0.269770  1109.000000  0.361496  ...   \n",
       "54     0.183399  1382.000000  0.199331  ...   \n",
       "55     0.248831  1221.000000  0.282720  ...   \n",
       "...         ...          ...       ...  ...   \n",
       "14055  0.519314   297.000000  0.587800  ...   \n",
       "14056  0.245076   278.000000  0.327376  ...   \n",
       "14057  0.489922   277.000000  0.540476  ...   \n",
       "14058  0.638738   298.000000  0.654236  ...   \n",
       "14059  0.488651   242.000000  0.522177  ...   \n",
       "\n",
       "       commodity_trade_Close_quarterly_return  \\\n",
       "51                                   0.062957   \n",
       "52                                   0.120922   \n",
       "53                                  -0.087046   \n",
       "54                                  -0.191850   \n",
       "55                                   0.484614   \n",
       "...                                       ...   \n",
       "14055                                0.355782   \n",
       "14056                               -0.178828   \n",
       "14057                                0.081598   \n",
       "14058                               -0.036101   \n",
       "14059                               -0.063856   \n",
       "\n",
       "       C_Discretionary_Close_quarterly_return  \\\n",
       "51                                   2.970550   \n",
       "52                                 -13.305144   \n",
       "53                                  11.933447   \n",
       "54                                  15.556124   \n",
       "55                                  -2.905304   \n",
       "...                                       ...   \n",
       "14055                              -15.534891   \n",
       "14056                               14.988381   \n",
       "14057                                4.699164   \n",
       "14058                                1.258389   \n",
       "14059                                3.910523   \n",
       "\n",
       "       C_Staples_Close_quarterly_return  Energy_Close_quarterly_return  \\\n",
       "51                             4.378340                      -5.517243   \n",
       "52                            -5.027217                     -22.349038   \n",
       "53                             9.541476                      18.150742   \n",
       "54                             4.893814                       3.789965   \n",
       "55                             2.024644                      -7.498254   \n",
       "...                                 ...                            ...   \n",
       "14055                         -5.840908                     -24.280433   \n",
       "14056                         10.496262                      15.292074   \n",
       "14057                          3.493137                      -3.644893   \n",
       "14058                          5.768897                      -7.078949   \n",
       "14059                          2.539892                       1.418919   \n",
       "\n",
       "       Financials_Close_quarterly_return  Health_care_Close_quarterly_return  \\\n",
       "51                             -6.345335                            7.308964   \n",
       "52                            -23.061887                          -10.695185   \n",
       "53                             10.076208                            9.328708   \n",
       "54                             21.538450                            8.417417   \n",
       "55                             -7.341768                            1.063541   \n",
       "...                                  ...                                 ...   \n",
       "14055                         -13.633068                           -9.080399   \n",
       "14056                           7.934506                            6.057101   \n",
       "14057                           7.351231                            0.970027   \n",
       "14058                           1.449274                           -2.709415   \n",
       "14059                           9.928574                           13.014539   \n",
       "\n",
       "       industrials_Close_quarterly_return  information_Close_quarterly_return  \\\n",
       "51                              -1.141483                           -1.381423   \n",
       "52                             -21.535988                           -8.171207   \n",
       "53                              15.503083                            7.838985   \n",
       "54                              10.874069                           18.506872   \n",
       "55                              -4.676644                           -4.741380   \n",
       "...                                   ...                                 ...   \n",
       "14055                          -17.844385                          -17.722026   \n",
       "14056                           16.488114                           19.393354   \n",
       "14057                            3.185392                            5.459461   \n",
       "14058                            0.271247                            3.190669   \n",
       "14059                            4.946547                           13.833353   \n",
       "\n",
       "       materials_Close_quarterly_return  utilities_Close_quarterly_return  \n",
       "51                            -1.624192                          5.051769  \n",
       "52                           -25.425447                          0.418158  \n",
       "53                            14.100815                          7.019633  \n",
       "54                            10.358213                         -2.612559  \n",
       "55                            -4.544226                          5.565071  \n",
       "...                                 ...                               ...  \n",
       "14055                        -12.791300                          0.512814  \n",
       "14056                          9.857481                          9.920635  \n",
       "14057                          5.405405                          2.509890  \n",
       "14058                         -0.512819                          8.569506  \n",
       "14059                          5.532642                         -0.185349  \n",
       "\n",
       "[1260 rows x 56 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values = pivoted_data.drop(columns=[\"OFTIC\",\"PENDS\",\"Date\",\"Sector\",\"numeric_sector\"])\n",
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ],
         "text": [
          [
           1,
           0.81,
           0.47,
           0.33,
           0.03,
           0.01,
           0.44,
           0.12,
           0.06,
           0.23,
           0.03,
           0.08,
           0.5,
           0.11,
           0.44,
           -0.03,
           0.1,
           0.09,
           0.08,
           0.03,
           0.06,
           -0.06,
           0.02,
           0.24,
           -0.01,
           -0.08,
           -0.13,
           0.17,
           -0.17,
           0.02,
           0.17,
           0.22,
           0.21,
           -0.06,
           0.22,
           0.22,
           0.22,
           0.2,
           0.21,
           0.21,
           0.01,
           0,
           0.02,
           -0.01,
           0.02,
           0.04,
           -0.01,
           -0.01,
           -0.03,
           -0.01,
           0,
           -0.03,
           0,
           0.01,
           0,
           0
          ],
          [
           0.81,
           1,
           0.22,
           0.24,
           -0.03,
           -0.08,
           0.11,
           0.02,
           0.01,
           -0.06,
           -0.01,
           -0.02,
           0.16,
           0.01,
           0.12,
           -0.11,
           0.06,
           0.03,
           0.02,
           -0.02,
           0,
           -0.13,
           -0.03,
           0.2,
           0.02,
           -0.04,
           -0.07,
           0.01,
           -0.05,
           -0.02,
           0.05,
           0.04,
           0.06,
           0.03,
           0.05,
           0.05,
           0.05,
           0.03,
           0.05,
           0.05,
           -0.02,
           -0.01,
           0,
           -0.01,
           0,
           0.02,
           0.01,
           -0.01,
           -0.01,
           0.01,
           -0.01,
           -0.01,
           0,
           -0.01,
           0,
           0.01
          ],
          [
           0.47,
           0.22,
           1,
           0.3,
           0.48,
           0.14,
           0.67,
           0.09,
           0.26,
           0.59,
           0.25,
           0.52,
           0.87,
           0.07,
           0.73,
           -0.29,
           0.39,
           0.46,
           0.27,
           0.26,
           0.25,
           -0.33,
           -0.1,
           0.3,
           0,
           -0.04,
           -0.05,
           0.11,
           -0.09,
           0,
           0.1,
           0.13,
           0.13,
           -0.06,
           0.13,
           0.13,
           0.13,
           0.13,
           0.12,
           0.13,
           0.03,
           0,
           0.02,
           0,
           0.02,
           0.01,
           -0.02,
           -0.01,
           -0.01,
           -0.01,
           0.01,
           -0.01,
           0,
           0,
           0,
           0
          ],
          [
           0.33,
           0.24,
           0.3,
           1,
           0,
           0.06,
           0.19,
           0.11,
           0.12,
           0.06,
           0.1,
           0.02,
           0.25,
           0.1,
           0.2,
           -0.08,
           0.11,
           0.06,
           0.08,
           0.06,
           0.08,
           -0.09,
           -0.02,
           0.25,
           0.09,
           0.09,
           0.04,
           0.05,
           0.02,
           0.04,
           -0.01,
           0.02,
           -0.02,
           -0.07,
           0.01,
           0,
           0,
           0.03,
           -0.01,
           0.01,
           -0.02,
           0.05,
           -0.02,
           0.02,
           -0.03,
           -0.04,
           0.02,
           -0.01,
           -0.01,
           -0.04,
           -0.03,
           -0.03,
           -0.04,
           -0.01,
           -0.05,
           -0.02
          ],
          [
           0.03,
           -0.03,
           0.48,
           0,
           1,
           0.01,
           0.09,
           0.04,
           0.24,
           0.57,
           0.25,
           0.83,
           0.45,
           0.05,
           0.35,
           -0.1,
           0.3,
           0.68,
           0.26,
           0.26,
           0.25,
           -0.11,
           -0.02,
           0.2,
           -0.02,
           -0.01,
           0,
           0.08,
           -0.02,
           0.03,
           0.03,
           0.07,
           0.05,
           -0.03,
           0.07,
           0.06,
           0.07,
           0.07,
           0.06,
           0.07,
           0.02,
           0.01,
           0.01,
           -0.01,
           0.01,
           -0.02,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           0,
           -0.01,
           -0.01,
           0,
           -0.01,
           0
          ],
          [
           0.01,
           -0.08,
           0.14,
           0.06,
           0.01,
           1,
           0.12,
           0.18,
           -0.06,
           -0.03,
           -0.07,
           -0.03,
           0.16,
           0.17,
           0.12,
           -0.11,
           -0.06,
           -0.06,
           -0.06,
           -0.06,
           -0.06,
           -0.11,
           0,
           -0.07,
           0,
           0,
           -0.02,
           0.1,
           -0.03,
           0.02,
           0.04,
           0.1,
           0.08,
           -0.04,
           0.1,
           0.1,
           0.1,
           0.1,
           0.09,
           0.1,
           0.03,
           0.02,
           0,
           -0.03,
           -0.02,
           -0.03,
           0.01,
           -0.03,
           -0.03,
           -0.03,
           -0.02,
           -0.02,
           -0.02,
           -0.02,
           -0.02,
           -0.01
          ],
          [
           0.44,
           0.11,
           0.67,
           0.19,
           0.09,
           0.12,
           1,
           0.14,
           0.1,
           0.54,
           0.09,
           0.16,
           0.73,
           0.14,
           0.64,
           -0.08,
           0.15,
           0.13,
           0.14,
           0.1,
           0.11,
           -0.07,
           -0.02,
           0.11,
           0,
           -0.07,
           -0.07,
           0.21,
           -0.14,
           0.03,
           0.16,
           0.24,
           0.22,
           -0.1,
           0.23,
           0.23,
           0.23,
           0.23,
           0.22,
           0.23,
           0.04,
           -0.01,
           0.01,
           0,
           0.02,
           0.05,
           -0.01,
           0,
           -0.02,
           -0.01,
           0.02,
           -0.01,
           0.02,
           0.01,
           0.02,
           -0.01
          ],
          [
           0.12,
           0.02,
           0.09,
           0.11,
           0.04,
           0.18,
           0.14,
           1,
           0.03,
           0.05,
           0.02,
           -0.02,
           0.16,
           0.95,
           0.15,
           -0.01,
           0.03,
           -0.01,
           0.06,
           0.01,
           0.03,
           -0.04,
           0.07,
           0.04,
           0.13,
           -0.02,
           -0.09,
           0.56,
           -0.22,
           0.12,
           0.24,
           0.58,
           0.46,
           -0.24,
           0.53,
           0.53,
           0.53,
           0.58,
           0.48,
           0.56,
           0.16,
           0.13,
           0.06,
           -0.07,
           -0.11,
           -0.15,
           0.04,
           -0.16,
           -0.14,
           -0.17,
           -0.1,
           -0.1,
           -0.11,
           -0.05,
           -0.11,
           -0.02
          ],
          [
           0.06,
           0.01,
           0.26,
           0.12,
           0.24,
           -0.06,
           0.1,
           0.03,
           1,
           0.09,
           0.98,
           0.2,
           0.27,
           0.04,
           0.22,
           -0.12,
           0.93,
           0.19,
           0.97,
           0.96,
           0.98,
           -0.12,
           0,
           0.86,
           -0.01,
           -0.03,
           -0.03,
           0.08,
           -0.04,
           0,
           0.04,
           0.08,
           0.08,
           0,
           0.08,
           0.08,
           0.08,
           0.09,
           0.08,
           0.07,
           0.02,
           0,
           0,
           0.12,
           0.01,
           0.01,
           -0.01,
           0.06,
           0.02,
           0,
           0.04,
           0.06,
           0.04,
           0.1,
           0.03,
           0.01
          ],
          [
           0.23,
           -0.06,
           0.59,
           0.06,
           0.57,
           -0.03,
           0.54,
           0.05,
           0.09,
           1,
           0.11,
           0.75,
           0.7,
           0.04,
           0.58,
           0.17,
           0.14,
           0.66,
           0.13,
           0.15,
           0.13,
           0.11,
           0.04,
           0.01,
           -0.03,
           -0.02,
           -0.04,
           0.08,
           -0.06,
           0.01,
           0.06,
           0.09,
           0.08,
           -0.03,
           0.09,
           0.09,
           0.09,
           0.08,
           0.08,
           0.09,
           0.02,
           0.01,
           0.01,
           -0.03,
           0.01,
           0.01,
           -0.01,
           -0.02,
           -0.02,
           -0.01,
           -0.01,
           -0.02,
           -0.01,
           -0.01,
           0,
           0
          ],
          [
           0.03,
           -0.01,
           0.25,
           0.1,
           0.25,
           -0.07,
           0.09,
           0.02,
           0.98,
           0.11,
           1,
           0.22,
           0.25,
           0.02,
           0.19,
           -0.12,
           0.93,
           0.21,
           0.97,
           0.97,
           0.98,
           -0.13,
           0,
           0.85,
           0,
           -0.01,
           -0.02,
           0.05,
           -0.03,
           0,
           0.03,
           0.06,
           0.05,
           0,
           0.05,
           0.06,
           0.06,
           0.06,
           0.06,
           0.05,
           0.01,
           0,
           -0.01,
           0.12,
           0,
           0,
           0,
           0.06,
           0.02,
           0,
           0.04,
           0.06,
           0.04,
           0.1,
           0.03,
           0.01
          ],
          [
           0.08,
           -0.02,
           0.52,
           0.02,
           0.83,
           -0.03,
           0.16,
           -0.02,
           0.2,
           0.75,
           0.22,
           1,
           0.5,
           -0.03,
           0.37,
           0.02,
           0.28,
           0.89,
           0.24,
           0.25,
           0.24,
           0.01,
           0.02,
           0.15,
           -0.09,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.01,
           0,
           0,
           -0.01,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.01,
           0,
           0
          ],
          [
           0.5,
           0.16,
           0.87,
           0.25,
           0.45,
           0.16,
           0.73,
           0.16,
           0.27,
           0.7,
           0.25,
           0.5,
           1,
           0.15,
           0.83,
           -0.08,
           0.3,
           0.43,
           0.31,
           0.24,
           0.28,
           -0.13,
           0.02,
           0.27,
           0.01,
           -0.1,
           -0.11,
           0.25,
           -0.15,
           0.06,
           0.16,
           0.28,
           0.25,
           -0.06,
           0.28,
           0.27,
           0.28,
           0.27,
           0.26,
           0.26,
           0.04,
           -0.02,
           0.01,
           0,
           0.04,
           0.03,
           -0.04,
           0.01,
           -0.02,
           0,
           0.03,
           0,
           0.02,
           0.02,
           0.03,
           -0.01
          ],
          [
           0.11,
           0.01,
           0.07,
           0.1,
           0.05,
           0.17,
           0.14,
           0.95,
           0.04,
           0.04,
           0.02,
           -0.03,
           0.15,
           1,
           0.13,
           0,
           0.04,
           -0.02,
           0.06,
           0.01,
           0.04,
           -0.04,
           0.09,
           0.06,
           0.13,
           -0.01,
           -0.08,
           0.56,
           -0.2,
           0.13,
           0.21,
           0.57,
           0.44,
           -0.23,
           0.52,
           0.52,
           0.52,
           0.58,
           0.47,
           0.54,
           0.15,
           0.13,
           0.05,
           -0.07,
           -0.11,
           -0.16,
           0.04,
           -0.16,
           -0.14,
           -0.17,
           -0.11,
           -0.1,
           -0.12,
           -0.05,
           -0.12,
           -0.02
          ],
          [
           0.44,
           0.12,
           0.73,
           0.2,
           0.35,
           0.12,
           0.64,
           0.15,
           0.22,
           0.58,
           0.19,
           0.37,
           0.83,
           0.13,
           1,
           -0.07,
           0.24,
           0.32,
           0.25,
           0.2,
           0.23,
           -0.11,
           0.01,
           0.19,
           0,
           -0.08,
           -0.09,
           0.21,
           -0.13,
           0.04,
           0.14,
           0.24,
           0.21,
           -0.06,
           0.23,
           0.23,
           0.23,
           0.23,
           0.22,
           0.22,
           0.04,
           -0.02,
           0.03,
           0,
           0.04,
           0.01,
           -0.03,
           0.01,
           -0.03,
           0,
           0.01,
           0,
           0.01,
           0.03,
           0.01,
           0
          ],
          [
           -0.03,
           -0.11,
           -0.29,
           -0.08,
           -0.1,
           -0.11,
           -0.08,
           -0.01,
           -0.12,
           0.17,
           -0.12,
           0.02,
           -0.08,
           0,
           -0.07,
           1,
           -0.22,
           -0.02,
           -0.11,
           -0.09,
           -0.11,
           0.5,
           0.18,
           -0.22,
           -0.12,
           -0.01,
           -0.01,
           0.01,
           -0.01,
           0,
           0.01,
           0.01,
           0.01,
           0,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0,
           0,
           -0.02,
           0,
           0,
           0,
           -0.01,
           -0.01,
           0,
           0,
           -0.01,
           -0.01,
           -0.01,
           0,
           0
          ],
          [
           0.1,
           0.06,
           0.39,
           0.11,
           0.3,
           -0.06,
           0.15,
           0.03,
           0.93,
           0.14,
           0.93,
           0.28,
           0.3,
           0.04,
           0.24,
           -0.22,
           1,
           0.25,
           0.93,
           0.94,
           0.94,
           -0.24,
           -0.07,
           0.83,
           -0.01,
           -0.03,
           -0.04,
           0.07,
           -0.06,
           0,
           0.06,
           0.08,
           0.08,
           -0.02,
           0.08,
           0.08,
           0.08,
           0.08,
           0.08,
           0.08,
           0.02,
           -0.01,
           0.01,
           0.09,
           0.01,
           0.01,
           0,
           0.05,
           0.02,
           0,
           0.03,
           0.04,
           0.03,
           0.08,
           0.03,
           0.01
          ],
          [
           0.09,
           0.03,
           0.46,
           0.06,
           0.68,
           -0.06,
           0.13,
           -0.01,
           0.19,
           0.66,
           0.21,
           0.89,
           0.43,
           -0.02,
           0.32,
           -0.02,
           0.25,
           1,
           0.21,
           0.22,
           0.21,
           -0.03,
           -0.01,
           0.17,
           -0.05,
           0,
           0,
           0.01,
           0,
           0,
           0.01,
           0.01,
           0.01,
           0,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0,
           0,
           -0.02,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0.08,
           0.02,
           0.27,
           0.08,
           0.26,
           -0.06,
           0.14,
           0.06,
           0.97,
           0.13,
           0.97,
           0.24,
           0.31,
           0.06,
           0.25,
           -0.11,
           0.93,
           0.21,
           1,
           0.95,
           0.99,
           -0.1,
           0.02,
           0.85,
           0,
           -0.05,
           -0.05,
           0.12,
           -0.06,
           0.03,
           0.06,
           0.13,
           0.12,
           0,
           0.13,
           0.13,
           0.13,
           0.13,
           0.13,
           0.11,
           0.02,
           -0.02,
           0,
           0.11,
           0.02,
           0.01,
           -0.01,
           0.07,
           0.02,
           0.01,
           0.05,
           0.07,
           0.05,
           0.1,
           0.04,
           0.01
          ],
          [
           0.03,
           -0.02,
           0.26,
           0.06,
           0.26,
           -0.06,
           0.1,
           0.01,
           0.96,
           0.15,
           0.97,
           0.25,
           0.24,
           0.01,
           0.2,
           -0.09,
           0.94,
           0.22,
           0.95,
           1,
           0.96,
           -0.11,
           -0.01,
           0.81,
           -0.02,
           -0.01,
           -0.02,
           0.04,
           -0.03,
           -0.01,
           0.02,
           0.04,
           0.04,
           0,
           0.04,
           0.05,
           0.04,
           0.05,
           0.04,
           0.04,
           0.02,
           0,
           0.01,
           0.11,
           0,
           -0.01,
           0,
           0.06,
           0.02,
           0,
           0.03,
           0.06,
           0.03,
           0.09,
           0.02,
           0.02
          ],
          [
           0.06,
           0,
           0.25,
           0.08,
           0.25,
           -0.06,
           0.11,
           0.03,
           0.98,
           0.13,
           0.98,
           0.24,
           0.28,
           0.04,
           0.23,
           -0.11,
           0.94,
           0.21,
           0.99,
           0.96,
           1,
           -0.11,
           0.01,
           0.84,
           -0.01,
           -0.04,
           -0.04,
           0.08,
           -0.04,
           0.01,
           0.04,
           0.09,
           0.08,
           0.01,
           0.09,
           0.09,
           0.09,
           0.09,
           0.09,
           0.07,
           0.02,
           -0.01,
           -0.01,
           0.12,
           0.01,
           0.01,
           -0.01,
           0.07,
           0.02,
           0.01,
           0.05,
           0.06,
           0.05,
           0.1,
           0.04,
           0.01
          ],
          [
           -0.06,
           -0.13,
           -0.33,
           -0.09,
           -0.11,
           -0.11,
           -0.07,
           -0.04,
           -0.12,
           0.11,
           -0.13,
           0.01,
           -0.13,
           -0.04,
           -0.11,
           0.5,
           -0.24,
           -0.03,
           -0.1,
           -0.11,
           -0.11,
           1,
           0.28,
           -0.23,
           -0.04,
           -0.01,
           0.01,
           -0.05,
           0.06,
           -0.01,
           -0.07,
           -0.05,
           -0.05,
           0.08,
           -0.05,
           -0.04,
           -0.05,
           -0.05,
           -0.04,
           -0.06,
           -0.01,
           -0.01,
           -0.03,
           -0.02,
           -0.02,
           -0.03,
           0.02,
           0.01,
           0.01,
           0.01,
           0.01,
           0.02,
           0.01,
           -0.01,
           0.01,
           -0.01
          ],
          [
           0.02,
           -0.03,
           -0.1,
           -0.02,
           -0.02,
           0,
           -0.02,
           0.07,
           0,
           0.04,
           0,
           0.02,
           0.02,
           0.09,
           0.01,
           0.18,
           -0.07,
           -0.01,
           0.02,
           -0.01,
           0.01,
           0.28,
           1,
           -0.03,
           0.05,
           -0.05,
           -0.02,
           0.09,
           0.01,
           0.05,
           -0.01,
           0.09,
           0.06,
           0.04,
           0.08,
           0.08,
           0.08,
           0.08,
           0.09,
           0.07,
           0.04,
           -0.03,
           -0.01,
           0,
           0.02,
           -0.03,
           -0.02,
           0.02,
           0.02,
           0.02,
           0.02,
           0.02,
           0.02,
           0.01,
           0.02,
           0.03
          ],
          [
           0.24,
           0.2,
           0.3,
           0.25,
           0.2,
           -0.07,
           0.11,
           0.04,
           0.86,
           0.01,
           0.85,
           0.15,
           0.27,
           0.06,
           0.19,
           -0.22,
           0.83,
           0.17,
           0.85,
           0.81,
           0.84,
           -0.23,
           -0.03,
           1,
           0.06,
           -0.03,
           0,
           0.08,
           -0.03,
           0.02,
           0.03,
           0.07,
           0.06,
           -0.02,
           0.07,
           0.07,
           0.07,
           0.08,
           0.07,
           0.07,
           0.01,
           -0.01,
           0,
           0.09,
           0.01,
           0.01,
           -0.01,
           0.05,
           0.01,
           0,
           0.04,
           0.05,
           0.04,
           0.08,
           0.03,
           0
          ],
          [
           -0.01,
           0.02,
           0,
           0.09,
           -0.02,
           0,
           0,
           0.13,
           -0.01,
           -0.03,
           0,
           -0.09,
           0.01,
           0.13,
           0,
           -0.12,
           -0.01,
           -0.05,
           0,
           -0.02,
           -0.01,
           -0.04,
           0.05,
           0.06,
           1,
           -0.03,
           0.44,
           0.03,
           0.42,
           0.14,
           -0.42,
           -0.06,
           -0.14,
           0.14,
           -0.1,
           -0.15,
           -0.07,
           0.04,
           -0.05,
           0.01,
           0.26,
           -0.09,
           0.06,
           0.03,
           0.02,
           -0.06,
           -0.03,
           0.14,
           0.27,
           0.15,
           0.14,
           0.19,
           0.17,
           0.12,
           0.17,
           0.21
          ],
          [
           -0.08,
           -0.04,
           -0.04,
           0.09,
           -0.01,
           0,
           -0.07,
           -0.02,
           -0.03,
           -0.02,
           -0.01,
           0,
           -0.1,
           -0.01,
           -0.08,
           -0.01,
           -0.03,
           0,
           -0.05,
           -0.01,
           -0.04,
           -0.01,
           -0.05,
           -0.03,
           -0.03,
           1,
           0.22,
           -0.14,
           0.11,
           0.04,
           -0.14,
           -0.35,
           -0.44,
           -0.37,
           -0.41,
           -0.36,
           -0.44,
           -0.3,
           -0.51,
           -0.34,
           -0.11,
           0.76,
           0.07,
           -0.03,
           -0.38,
           -0.1,
           0.32,
           -0.35,
           -0.29,
           -0.56,
           -0.53,
           -0.37,
           -0.54,
           -0.24,
           -0.65,
           -0.08
          ],
          [
           -0.13,
           -0.07,
           -0.05,
           0.04,
           0,
           -0.02,
           -0.07,
           -0.09,
           -0.03,
           -0.04,
           -0.02,
           0,
           -0.11,
           -0.08,
           -0.09,
           -0.01,
           -0.04,
           0,
           -0.05,
           -0.02,
           -0.04,
           0.01,
           -0.02,
           0,
           0.44,
           0.22,
           1,
           -0.09,
           0.62,
           0.2,
           -0.53,
           -0.4,
           -0.51,
           -0.14,
           -0.45,
           -0.52,
           -0.44,
           -0.22,
           -0.43,
           -0.31,
           0.04,
           0.05,
           0.31,
           0.08,
           0.18,
           -0.05,
           -0.21,
           0.11,
           0.15,
           0.14,
           0.07,
           0.12,
           0.03,
           0.09,
           0.05,
           0.07
          ],
          [
           0.17,
           0.01,
           0.11,
           0.05,
           0.08,
           0.1,
           0.21,
           0.56,
           0.08,
           0.08,
           0.05,
           0,
           0.25,
           0.56,
           0.21,
           0.01,
           0.07,
           0.01,
           0.12,
           0.04,
           0.08,
           -0.05,
           0.09,
           0.08,
           0.03,
           -0.14,
           -0.09,
           1,
           -0.34,
           0.4,
           0.44,
           0.86,
           0.67,
           -0.38,
           0.81,
           0.78,
           0.82,
           0.9,
           0.76,
           0.79,
           0.29,
           0.06,
           0.18,
           0.16,
           0.14,
           0.02,
           -0.13,
           0.15,
           0.04,
           -0.02,
           0.07,
           0.09,
           0.09,
           0.24,
           0.08,
           0.12
          ],
          [
           -0.17,
           -0.05,
           -0.09,
           0.02,
           -0.02,
           -0.03,
           -0.14,
           -0.22,
           -0.04,
           -0.06,
           -0.03,
           0,
           -0.15,
           -0.2,
           -0.13,
           -0.01,
           -0.06,
           0,
           -0.06,
           -0.03,
           -0.04,
           0.06,
           0.01,
           -0.03,
           0.42,
           0.11,
           0.62,
           -0.34,
           1,
           0.43,
           -0.98,
           -0.61,
           -0.78,
           0.54,
           -0.6,
           -0.71,
           -0.6,
           -0.51,
           -0.51,
           -0.67,
           -0.05,
           -0.03,
           -0.11,
           0.03,
           0.14,
           -0.08,
           -0.23,
           0.16,
           0.13,
           0.25,
           0.12,
           0.21,
           0.11,
           0.05,
           0.11,
           -0.03
          ],
          [
           0.02,
           -0.02,
           0,
           0.04,
           0.03,
           0.02,
           0.03,
           0.12,
           0,
           0.01,
           0,
           0,
           0.06,
           0.13,
           0.04,
           0,
           0,
           0,
           0.03,
           -0.01,
           0.01,
           -0.01,
           0.05,
           0.02,
           0.14,
           0.04,
           0.2,
           0.4,
           0.43,
           1,
           -0.35,
           0.07,
           -0.15,
           0.21,
           0.12,
           -0.02,
           0.12,
           0.15,
           0.16,
           -0.02,
           0.07,
           0.11,
           -0.09,
           0.1,
           0.24,
           0.27,
           -0.23,
           0.12,
           -0.07,
           0.17,
           0.02,
           0.13,
           0.05,
           0.05,
           0.02,
           0.02
          ],
          [
           0.17,
           0.05,
           0.1,
           -0.01,
           0.03,
           0.04,
           0.16,
           0.24,
           0.04,
           0.06,
           0.03,
           0,
           0.16,
           0.21,
           0.14,
           0.01,
           0.06,
           0.01,
           0.06,
           0.02,
           0.04,
           -0.07,
           -0.01,
           0.03,
           -0.42,
           -0.14,
           -0.53,
           0.44,
           -0.98,
           -0.35,
           1,
           0.66,
           0.79,
           -0.61,
           0.63,
           0.72,
           0.64,
           0.57,
           0.53,
           0.7,
           0.09,
           -0.01,
           0.14,
           -0.03,
           -0.06,
           0.1,
           0.17,
           -0.13,
           -0.13,
           -0.19,
           -0.1,
           -0.21,
           -0.09,
           -0.03,
           -0.09,
           0.04
          ],
          [
           0.22,
           0.04,
           0.13,
           0.02,
           0.07,
           0.1,
           0.24,
           0.58,
           0.08,
           0.09,
           0.06,
           0,
           0.28,
           0.57,
           0.24,
           0.01,
           0.08,
           0.01,
           0.13,
           0.04,
           0.09,
           -0.05,
           0.09,
           0.07,
           -0.06,
           -0.35,
           -0.4,
           0.86,
           -0.61,
           0.07,
           0.66,
           1,
           0.93,
           -0.3,
           0.97,
           0.98,
           0.98,
           0.97,
           0.93,
           0.95,
           0.22,
           -0.1,
           0.11,
           0.09,
           0.15,
           0.1,
           -0.11,
           0.12,
           0.03,
           0.01,
           0.13,
           0.08,
           0.13,
           0.22,
           0.14,
           0.08
          ],
          [
           0.21,
           0.06,
           0.13,
           -0.02,
           0.05,
           0.08,
           0.22,
           0.46,
           0.08,
           0.08,
           0.05,
           0,
           0.25,
           0.44,
           0.21,
           0.01,
           0.08,
           0.01,
           0.12,
           0.04,
           0.08,
           -0.05,
           0.06,
           0.06,
           -0.14,
           -0.44,
           -0.51,
           0.67,
           -0.78,
           -0.15,
           0.79,
           0.93,
           1,
           -0.28,
           0.92,
           0.96,
           0.94,
           0.87,
           0.89,
           0.95,
           0.19,
           -0.15,
           0.13,
           0.07,
           0.14,
           0.19,
           -0.08,
           0.08,
           0.04,
           0.03,
           0.17,
           0.05,
           0.15,
           0.19,
           0.18,
           0.1
          ],
          [
           -0.06,
           0.03,
           -0.06,
           -0.07,
           -0.03,
           -0.04,
           -0.1,
           -0.24,
           0,
           -0.03,
           0,
           0,
           -0.06,
           -0.23,
           -0.06,
           0,
           -0.02,
           0,
           0,
           0,
           0.01,
           0.08,
           0.04,
           -0.02,
           0.14,
           -0.37,
           -0.14,
           -0.38,
           0.54,
           0.21,
           -0.61,
           -0.3,
           -0.28,
           1,
           -0.21,
           -0.26,
           -0.19,
           -0.33,
           -0.03,
           -0.38,
           -0.18,
           -0.25,
           -0.27,
           0.06,
           0.13,
           0.02,
           -0.14,
           0.24,
           0.14,
           0.38,
           0.26,
           0.31,
           0.26,
           0.13,
           0.3,
           -0.03
          ],
          [
           0.22,
           0.05,
           0.13,
           0.01,
           0.07,
           0.1,
           0.23,
           0.53,
           0.08,
           0.09,
           0.05,
           0,
           0.28,
           0.52,
           0.23,
           0.01,
           0.08,
           0.01,
           0.13,
           0.04,
           0.09,
           -0.05,
           0.08,
           0.07,
           -0.1,
           -0.41,
           -0.45,
           0.81,
           -0.6,
           0.12,
           0.63,
           0.97,
           0.92,
           -0.21,
           1,
           0.96,
           0.99,
           0.94,
           0.97,
           0.92,
           0.13,
           -0.08,
           0.05,
           0.08,
           0.15,
           0.16,
           -0.11,
           0.1,
           -0.03,
           0,
           0.17,
           0.07,
           0.13,
           0.19,
           0.15,
           0.01
          ],
          [
           0.22,
           0.05,
           0.13,
           0,
           0.06,
           0.1,
           0.23,
           0.53,
           0.08,
           0.09,
           0.06,
           0,
           0.27,
           0.52,
           0.23,
           0.01,
           0.08,
           0.01,
           0.13,
           0.05,
           0.09,
           -0.04,
           0.08,
           0.07,
           -0.15,
           -0.36,
           -0.52,
           0.78,
           -0.71,
           -0.02,
           0.72,
           0.98,
           0.96,
           -0.26,
           0.96,
           1,
           0.97,
           0.93,
           0.93,
           0.93,
           0.15,
           -0.08,
           0.11,
           0.08,
           0.1,
           0.07,
           -0.05,
           0.08,
           0,
           -0.03,
           0.11,
           0.08,
           0.1,
           0.2,
           0.12,
           0.04
          ],
          [
           0.22,
           0.05,
           0.13,
           0,
           0.07,
           0.1,
           0.23,
           0.53,
           0.08,
           0.09,
           0.06,
           0,
           0.28,
           0.52,
           0.23,
           0.01,
           0.08,
           0.01,
           0.13,
           0.04,
           0.09,
           -0.05,
           0.08,
           0.07,
           -0.07,
           -0.44,
           -0.44,
           0.82,
           -0.6,
           0.12,
           0.64,
           0.98,
           0.94,
           -0.19,
           0.99,
           0.97,
           1,
           0.95,
           0.98,
           0.93,
           0.18,
           -0.12,
           0.1,
           0.1,
           0.19,
           0.15,
           -0.12,
           0.14,
           0.02,
           0.04,
           0.18,
           0.1,
           0.18,
           0.23,
           0.19,
           0.06
          ],
          [
           0.2,
           0.03,
           0.13,
           0.03,
           0.07,
           0.1,
           0.23,
           0.58,
           0.09,
           0.08,
           0.06,
           0,
           0.27,
           0.58,
           0.23,
           0.01,
           0.08,
           0.01,
           0.13,
           0.05,
           0.09,
           -0.05,
           0.08,
           0.08,
           0.04,
           -0.3,
           -0.22,
           0.9,
           -0.51,
           0.15,
           0.57,
           0.97,
           0.87,
           -0.33,
           0.94,
           0.93,
           0.95,
           1,
           0.9,
           0.93,
           0.25,
           -0.07,
           0.17,
           0.17,
           0.18,
           0.1,
           -0.14,
           0.2,
           0.08,
           0.03,
           0.18,
           0.16,
           0.18,
           0.33,
           0.18,
           0.1
          ],
          [
           0.21,
           0.05,
           0.12,
           -0.01,
           0.06,
           0.09,
           0.22,
           0.48,
           0.08,
           0.08,
           0.06,
           0,
           0.26,
           0.47,
           0.22,
           0.01,
           0.08,
           0.01,
           0.13,
           0.04,
           0.09,
           -0.04,
           0.09,
           0.07,
           -0.05,
           -0.51,
           -0.43,
           0.76,
           -0.51,
           0.16,
           0.53,
           0.93,
           0.89,
           -0.03,
           0.97,
           0.93,
           0.98,
           0.9,
           1,
           0.88,
           0.12,
           -0.15,
           0.1,
           0.11,
           0.21,
           0.13,
           -0.15,
           0.17,
           0.03,
           0.09,
           0.21,
           0.17,
           0.2,
           0.26,
           0.25,
           0.04
          ],
          [
           0.21,
           0.05,
           0.13,
           0.01,
           0.07,
           0.1,
           0.23,
           0.56,
           0.07,
           0.09,
           0.05,
           0,
           0.26,
           0.54,
           0.22,
           0.01,
           0.08,
           0.01,
           0.11,
           0.04,
           0.07,
           -0.06,
           0.07,
           0.07,
           0.01,
           -0.34,
           -0.31,
           0.79,
           -0.67,
           -0.02,
           0.7,
           0.95,
           0.95,
           -0.38,
           0.92,
           0.93,
           0.93,
           0.93,
           0.88,
           1,
           0.26,
           -0.1,
           0.21,
           0.04,
           0.14,
           0.14,
           -0.08,
           0.03,
           0.05,
           0.01,
           0.11,
           0.01,
           0.1,
           0.15,
           0.12,
           0.18
          ],
          [
           0.01,
           -0.02,
           0.03,
           -0.02,
           0.02,
           0.03,
           0.04,
           0.16,
           0.02,
           0.02,
           0.01,
           0.01,
           0.04,
           0.15,
           0.04,
           0.01,
           0.02,
           0.01,
           0.02,
           0.02,
           0.02,
           -0.01,
           0.04,
           0.01,
           0.26,
           -0.11,
           0.04,
           0.29,
           -0.05,
           0.07,
           0.09,
           0.22,
           0.19,
           -0.18,
           0.13,
           0.15,
           0.18,
           0.25,
           0.12,
           0.26,
           1,
           -0.25,
           0.11,
           0.1,
           0.12,
           0.08,
           -0.07,
           0.28,
           0.48,
           0.26,
           0.12,
           0.17,
           0.32,
           0.29,
           0.27,
           0.51
          ],
          [
           0,
           -0.01,
           0,
           0.05,
           0.01,
           0.02,
           -0.01,
           0.13,
           0,
           0.01,
           0,
           0,
           -0.02,
           0.13,
           -0.02,
           0,
           -0.01,
           0,
           -0.02,
           0,
           -0.01,
           -0.01,
           -0.03,
           -0.01,
           -0.09,
           0.76,
           0.05,
           0.06,
           -0.03,
           0.11,
           -0.01,
           -0.1,
           -0.15,
           -0.25,
           -0.08,
           -0.08,
           -0.12,
           -0.07,
           -0.15,
           -0.1,
           -0.25,
           1,
           0.08,
           -0.04,
           -0.38,
           0.01,
           0.31,
           -0.49,
           -0.53,
           -0.73,
           -0.55,
           -0.45,
           -0.68,
           -0.3,
           -0.75,
           -0.24
          ],
          [
           0.02,
           0,
           0.02,
           -0.02,
           0.01,
           0,
           0.01,
           0.06,
           0,
           0.01,
           -0.01,
           0,
           0.01,
           0.05,
           0.03,
           0,
           0.01,
           0,
           0,
           0.01,
           -0.01,
           -0.03,
           -0.01,
           0,
           0.06,
           0.07,
           0.31,
           0.18,
           -0.11,
           -0.09,
           0.14,
           0.11,
           0.13,
           -0.27,
           0.05,
           0.11,
           0.1,
           0.17,
           0.1,
           0.21,
           0.11,
           0.08,
           1,
           0.06,
           0.18,
           -0.04,
           -0.17,
           -0.02,
           0.09,
           -0.04,
           -0.19,
           -0.01,
           -0.06,
           0.12,
           0.06,
           0.31
          ],
          [
           -0.01,
           -0.01,
           0,
           0.02,
           -0.01,
           -0.03,
           0,
           -0.07,
           0.12,
           -0.03,
           0.12,
           -0.01,
           0,
           -0.07,
           0,
           -0.02,
           0.09,
           -0.02,
           0.11,
           0.11,
           0.12,
           -0.02,
           0,
           0.09,
           0.03,
           -0.03,
           0.08,
           0.16,
           0.03,
           0.1,
           -0.03,
           0.09,
           0.07,
           0.06,
           0.08,
           0.08,
           0.1,
           0.17,
           0.11,
           0.04,
           0.1,
           -0.04,
           0.06,
           1,
           0.04,
           0.14,
           -0.02,
           0.69,
           0.47,
           0.08,
           0.52,
           0.62,
           0.54,
           0.78,
           0.4,
           0.3
          ],
          [
           0.02,
           0,
           0.02,
           -0.03,
           0.01,
           -0.02,
           0.02,
           -0.11,
           0.01,
           0.01,
           0,
           0,
           0.04,
           -0.11,
           0.04,
           0,
           0.01,
           0,
           0.02,
           0,
           0.01,
           -0.02,
           0.02,
           0.01,
           0.02,
           -0.38,
           0.18,
           0.14,
           0.14,
           0.24,
           -0.06,
           0.15,
           0.14,
           0.13,
           0.15,
           0.1,
           0.19,
           0.18,
           0.21,
           0.14,
           0.12,
           -0.38,
           0.18,
           0.04,
           1,
           0.46,
           -0.9,
           0.29,
           0.08,
           0.74,
           0.36,
           0.16,
           0.36,
           0.25,
           0.46,
           -0.09
          ],
          [
           0.04,
           0.02,
           0.01,
           -0.04,
           -0.02,
           -0.03,
           0.05,
           -0.15,
           0.01,
           0.01,
           0,
           0,
           0.03,
           -0.16,
           0.01,
           0,
           0.01,
           0,
           0.01,
           -0.01,
           0.01,
           -0.03,
           -0.03,
           0.01,
           -0.06,
           -0.1,
           -0.05,
           0.02,
           -0.08,
           0.27,
           0.1,
           0.1,
           0.19,
           0.02,
           0.16,
           0.07,
           0.15,
           0.1,
           0.13,
           0.14,
           0.08,
           0.01,
           -0.04,
           0.14,
           0.46,
           1,
           -0.4,
           0.11,
           -0.02,
           0.32,
           0.26,
           -0.01,
           0.21,
           0.08,
           0.2,
           0.09
          ],
          [
           -0.01,
           0.01,
           -0.02,
           0.02,
           -0.01,
           0.01,
           -0.01,
           0.04,
           -0.01,
           -0.01,
           0,
           0,
           -0.04,
           0.04,
           -0.03,
           0,
           0,
           0,
           -0.01,
           0,
           -0.01,
           0.02,
           -0.02,
           -0.01,
           -0.03,
           0.32,
           -0.21,
           -0.13,
           -0.23,
           -0.23,
           0.17,
           -0.11,
           -0.08,
           -0.14,
           -0.11,
           -0.05,
           -0.12,
           -0.14,
           -0.15,
           -0.08,
           -0.07,
           0.31,
           -0.17,
           -0.02,
           -0.9,
           -0.4,
           1,
           -0.23,
           -0.04,
           -0.67,
           -0.32,
           -0.15,
           -0.29,
           -0.2,
           -0.42,
           0.17
          ],
          [
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           -0.03,
           0,
           -0.16,
           0.06,
           -0.02,
           0.06,
           0,
           0.01,
           -0.16,
           0.01,
           -0.01,
           0.05,
           0,
           0.07,
           0.06,
           0.07,
           0.01,
           0.02,
           0.05,
           0.14,
           -0.35,
           0.11,
           0.15,
           0.16,
           0.12,
           -0.13,
           0.12,
           0.08,
           0.24,
           0.1,
           0.08,
           0.14,
           0.2,
           0.17,
           0.03,
           0.28,
           -0.49,
           -0.02,
           0.69,
           0.29,
           0.11,
           -0.23,
           1,
           0.78,
           0.52,
           0.79,
           0.86,
           0.9,
           0.91,
           0.78,
           0.34
          ],
          [
           -0.03,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           -0.03,
           -0.02,
           -0.14,
           0.02,
           -0.02,
           0.02,
           0,
           -0.02,
           -0.14,
           -0.03,
           -0.01,
           0.02,
           0,
           0.02,
           0.02,
           0.02,
           0.01,
           0.02,
           0.01,
           0.27,
           -0.29,
           0.15,
           0.04,
           0.13,
           -0.07,
           -0.13,
           0.03,
           0.04,
           0.14,
           -0.03,
           0,
           0.02,
           0.08,
           0.03,
           0.05,
           0.48,
           -0.53,
           0.09,
           0.47,
           0.08,
           -0.02,
           -0.04,
           0.78,
           1,
           0.43,
           0.58,
           0.76,
           0.78,
           0.68,
           0.66,
           0.68
          ],
          [
           -0.01,
           0.01,
           -0.01,
           -0.04,
           -0.01,
           -0.03,
           -0.01,
           -0.17,
           0,
           -0.01,
           0,
           0,
           0,
           -0.17,
           0,
           0,
           0,
           0,
           0.01,
           0,
           0.01,
           0.01,
           0.02,
           0,
           0.15,
           -0.56,
           0.14,
           -0.02,
           0.25,
           0.17,
           -0.19,
           0.01,
           0.03,
           0.38,
           0,
           -0.03,
           0.04,
           0.03,
           0.09,
           0.01,
           0.26,
           -0.73,
           -0.04,
           0.08,
           0.74,
           0.32,
           -0.67,
           0.52,
           0.43,
           1,
           0.56,
           0.43,
           0.65,
           0.37,
           0.76,
           0.17
          ],
          [
           0,
           -0.01,
           0.01,
           -0.03,
           0,
           -0.02,
           0.02,
           -0.1,
           0.04,
           -0.01,
           0.04,
           0,
           0.03,
           -0.11,
           0.01,
           0,
           0.03,
           0,
           0.05,
           0.03,
           0.05,
           0.01,
           0.02,
           0.04,
           0.14,
           -0.53,
           0.07,
           0.07,
           0.12,
           0.02,
           -0.1,
           0.13,
           0.17,
           0.26,
           0.17,
           0.11,
           0.18,
           0.18,
           0.21,
           0.11,
           0.12,
           -0.55,
           -0.19,
           0.52,
           0.36,
           0.26,
           -0.32,
           0.79,
           0.58,
           0.56,
           1,
           0.68,
           0.87,
           0.69,
           0.82,
           0.14
          ],
          [
           -0.03,
           -0.01,
           -0.01,
           -0.03,
           -0.01,
           -0.02,
           -0.01,
           -0.1,
           0.06,
           -0.02,
           0.06,
           0,
           0,
           -0.1,
           0,
           -0.01,
           0.04,
           0,
           0.07,
           0.06,
           0.06,
           0.02,
           0.02,
           0.05,
           0.19,
           -0.37,
           0.12,
           0.09,
           0.21,
           0.13,
           -0.21,
           0.08,
           0.05,
           0.31,
           0.07,
           0.08,
           0.1,
           0.16,
           0.17,
           0.01,
           0.17,
           -0.45,
           -0.01,
           0.62,
           0.16,
           -0.01,
           -0.15,
           0.86,
           0.76,
           0.43,
           0.68,
           1,
           0.8,
           0.83,
           0.73,
           0.32
          ],
          [
           0,
           0,
           0,
           -0.04,
           -0.01,
           -0.02,
           0.02,
           -0.11,
           0.04,
           -0.01,
           0.04,
           0,
           0.02,
           -0.12,
           0.01,
           -0.01,
           0.03,
           0,
           0.05,
           0.03,
           0.05,
           0.01,
           0.02,
           0.04,
           0.17,
           -0.54,
           0.03,
           0.09,
           0.11,
           0.05,
           -0.09,
           0.13,
           0.15,
           0.26,
           0.13,
           0.1,
           0.18,
           0.18,
           0.2,
           0.1,
           0.32,
           -0.68,
           -0.06,
           0.54,
           0.36,
           0.21,
           -0.29,
           0.9,
           0.78,
           0.65,
           0.87,
           0.8,
           1,
           0.79,
           0.9,
           0.37
          ],
          [
           0.01,
           -0.01,
           0,
           -0.01,
           0,
           -0.02,
           0.01,
           -0.05,
           0.1,
           -0.01,
           0.1,
           -0.01,
           0.02,
           -0.05,
           0.03,
           -0.01,
           0.08,
           0,
           0.1,
           0.09,
           0.1,
           -0.01,
           0.01,
           0.08,
           0.12,
           -0.24,
           0.09,
           0.24,
           0.05,
           0.05,
           -0.03,
           0.22,
           0.19,
           0.13,
           0.19,
           0.2,
           0.23,
           0.33,
           0.26,
           0.15,
           0.29,
           -0.3,
           0.12,
           0.78,
           0.25,
           0.08,
           -0.2,
           0.91,
           0.68,
           0.37,
           0.69,
           0.83,
           0.79,
           1,
           0.67,
           0.35
          ],
          [
           0,
           0,
           0,
           -0.05,
           -0.01,
           -0.02,
           0.02,
           -0.11,
           0.03,
           0,
           0.03,
           0,
           0.03,
           -0.12,
           0.01,
           0,
           0.03,
           0,
           0.04,
           0.02,
           0.04,
           0.01,
           0.02,
           0.03,
           0.17,
           -0.65,
           0.05,
           0.08,
           0.11,
           0.02,
           -0.09,
           0.14,
           0.18,
           0.3,
           0.15,
           0.12,
           0.19,
           0.18,
           0.25,
           0.12,
           0.27,
           -0.75,
           0.06,
           0.4,
           0.46,
           0.2,
           -0.42,
           0.78,
           0.66,
           0.76,
           0.82,
           0.73,
           0.9,
           0.67,
           1,
           0.24
          ],
          [
           0,
           0.01,
           0,
           -0.02,
           0,
           -0.01,
           -0.01,
           -0.02,
           0.01,
           0,
           0.01,
           0,
           -0.01,
           -0.02,
           0,
           0,
           0.01,
           0,
           0.01,
           0.02,
           0.01,
           -0.01,
           0.03,
           0,
           0.21,
           -0.08,
           0.07,
           0.12,
           -0.03,
           0.02,
           0.04,
           0.08,
           0.1,
           -0.03,
           0.01,
           0.04,
           0.06,
           0.1,
           0.04,
           0.18,
           0.51,
           -0.24,
           0.31,
           0.3,
           -0.09,
           0.09,
           0.17,
           0.34,
           0.68,
           0.17,
           0.14,
           0.32,
           0.37,
           0.35,
           0.24,
           1
          ]
         ],
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "MEAN",
          "STDEV",
          "BPS",
          "CPS",
          "CPX",
          "CSH",
          "DPS",
          "EBG",
          "EBI",
          "EBS",
          "EBT",
          "ENT",
          "EPS",
          "FFO",
          "GPS",
          "GRM",
          "NAV",
          "NDT",
          "NET",
          "OPR",
          "PRE",
          "ROA",
          "ROE",
          "SAL",
          "Real_Estate_Index_Price",
          "VIX_Close",
          "Gold_Close",
          "Three_Month_Yield",
          "Brent_Close",
          "Hrc_close",
          "commodity_trade_Close",
          "C_Discretionary_Close",
          "C_Staples_Close",
          "Energy_Close",
          "Financials_Close",
          "Health_care_Close",
          "industrials_Close",
          "information_Close",
          "materials_Close",
          "utilities_Close",
          "Real_Estate_Index_Price_quarterly_return",
          "VIX_Close_quarterly_return",
          "Gold_Close_quarterly_return",
          "Three_Month_Yield_quarterly_return",
          "Brent_Close_quarterly_return",
          "Hrc_close_quarterly_return",
          "commodity_trade_Close_quarterly_return",
          "C_Discretionary_Close_quarterly_return",
          "C_Staples_Close_quarterly_return",
          "Energy_Close_quarterly_return",
          "Financials_Close_quarterly_return",
          "Health_care_Close_quarterly_return",
          "industrials_Close_quarterly_return",
          "information_Close_quarterly_return",
          "materials_Close_quarterly_return",
          "utilities_Close_quarterly_return"
         ],
         "y": [
          "MEAN",
          "STDEV",
          "BPS",
          "CPS",
          "CPX",
          "CSH",
          "DPS",
          "EBG",
          "EBI",
          "EBS",
          "EBT",
          "ENT",
          "EPS",
          "FFO",
          "GPS",
          "GRM",
          "NAV",
          "NDT",
          "NET",
          "OPR",
          "PRE",
          "ROA",
          "ROE",
          "SAL",
          "Real_Estate_Index_Price",
          "VIX_Close",
          "Gold_Close",
          "Three_Month_Yield",
          "Brent_Close",
          "Hrc_close",
          "commodity_trade_Close",
          "C_Discretionary_Close",
          "C_Staples_Close",
          "Energy_Close",
          "Financials_Close",
          "Health_care_Close",
          "industrials_Close",
          "information_Close",
          "materials_Close",
          "utilities_Close",
          "Real_Estate_Index_Price_quarterly_return",
          "VIX_Close_quarterly_return",
          "Gold_Close_quarterly_return",
          "Three_Month_Yield_quarterly_return",
          "Brent_Close_quarterly_return",
          "Hrc_close_quarterly_return",
          "commodity_trade_Close_quarterly_return",
          "C_Discretionary_Close_quarterly_return",
          "C_Staples_Close_quarterly_return",
          "Energy_Close_quarterly_return",
          "Financials_Close_quarterly_return",
          "Health_care_Close_quarterly_return",
          "industrials_Close_quarterly_return",
          "information_Close_quarterly_return",
          "materials_Close_quarterly_return",
          "utilities_Close_quarterly_return"
         ],
         "z": [
          [
           1,
           0.8050784828381307,
           0.47090024504757505,
           0.32543339697168006,
           0.03016076051431954,
           0.013171222129199462,
           0.44213817663043536,
           0.12494006708004081,
           0.0646578026000109,
           0.22822758047954683,
           0.030522121684093705,
           0.07799366666960708,
           0.4989119648162343,
           0.1083638188250145,
           0.43887296208350635,
           -0.03444606750229962,
           0.0958342123122297,
           0.08583419893131386,
           0.08392567789673709,
           0.026063344376151206,
           0.05715960102988382,
           -0.06276754065978636,
           0.02089265512769004,
           0.23527588517752202,
           -0.008115822687070985,
           -0.07589700610776363,
           -0.12996693035759324,
           0.17491731404055139,
           -0.1694025690663006,
           0.020728248287895275,
           0.17405776567189132,
           0.21523916331755188,
           0.214222067936488,
           -0.05605604976988191,
           0.22123424063864597,
           0.2196591416020006,
           0.2203919861969848,
           0.1980457097321248,
           0.21408915895942204,
           0.20973454805427058,
           0.01484224259417869,
           -0.003342866092776462,
           0.02192118439307835,
           -0.013240205609424042,
           0.021308921855466872,
           0.039698078465774445,
           -0.0061619190199018545,
           -0.009661321905655648,
           -0.029884641455015522,
           -0.014402874416782258,
           -0.00019903505225027146,
           -0.026304472555636452,
           -0.004761710210729915,
           0.005898376841279733,
           0.004693298428085511,
           0.003992881619631378
          ],
          [
           0.8050784828381307,
           1,
           0.21786950929796076,
           0.24311218696735543,
           -0.0343638856113983,
           -0.0765629442230573,
           0.10639208874350391,
           0.019409384771374414,
           0.014303247231069965,
           -0.064068214311079,
           -0.008937434122555448,
           -0.02058507867263883,
           0.1632736538522807,
           0.012444030597724668,
           0.12452647315710294,
           -0.10562390190393958,
           0.05937972349113007,
           0.029609829293273247,
           0.01806544432947724,
           -0.018924226287299516,
           0.0033074123302717123,
           -0.13024783727799316,
           -0.031597279025333734,
           0.20142163169396596,
           0.022446080737654917,
           -0.03612070898470807,
           -0.07105545139170708,
           0.01146866028221214,
           -0.054764869491111104,
           -0.015233229041393424,
           0.04850212010719168,
           0.04277215661095484,
           0.058186839758136176,
           0.030714082830391315,
           0.049043670044639574,
           0.05436353984860677,
           0.050046211836822084,
           0.028371741350146366,
           0.05440170963806342,
           0.04518977397054094,
           -0.017671948903775683,
           -0.010208921986455215,
           0.004574288425679268,
           -0.014989918647526256,
           -0.0007663471459661916,
           0.019420517760908886,
           0.006750749472342707,
           -0.011452017281917476,
           -0.010640074957539544,
           0.006220338543099035,
           -0.005904586199871624,
           -0.013077435262994938,
           -0.004700426808261579,
           -0.006354683638137804,
           0.0029729629221145097,
           0.008514074924936183
          ],
          [
           0.47090024504757505,
           0.21786950929796076,
           1,
           0.2983248636902107,
           0.4833370905556069,
           0.13684607115616293,
           0.6671933245876446,
           0.08856281544401919,
           0.25977824175340936,
           0.5878429502635777,
           0.24720374126716574,
           0.5215378363485044,
           0.8650886919480705,
           0.07306922846436016,
           0.7299118666200722,
           -0.29232774948917156,
           0.3908938230121554,
           0.461231492112366,
           0.2662647977437249,
           0.25903984120665446,
           0.2507434198969878,
           -0.32952650860719573,
           -0.10139221803779914,
           0.2999426311801129,
           0.0005186314978819647,
           -0.040737641243157326,
           -0.045303032338727796,
           0.11016246275550752,
           -0.08944111056298266,
           0.002759721482153803,
           0.09501761241835162,
           0.13002644182809137,
           0.12548172698643945,
           -0.05579994107391595,
           0.12727480262554025,
           0.1269821462714237,
           0.12756328711370915,
           0.1251960322363448,
           0.119176374825793,
           0.13216959296555744,
           0.02819347362035758,
           -0.004238290560153136,
           0.021347859872489924,
           -0.00453075176311033,
           0.01958555987158994,
           0.014192513170354996,
           -0.016481048168796517,
           -0.008524934226266975,
           -0.01271394248038454,
           -0.005999125838318191,
           0.006122021087149214,
           -0.013910586855077645,
           -0.0004087927341493351,
           0.004952807977427503,
           0.004880707062188499,
           0.0024820047808740107
          ],
          [
           0.32543339697168006,
           0.24311218696735543,
           0.2983248636902107,
           1,
           -0.004306846861350403,
           0.06493143963313641,
           0.19301176477602547,
           0.10627810243619698,
           0.11830772822841824,
           0.062474729013915306,
           0.09839223569035165,
           0.015601332815957467,
           0.2547742683417634,
           0.09755570607971698,
           0.1990830235246377,
           -0.08129832996385303,
           0.11043986915695539,
           0.05781303938497864,
           0.07932513475954263,
           0.05783278399633828,
           0.08048062404764976,
           -0.09070523352696139,
           -0.02032469421004238,
           0.24910811284479054,
           0.09238104510822086,
           0.08519105638831735,
           0.043026534103082846,
           0.054625316681871434,
           0.019848876146946002,
           0.04132754413664121,
           -0.012320542563059804,
           0.01743717465551732,
           -0.020688290817673243,
           -0.06584778377910637,
           0.00947713084548406,
           0.002692746800971107,
           0.003736251165632428,
           0.03385414870308001,
           -0.006033322599852827,
           0.011147361231729613,
           -0.02024628436734244,
           0.04899244299203558,
           -0.016366759229025012,
           0.022816914316251723,
           -0.03258596185156092,
           -0.04071131634359541,
           0.02253409788596724,
           -0.013824051455925215,
           -0.014422480541239187,
           -0.03735058011746879,
           -0.030467070750304935,
           -0.025392011725204,
           -0.03609347212204058,
           -0.012419366819635489,
           -0.04634672169467193,
           -0.023165887229700595
          ],
          [
           0.03016076051431954,
           -0.0343638856113983,
           0.4833370905556069,
           -0.004306846861350403,
           1,
           0.00981614900937605,
           0.08804617878831508,
           0.041001517103188355,
           0.2356128443035433,
           0.569069794592603,
           0.24640017453568094,
           0.83153706897375,
           0.453978992929262,
           0.048991149788885205,
           0.34544920953165925,
           -0.09541325184715027,
           0.2971809321952692,
           0.6787462757420698,
           0.2554263534867414,
           0.2599026627641746,
           0.24672385521604345,
           -0.11034501800364377,
           -0.019664915323582974,
           0.20032399019983146,
           -0.023609281732385648,
           -0.012359109519320868,
           0.0015122759162525248,
           0.07750108765779828,
           -0.01992526121922691,
           0.029638394945205013,
           0.027075653646306472,
           0.06920073595342086,
           0.05179178183223123,
           -0.031678879527146216,
           0.06800645465435586,
           0.06134631946425617,
           0.06636271888239749,
           0.07245172342529724,
           0.06243543328621054,
           0.06687538477099265,
           0.018965149251168132,
           0.009087444520673211,
           0.012546450487575711,
           -0.008245823825335818,
           0.00650646685814862,
           -0.015199477915651363,
           -0.009917538808664745,
           -0.007293994738665724,
           -0.00977542325137606,
           -0.01177551222115849,
           -0.004561007634398637,
           -0.006425096854187829,
           -0.011128519823351368,
           -0.0031009246880221663,
           -0.008773424020452062,
           -0.0031774791447154054
          ],
          [
           0.013171222129199462,
           -0.0765629442230573,
           0.13684607115616293,
           0.06493143963313641,
           0.00981614900937605,
           1,
           0.11760182545454438,
           0.18002172892178026,
           -0.06380024479713148,
           -0.03494937179801078,
           -0.0683506731117304,
           -0.028903677580426737,
           0.1587010390967541,
           0.17130758694998435,
           0.12262711409571428,
           -0.10969355663046503,
           -0.05540989555221216,
           -0.0634632935066897,
           -0.0573542930551584,
           -0.06139909848476189,
           -0.060090967075131424,
           -0.11040140987157668,
           0.0034444738983196582,
           -0.07425444483663003,
           0.003245478381230237,
           -0.002461821586005459,
           -0.016410939859861164,
           0.1018203111130505,
           -0.0335514237215928,
           0.02195052883142534,
           0.035655845674097936,
           0.10454687554818487,
           0.0807504605610682,
           -0.03904006501743025,
           0.09513221353187987,
           0.0951731359223699,
           0.09518953331566979,
           0.10427125891233889,
           0.0872586950007477,
           0.10025061821630528,
           0.027074313503602364,
           0.02372049982950889,
           0.00390744488541775,
           -0.026571880450256637,
           -0.023845927494004673,
           -0.03258994489627726,
           0.00956064433140609,
           -0.034849815844088486,
           -0.026797747106930225,
           -0.03154534106614697,
           -0.01976875418184168,
           -0.020848003688982653,
           -0.024063325532010654,
           -0.017683149596033287,
           -0.022473640761804893,
           -0.005104505009606013
          ],
          [
           0.44213817663043536,
           0.10639208874350391,
           0.6671933245876446,
           0.19301176477602547,
           0.08804617878831508,
           0.11760182545454438,
           1,
           0.1375489457808459,
           0.10001353764629743,
           0.5372592343548269,
           0.08558804808735887,
           0.15515186698790845,
           0.7252134034878278,
           0.13950981982169713,
           0.6415760937049177,
           -0.07839434445220722,
           0.14872855051924588,
           0.1274133268056597,
           0.1365887245189296,
           0.09901062660302902,
           0.1148075684093623,
           -0.0749586759691843,
           -0.017135921932660163,
           0.10792440125864924,
           0.004752043655646494,
           -0.06760096004810069,
           -0.0742554178645739,
           0.20855976416468805,
           -0.14352671935124364,
           0.03013932400124241,
           0.15835725164241804,
           0.2364291541645233,
           0.21536068913273101,
           -0.09995865649911868,
           0.23174425710284394,
           0.2272710384809254,
           0.2307899320455023,
           0.23039443257050182,
           0.21552777828161315,
           0.22548120570341706,
           0.036528272428582836,
           -0.008827561785280828,
           0.00833460691373781,
           -0.0043837885634771855,
           0.023824428210294495,
           0.05025635243095818,
           -0.006696124780421833,
           -0.0010660599589879004,
           -0.023927148793197493,
           -0.014122846043596099,
           0.024860983705798166,
           -0.013491945816981443,
           0.019527067418685682,
           0.012229947856316448,
           0.022082512027255183,
           -0.012151855586079358
          ],
          [
           0.12494006708004081,
           0.019409384771374414,
           0.08856281544401919,
           0.10627810243619698,
           0.041001517103188355,
           0.18002172892178026,
           0.1375489457808459,
           1,
           0.03276214876367547,
           0.05050721060778833,
           0.021278909619267608,
           -0.022307187273628196,
           0.16490037512106895,
           0.9490836964864436,
           0.14770702034297928,
           -0.005803044108109921,
           0.03260732722944577,
           -0.007873327936502165,
           0.05654138985744664,
           0.006657421913089332,
           0.028326957433014638,
           -0.03604633100956447,
           0.07307394192474484,
           0.04437357386771557,
           0.12902470114714273,
           -0.015563149458167027,
           -0.08559460808343647,
           0.5647889458261961,
           -0.22088150363698486,
           0.11515993799539095,
           0.2381776548621473,
           0.5755096708250129,
           0.45849172375946384,
           -0.2430273931814662,
           0.5255899500133793,
           0.5287768831855216,
           0.5294174638417082,
           0.5832737839185593,
           0.48271389234138606,
           0.55719355765835,
           0.1550949026116112,
           0.1254922738683701,
           0.055024734526707295,
           -0.07008197052718575,
           -0.10567035975530474,
           -0.1472536794139184,
           0.038190008348639816,
           -0.16007931820663254,
           -0.14000501346261762,
           -0.16644997782208437,
           -0.09844877303348375,
           -0.09659285170717641,
           -0.11000779143346759,
           -0.045371030879774926,
           -0.11172518878033419,
           -0.017480581007640376
          ],
          [
           0.0646578026000109,
           0.014303247231069965,
           0.25977824175340936,
           0.11830772822841824,
           0.2356128443035433,
           -0.06380024479713148,
           0.10001353764629743,
           0.03276214876367547,
           1,
           0.08907363955165831,
           0.9827388095763052,
           0.19985265280783737,
           0.26906999463073067,
           0.0391749524638194,
           0.21779746190307753,
           -0.12099053094953799,
           0.927022091859663,
           0.18759007917002252,
           0.9722540405104204,
           0.9562781733325978,
           0.9825328581441753,
           -0.12287460077349945,
           0.0013205599453587442,
           0.8560299868939131,
           -0.00784237578101487,
           -0.027240571089424997,
           -0.03187826080102558,
           0.0754585456808525,
           -0.04373452305616672,
           0.003949177057719133,
           0.042237936821644166,
           0.08127348522522176,
           0.07602277165083447,
           -0.0026763248254370307,
           0.07904944115091408,
           0.08200639275498162,
           0.08132081759495405,
           0.08670793877758158,
           0.08236333264217684,
           0.06894893909530812,
           0.01828946561507515,
           -0.00289260308546518,
           -0.0024411846788910925,
           0.11776853467850759,
           0.007295810040806446,
           0.005450680723231817,
           -0.005864744360185743,
           0.06442111520183892,
           0.019137238478933727,
           -0.001027720451470733,
           0.04219704425659732,
           0.06085389261655755,
           0.04469546331052669,
           0.10066251474790867,
           0.030478311831685426,
           0.010220835121424498
          ],
          [
           0.22822758047954683,
           -0.064068214311079,
           0.5878429502635777,
           0.062474729013915306,
           0.569069794592603,
           -0.03494937179801078,
           0.5372592343548269,
           0.05050721060778833,
           0.08907363955165831,
           1,
           0.10596886852725541,
           0.7539724372217373,
           0.7014168515466613,
           0.03814046606459434,
           0.5831485549119798,
           0.16855190000048104,
           0.1430263933689944,
           0.6595533776251902,
           0.1341805044853191,
           0.14580668959160478,
           0.12927951744844288,
           0.11069889042836356,
           0.04215247008634757,
           0.008643757504631724,
           -0.025317907431899087,
           -0.022933433542121563,
           -0.036043776579837504,
           0.07733631770971555,
           -0.056132286702011196,
           0.009039300682583748,
           0.059457044887201584,
           0.08856988751618453,
           0.08225272268319163,
           -0.03290849242843754,
           0.08730288551978482,
           0.08616579041321265,
           0.08709844291141079,
           0.08471028211030428,
           0.08192548750007889,
           0.08624745502174554,
           0.020781117219353707,
           0.005132697836497742,
           0.009651524284567893,
           -0.025016385836222177,
           0.0057877620742679305,
           0.005634339081640727,
           -0.005159270789444962,
           -0.01572513335352094,
           -0.01949230065441609,
           -0.010455338075208648,
           -0.005872240686077308,
           -0.017592580608633987,
           -0.008930632046425422,
           -0.006339524558146231,
           -0.00389384954542584,
           -0.004839973992381817
          ],
          [
           0.030522121684093705,
           -0.008937434122555448,
           0.24720374126716574,
           0.09839223569035165,
           0.24640017453568094,
           -0.0683506731117304,
           0.08558804808735887,
           0.021278909619267608,
           0.9827388095763052,
           0.10596886852725541,
           1,
           0.2220577927378363,
           0.2450187862036798,
           0.02444298746061328,
           0.19128926644908922,
           -0.11930347851469747,
           0.9343741040261867,
           0.20694550807380246,
           0.9677424680507957,
           0.9704944498569561,
           0.9778965689394695,
           -0.12564632157217392,
           -0.002307991156748099,
           0.8478752191025352,
           -0.0007468216974063289,
           -0.01308484503361983,
           -0.020295823199406877,
           0.0523481253223899,
           -0.02940006979393982,
           -0.002836825237181664,
           0.025911927430619642,
           0.056753516480384784,
           0.053221810172827115,
           0.0013509025355115463,
           0.05429827293925442,
           0.05809480379216063,
           0.05612265236126545,
           0.06458545663499728,
           0.058245539709238736,
           0.04569584782750142,
           0.012252381567450251,
           -0.001036260614158971,
           -0.007103924524114475,
           0.11737808756666541,
           -0.002179134317563191,
           0.0032130157972044596,
           0.0019212315072620275,
           0.06398234615456934,
           0.022817868467389257,
           -0.0033206594442549086,
           0.0422942574940295,
           0.06330001273480196,
           0.04429569193119272,
           0.09978940536400055,
           0.02995498746034487,
           0.00716592228209776
          ],
          [
           0.07799366666960708,
           -0.02058507867263883,
           0.5215378363485044,
           0.015601332815957467,
           0.83153706897375,
           -0.028903677580426737,
           0.15515186698790845,
           -0.022307187273628196,
           0.19985265280783737,
           0.7539724372217373,
           0.2220577927378363,
           1,
           0.4952415601486471,
           -0.025682833769441533,
           0.37245353739589165,
           0.022537127389571216,
           0.27811457295698133,
           0.8892320194531579,
           0.23942083658572996,
           0.253719444272671,
           0.23759706535609323,
           0.009566997262378937,
           0.016299664099668345,
           0.15130151465453684,
           -0.08834343781663818,
           -0.0017063167081923653,
           -0.0013985440923853178,
           -0.0014791837473654196,
           -0.00010207631435353367,
           0.00022595530924144063,
           0.00011640107190988665,
           -0.00045291324350510626,
           0.00001042347936008765,
           -0.000007266868267057065,
           0.0003346262597932895,
           -0.0005182861653578059,
           -0.00003174304143950264,
           -0.001729962564359082,
           -0.00006203325860943357,
           0.000053577315480782955,
           0.014752401399160541,
           -0.00027712570250665677,
           -0.0006440174894136099,
           -0.01229541138433293,
           -0.00013121205812792095,
           -0.0004404069641473495,
           -0.00014824737081160975,
           -0.004185356744100419,
           -0.0018867072957929481,
           -0.00010497566864086708,
           -0.0021190393965980466,
           -0.003611728959811668,
           -0.002636238153640449,
           -0.006758970914903812,
           -0.001493942114388403,
           -0.0011170199638208826
          ],
          [
           0.4989119648162343,
           0.1632736538522807,
           0.8650886919480705,
           0.2547742683417634,
           0.453978992929262,
           0.1587010390967541,
           0.7252134034878278,
           0.16490037512106895,
           0.26906999463073067,
           0.7014168515466613,
           0.2450187862036798,
           0.4952415601486471,
           1,
           0.149833412010168,
           0.832397236285469,
           -0.08313008871587525,
           0.29595592196979925,
           0.43327738330977617,
           0.31452465245561806,
           0.2434672689060011,
           0.2786490823208669,
           -0.1346987557022522,
           0.016266709454054257,
           0.27310606873193877,
           0.014386171210772064,
           -0.10221930753566702,
           -0.10939819932860087,
           0.24887133356270022,
           -0.14826832679495175,
           0.057253380814684396,
           0.16280590827735067,
           0.28005718745126057,
           0.25045946807852526,
           -0.06424792663631743,
           0.27699153729037285,
           0.2703713723523952,
           0.27579346563097196,
           0.2677905477779262,
           0.2646717188282555,
           0.2594756573035441,
           0.04100822605811792,
           -0.01870822104812203,
           0.006567917223040462,
           -0.0014105665768239524,
           0.043027451408785854,
           0.02874108750380335,
           -0.036645354301327,
           0.011909854018751602,
           -0.02270592142829439,
           0.004583575472181388,
           0.032489627351544974,
           -0.00023266663544093614,
           0.019641962409467334,
           0.024252441643988397,
           0.025816393067269874,
           -0.010644984881431056
          ],
          [
           0.1083638188250145,
           0.012444030597724668,
           0.07306922846436016,
           0.09755570607971698,
           0.048991149788885205,
           0.17130758694998435,
           0.13950981982169713,
           0.9490836964864436,
           0.0391749524638194,
           0.03814046606459434,
           0.02444298746061328,
           -0.025682833769441533,
           0.149833412010168,
           1,
           0.13315588023216857,
           -0.004621827753585339,
           0.04040210692956116,
           -0.019500636121366297,
           0.06433077122235008,
           0.014126687385064068,
           0.037316277755701156,
           -0.04121078079903593,
           0.08504355851016003,
           0.05534190379007975,
           0.1347334580714346,
           -0.0056214327353572655,
           -0.07721652416720919,
           0.5634695062999386,
           -0.1979398981199328,
           0.12753158525842148,
           0.21465874676277452,
           0.5671518149371741,
           0.44123885583732114,
           -0.2317488918352337,
           0.5161820304714851,
           0.5184930639424177,
           0.5190109849484688,
           0.576146528819173,
           0.4749579969619134,
           0.5443285476388379,
           0.14975956215526648,
           0.13186444415900966,
           0.05296544069586066,
           -0.07186754026764917,
           -0.1086725676152516,
           -0.16093370015678019,
           0.03558415191554809,
           -0.1643461954064535,
           -0.14203821275094067,
           -0.16790805280879398,
           -0.11103706456007086,
           -0.0960554978539139,
           -0.12170813084872821,
           -0.05153910627177115,
           -0.11967333330314274,
           -0.021548458879295285
          ],
          [
           0.43887296208350635,
           0.12452647315710294,
           0.7299118666200722,
           0.1990830235246377,
           0.34544920953165925,
           0.12262711409571428,
           0.6415760937049177,
           0.14770702034297928,
           0.21779746190307753,
           0.5831485549119798,
           0.19128926644908922,
           0.37245353739589165,
           0.832397236285469,
           0.13315588023216857,
           1,
           -0.07050895633659462,
           0.2435202224267814,
           0.31956606674685845,
           0.2530476600978981,
           0.19850451212339762,
           0.22691977095424465,
           -0.1080362198102224,
           0.009560404841950861,
           0.1948254257559439,
           -0.0037707455572643708,
           -0.0768110780618754,
           -0.08732394535450695,
           0.2125349176167841,
           -0.12929025834891328,
           0.042221382880769744,
           0.1417341536750063,
           0.2378146691261771,
           0.21131721324060315,
           -0.062310041136743,
           0.22895984177838283,
           0.2307728620891947,
           0.23174879440138035,
           0.23132810902269507,
           0.22002185832600432,
           0.22191320520309119,
           0.04149190941934147,
           -0.015394952757898699,
           0.03101663446619098,
           -0.0017750630181504437,
           0.03926510864712119,
           0.007545279777187934,
           -0.02915151935333705,
           0.007780567960430482,
           -0.025239873670873048,
           -0.002236307312637326,
           0.005813373310463084,
           0.003349831564559948,
           0.010682993725853566,
           0.03254229736169695,
           0.012682677799700282,
           -0.0017910170172579995
          ],
          [
           -0.03444606750229962,
           -0.10562390190393958,
           -0.29232774948917156,
           -0.08129832996385303,
           -0.09541325184715027,
           -0.10969355663046503,
           -0.07839434445220722,
           -0.005803044108109921,
           -0.12099053094953799,
           0.16855190000048104,
           -0.11930347851469747,
           0.022537127389571216,
           -0.08313008871587525,
           -0.004621827753585339,
           -0.07050895633659462,
           1,
           -0.22074413250768551,
           -0.023520061486634672,
           -0.10659409140954529,
           -0.09367648135926786,
           -0.10755713328940839,
           0.5019209473211795,
           0.1828715127959909,
           -0.21535695972677793,
           -0.1190509863979263,
           -0.0051355796489571965,
           -0.005350839009756469,
           0.005271324947218961,
           -0.007211115867403615,
           -0.001398724847399598,
           0.007507673946448987,
           0.008776232091046717,
           0.009166276681429768,
           -0.0038795913109410454,
           0.00941407816742992,
           0.008604423322011707,
           0.00866695270213318,
           0.006225811532693846,
           0.00791273650293216,
           0.009532619113872907,
           0.014787524756816866,
           0.00036588600825235117,
           -0.0006601250164623818,
           -0.018609834709993795,
           -0.001852596533776769,
           -0.0008617088317452627,
           0.0007555059718743442,
           -0.008523292950110532,
           -0.005500255998286456,
           -0.0016655711098748165,
           -0.004359177836890809,
           -0.006717753296249841,
           -0.0054662469475318665,
           -0.010892949258435642,
           -0.003185979960957675,
           -0.0018755873998766994
          ],
          [
           0.0958342123122297,
           0.05937972349113007,
           0.3908938230121554,
           0.11043986915695539,
           0.2971809321952692,
           -0.05540989555221216,
           0.14872855051924588,
           0.03260732722944577,
           0.927022091859663,
           0.1430263933689944,
           0.9343741040261867,
           0.27811457295698133,
           0.29595592196979925,
           0.04040210692956116,
           0.2435202224267814,
           -0.22074413250768551,
           1,
           0.2490454882483616,
           0.9315952619920731,
           0.9373472727387705,
           0.9351114494508371,
           -0.24010990403257393,
           -0.06738618050917204,
           0.8313220711089693,
           -0.005683147111129104,
           -0.026534941403118333,
           -0.035768501100243245,
           0.06984811346138481,
           -0.0570677251689698,
           -0.0041297649577806085,
           0.05546771383301571,
           0.08163263009899414,
           0.0813403943766023,
           -0.01599250394631731,
           0.07863770064450064,
           0.08321116566784321,
           0.08154379883557229,
           0.08413257463018523,
           0.08041984378525291,
           0.07612540200277036,
           0.019053689763067982,
           -0.0067653139761942715,
           0.009899138978960114,
           0.08789466854672587,
           0.007846096465308343,
           0.009330724577449502,
           -0.004545546580670676,
           0.046038219302715344,
           0.015685027572562727,
           -0.0007685020919298283,
           0.03037290077640823,
           0.041999873910593004,
           0.034901781346080456,
           0.07706806364099333,
           0.026413948092432117,
           0.014260080284706335
          ],
          [
           0.08583419893131386,
           0.029609829293273247,
           0.461231492112366,
           0.05781303938497864,
           0.6787462757420698,
           -0.0634632935066897,
           0.1274133268056597,
           -0.007873327936502165,
           0.18759007917002252,
           0.6595533776251902,
           0.20694550807380246,
           0.8892320194531579,
           0.43327738330977617,
           -0.019500636121366297,
           0.31956606674685845,
           -0.023520061486634672,
           0.2490454882483616,
           1,
           0.20727611322478756,
           0.2177553513974179,
           0.20852776557072433,
           -0.03247650601032644,
           -0.006084931379063212,
           0.16578162873436258,
           -0.049767084570440503,
           -0.0027101546983417614,
           -0.0026854919581002666,
           0.008638491397458245,
           -0.004811607696752734,
           0.0020543542638882745,
           0.00565480732758031,
           0.008608961651620369,
           0.007812650201691963,
           -0.004070362179693465,
           0.008589368035587202,
           0.00801911179205245,
           0.00878059867286731,
           0.00828431634907061,
           0.008422004118264317,
           0.00794040755881214,
           0.010782671532921293,
           0.0003085398510986077,
           0.0020824074181408866,
           -0.023451763461548487,
           0.0006752437886069413,
           -0.0010142391845376275,
           -0.0003175278337418933,
           -0.0030013523800118407,
           -0.0019524088104962405,
           -0.0015818622783690206,
           -0.003235550135673405,
           -0.0025809939460106355,
           -0.002645752295054137,
           -0.002377831149621098,
           -0.001851309455635211,
           0.0005783593320193341
          ],
          [
           0.08392567789673709,
           0.01806544432947724,
           0.2662647977437249,
           0.07932513475954263,
           0.2554263534867414,
           -0.0573542930551584,
           0.1365887245189296,
           0.05654138985744664,
           0.9722540405104204,
           0.1341805044853191,
           0.9677424680507957,
           0.23942083658572996,
           0.31452465245561806,
           0.06433077122235008,
           0.2530476600978981,
           -0.10659409140954529,
           0.9315952619920731,
           0.20727611322478756,
           1,
           0.9470809131698235,
           0.991400946662585,
           -0.09930655840242307,
           0.016865117292890707,
           0.8452235404586226,
           -0.002151415032495119,
           -0.05386145220818655,
           -0.053441322656180686,
           0.11855510396183054,
           -0.05805058909968625,
           0.02668633722711934,
           0.05969632909564368,
           0.12956748963077683,
           0.11525665073321731,
           0.0025094393477845312,
           0.12663679062588024,
           0.1273142317619763,
           0.12947618789237011,
           0.13125571871225175,
           0.13070632673193122,
           0.10962815153883401,
           0.0210604834316476,
           -0.016971435780155475,
           -0.004011531998912033,
           0.11044433203755546,
           0.017353174706591836,
           0.008260083614467808,
           -0.01479197971411378,
           0.07059114207866082,
           0.01793221200925467,
           0.009820495505211616,
           0.049906829668761546,
           0.06543579657396785,
           0.05321498700348118,
           0.10145393149389736,
           0.0422663183145503,
           0.006410765624076502
          ],
          [
           0.026063344376151206,
           -0.018924226287299516,
           0.25903984120665446,
           0.05783278399633828,
           0.2599026627641746,
           -0.06139909848476189,
           0.09901062660302902,
           0.006657421913089332,
           0.9562781733325978,
           0.14580668959160478,
           0.9704944498569561,
           0.253719444272671,
           0.2434672689060011,
           0.014126687385064068,
           0.19850451212339762,
           -0.09367648135926786,
           0.9373472727387705,
           0.2177553513974179,
           0.9470809131698235,
           1,
           0.960981507145402,
           -0.10937620508580093,
           -0.007328080583192041,
           0.8087656132141688,
           -0.024567990022463553,
           -0.00940623025155737,
           -0.016378626410703992,
           0.04204831049925436,
           -0.02653880565798484,
           -0.006017294324750368,
           0.023661507275328685,
           0.04400992491058606,
           0.04205064618393252,
           -0.0024151681336341377,
           0.038979974101920147,
           0.04515319956015765,
           0.04278507726292092,
           0.051323060737403996,
           0.04371831326874511,
           0.03617664217545269,
           0.01800024468282453,
           -0.0046786178907434976,
           0.00532716084979277,
           0.11219519054222009,
           -0.00006908122321930606,
           -0.00715805932009112,
           -0.0003332164749933805,
           0.05504483663161365,
           0.022341445716498907,
           -0.004471407658047546,
           0.025654905222241967,
           0.05590274897333009,
           0.034946234245623535,
           0.09147678171246036,
           0.021613642460130745,
           0.015526279669654123
          ],
          [
           0.05715960102988382,
           0.0033074123302717123,
           0.2507434198969878,
           0.08048062404764976,
           0.24672385521604345,
           -0.060090967075131424,
           0.1148075684093623,
           0.028326957433014638,
           0.9825328581441753,
           0.12927951744844288,
           0.9778965689394695,
           0.23759706535609323,
           0.2786490823208669,
           0.037316277755701156,
           0.22691977095424465,
           -0.10755713328940839,
           0.9351114494508371,
           0.20852776557072433,
           0.991400946662585,
           0.960981507145402,
           1,
           -0.10611298287252106,
           0.007958887040815156,
           0.8406139629151668,
           -0.014326722130355374,
           -0.04035446927297504,
           -0.04481560756277988,
           0.07517279887504706,
           -0.04278048675172523,
           0.007166811858396961,
           0.040275235754677376,
           0.08735676251789809,
           0.0824000570437074,
           0.014522521320475852,
           0.08612713597474983,
           0.08862576270801625,
           0.08914537616999445,
           0.08955517294859824,
           0.09224135363935175,
           0.07161579885762054,
           0.017173305081369928,
           -0.012941417125253705,
           -0.008062369613235365,
           0.11572468633835511,
           0.012948840998605516,
           0.00657359297647599,
           -0.010072609159906744,
           0.06842903820139813,
           0.019340665511081413,
           0.0078222800028322,
           0.047175258522891846,
           0.06332142980411716,
           0.050486093552641005,
           0.10106064567962264,
           0.03711077599085009,
           0.005606756809990386
          ],
          [
           -0.06276754065978636,
           -0.13024783727799316,
           -0.32952650860719573,
           -0.09070523352696139,
           -0.11034501800364377,
           -0.11040140987157668,
           -0.0749586759691843,
           -0.03604633100956447,
           -0.12287460077349945,
           0.11069889042836356,
           -0.12564632157217392,
           0.009566997262378937,
           -0.1346987557022522,
           -0.04121078079903593,
           -0.1080362198102224,
           0.5019209473211795,
           -0.24010990403257393,
           -0.03247650601032644,
           -0.09930655840242307,
           -0.10937620508580093,
           -0.10611298287252106,
           1,
           0.2832107861386245,
           -0.23304908341110425,
           -0.038522793765031434,
           -0.007415615126974019,
           0.006418669816604019,
           -0.05259269356857626,
           0.06243550489670563,
           -0.010296588738321477,
           -0.07117177197988575,
           -0.04714824520432729,
           -0.04905885052854838,
           0.0760129144648182,
           -0.04633768314173026,
           -0.0446592274927523,
           -0.046908451178234334,
           -0.05040110102668377,
           -0.037094133469945244,
           -0.055117842793691675,
           -0.011500240650629047,
           -0.011836686965584979,
           -0.03345893641919585,
           -0.017396773589844775,
           -0.022448837512780224,
           -0.025302210533206377,
           0.015962405896697753,
           0.008150303359763349,
           0.013620637164737765,
           0.009630322717504003,
           0.012579009244469419,
           0.018418160519771905,
           0.00876364967080746,
           -0.006050192766825649,
           0.008388494672463259,
           -0.005843848579420259
          ],
          [
           0.02089265512769004,
           -0.031597279025333734,
           -0.10139221803779914,
           -0.02032469421004238,
           -0.019664915323582974,
           0.0034444738983196582,
           -0.017135921932660163,
           0.07307394192474484,
           0.0013205599453587442,
           0.04215247008634757,
           -0.002307991156748099,
           0.016299664099668345,
           0.016266709454054257,
           0.08504355851016003,
           0.009560404841950861,
           0.1828715127959909,
           -0.06738618050917204,
           -0.006084931379063212,
           0.016865117292890707,
           -0.007328080583192041,
           0.007958887040815156,
           0.2832107861386245,
           1,
           -0.029799545200306615,
           0.047764978451129106,
           -0.05444310381723607,
           -0.02143804295442268,
           0.09246713368325743,
           0.013579677465961924,
           0.04911146705368587,
           -0.009728950409117478,
           0.08681994456212859,
           0.05908489327677447,
           0.03930266937080006,
           0.07943881771528555,
           0.07519713367570548,
           0.08242504805394245,
           0.08345458495128134,
           0.08536268718883964,
           0.07412288924067655,
           0.04315231286257555,
           -0.02636106994424967,
           -0.012406887324206747,
           -0.002570270096137871,
           0.016666047976430794,
           -0.033421759454455566,
           -0.021421078419625473,
           0.017338710841185937,
           0.02389200764444259,
           0.023319695016431777,
           0.017850470586752976,
           0.018679349220232285,
           0.018546044891099206,
           0.012924756741626239,
           0.021522960884135755,
           0.028373072487753396
          ],
          [
           0.23527588517752202,
           0.20142163169396596,
           0.2999426311801129,
           0.24910811284479054,
           0.20032399019983146,
           -0.07425444483663003,
           0.10792440125864924,
           0.04437357386771557,
           0.8560299868939131,
           0.008643757504631724,
           0.8478752191025352,
           0.15130151465453684,
           0.27310606873193877,
           0.05534190379007975,
           0.1948254257559439,
           -0.21535695972677793,
           0.8313220711089693,
           0.16578162873436258,
           0.8452235404586226,
           0.8087656132141688,
           0.8406139629151668,
           -0.23304908341110425,
           -0.029799545200306615,
           1,
           0.06357228055281873,
           -0.02640361038666518,
           -0.0028540364603386143,
           0.07705613038141887,
           -0.029881146787594737,
           0.01640388998743268,
           0.034199247219551025,
           0.07447119513804648,
           0.06407046027858136,
           -0.01764744150216741,
           0.07167078666731302,
           0.07043886806775787,
           0.0727387860902134,
           0.08269037504049404,
           0.0726109623840008,
           0.06543012594895929,
           0.012979436240186638,
           -0.006419945901678516,
           -0.0004720770722885624,
           0.08903491764448616,
           0.010851003493069474,
           0.01068543348487271,
           -0.008266675875604708,
           0.0528977317692586,
           0.014523476366802547,
           0.0019123676064138177,
           0.04111984270461193,
           0.048505432718014,
           0.04095806845674561,
           0.0768010551418292,
           0.03061626014660704,
           0.0038681540476782957
          ],
          [
           -0.008115822687070985,
           0.022446080737654917,
           0.0005186314978819647,
           0.09238104510822086,
           -0.023609281732385648,
           0.003245478381230237,
           0.004752043655646494,
           0.12902470114714273,
           -0.00784237578101487,
           -0.025317907431899087,
           -0.0007468216974063289,
           -0.08834343781663818,
           0.014386171210772064,
           0.1347334580714346,
           -0.0037707455572643708,
           -0.1190509863979263,
           -0.005683147111129104,
           -0.049767084570440503,
           -0.002151415032495119,
           -0.024567990022463553,
           -0.014326722130355374,
           -0.038522793765031434,
           0.047764978451129106,
           0.06357228055281873,
           1,
           -0.030953230442729716,
           0.44258742058076983,
           0.03373366241247356,
           0.42202750022099067,
           0.13627040121112163,
           -0.4191689104973375,
           -0.06176930230689434,
           -0.13644080100893294,
           0.13636066524703577,
           -0.09553402787418426,
           -0.14921858254907056,
           -0.07338777444352214,
           0.036939648028959104,
           -0.0458474224453918,
           0.01380058324167279,
           0.2641154038445176,
           -0.0897661640745926,
           0.06054454541368402,
           0.027188777512967565,
           0.02039576223600634,
           -0.06302157361333345,
           -0.03426086606716991,
           0.1360357294514458,
           0.27227219159349436,
           0.15358149398143697,
           0.14320164196600474,
           0.19162675036919333,
           0.16816977088910287,
           0.12243571774092732,
           0.17346481425184884,
           0.207565914719494
          ],
          [
           -0.07589700610776363,
           -0.03612070898470807,
           -0.040737641243157326,
           0.08519105638831735,
           -0.012359109519320868,
           -0.002461821586005459,
           -0.06760096004810069,
           -0.015563149458167027,
           -0.027240571089424997,
           -0.022933433542121563,
           -0.01308484503361983,
           -0.0017063167081923653,
           -0.10221930753566702,
           -0.0056214327353572655,
           -0.0768110780618754,
           -0.0051355796489571965,
           -0.026534941403118333,
           -0.0027101546983417614,
           -0.05386145220818655,
           -0.00940623025155737,
           -0.04035446927297504,
           -0.007415615126974019,
           -0.05444310381723607,
           -0.02640361038666518,
           -0.030953230442729716,
           1,
           0.21854296308971952,
           -0.14030595234304444,
           0.11183991369090285,
           0.037933890480212444,
           -0.13929386967371288,
           -0.346723651090091,
           -0.43853464131516495,
           -0.3698530982921998,
           -0.4124793485936792,
           -0.35727367958822637,
           -0.43609251604862936,
           -0.30047524920292484,
           -0.5120872343995113,
           -0.3440423185860083,
           -0.10793726145692246,
           0.7588845132333762,
           0.07204915416438476,
           -0.026503972550133176,
           -0.37805429974768195,
           -0.09889587498205865,
           0.31752074194945207,
           -0.34788527774308153,
           -0.28893376896377315,
           -0.5554242288810061,
           -0.5279869423443261,
           -0.3683336530860664,
           -0.5374462556698194,
           -0.23603354350553982,
           -0.6512453554738035,
           -0.07780672475004331
          ],
          [
           -0.12996693035759324,
           -0.07105545139170708,
           -0.045303032338727796,
           0.043026534103082846,
           0.0015122759162525248,
           -0.016410939859861164,
           -0.0742554178645739,
           -0.08559460808343647,
           -0.03187826080102558,
           -0.036043776579837504,
           -0.020295823199406877,
           -0.0013985440923853178,
           -0.10939819932860087,
           -0.07721652416720919,
           -0.08732394535450695,
           -0.005350839009756469,
           -0.035768501100243245,
           -0.0026854919581002666,
           -0.053441322656180686,
           -0.016378626410703992,
           -0.04481560756277988,
           0.006418669816604019,
           -0.02143804295442268,
           -0.0028540364603386143,
           0.44258742058076983,
           0.21854296308971952,
           1,
           -0.08698179613976788,
           0.6172313619433962,
           0.20293560262361449,
           -0.5288879796856472,
           -0.4043825468749695,
           -0.5119900708116529,
           -0.14364274605307,
           -0.4467636941148178,
           -0.5153869027207505,
           -0.4420361010048875,
           -0.21738346794254929,
           -0.42904228412093154,
           -0.30656650011309805,
           0.04450572972266792,
           0.04871862242353827,
           0.31324767026359734,
           0.07809586100901798,
           0.1768116026289732,
           -0.05215217894334675,
           -0.21151163623726518,
           0.1085173737359561,
           0.1466031602183782,
           0.13967167914886391,
           0.07062250966159672,
           0.12294561302067354,
           0.0334311927427403,
           0.09080136712749994,
           0.05314093057312371,
           0.06558529358289407
          ],
          [
           0.17491731404055139,
           0.01146866028221214,
           0.11016246275550752,
           0.054625316681871434,
           0.07750108765779828,
           0.1018203111130505,
           0.20855976416468805,
           0.5647889458261961,
           0.0754585456808525,
           0.07733631770971555,
           0.0523481253223899,
           -0.0014791837473654196,
           0.24887133356270022,
           0.5634695062999386,
           0.2125349176167841,
           0.005271324947218961,
           0.06984811346138481,
           0.008638491397458245,
           0.11855510396183054,
           0.04204831049925436,
           0.07517279887504706,
           -0.05259269356857626,
           0.09246713368325743,
           0.07705613038141887,
           0.03373366241247356,
           -0.14030595234304444,
           -0.08698179613976788,
           1,
           -0.34300854886386367,
           0.3966770947758288,
           0.4389575839037729,
           0.8567869662428633,
           0.6666492751714512,
           -0.3829862499698254,
           0.8096228023360402,
           0.7811003550643558,
           0.8196421043952123,
           0.9020075577701704,
           0.7623196327504825,
           0.7855919915695732,
           0.2923759400904117,
           0.06406184345520721,
           0.17775027415223135,
           0.16283578889433092,
           0.1432640273945476,
           0.01826849662464291,
           -0.12635118287438812,
           0.14716408921963745,
           0.04165956093788422,
           -0.02276045418929416,
           0.06798918022221863,
           0.08921468086936798,
           0.09362744610542728,
           0.24166699578051443,
           0.08002640399013289,
           0.11797794244387573
          ],
          [
           -0.1694025690663006,
           -0.054764869491111104,
           -0.08944111056298266,
           0.019848876146946002,
           -0.01992526121922691,
           -0.0335514237215928,
           -0.14352671935124364,
           -0.22088150363698486,
           -0.04373452305616672,
           -0.056132286702011196,
           -0.02940006979393982,
           -0.00010207631435353367,
           -0.14826832679495175,
           -0.1979398981199328,
           -0.12929025834891328,
           -0.007211115867403615,
           -0.0570677251689698,
           -0.004811607696752734,
           -0.05805058909968625,
           -0.02653880565798484,
           -0.04278048675172523,
           0.06243550489670563,
           0.013579677465961924,
           -0.029881146787594737,
           0.42202750022099067,
           0.11183991369090285,
           0.6172313619433962,
           -0.34300854886386367,
           1,
           0.4317008770775391,
           -0.9785449790060089,
           -0.6147891863446056,
           -0.7779799134139744,
           0.5386287886647586,
           -0.5979705261966006,
           -0.7052809181059466,
           -0.5996170743718556,
           -0.5077508938133616,
           -0.5114789250472294,
           -0.6738369277850481,
           -0.05136325645177159,
           -0.026527350869294468,
           -0.11242939563161047,
           0.03240202009384742,
           0.14035839171204076,
           -0.07960971971587794,
           -0.22992600822312032,
           0.15931885460496495,
           0.1299500844933461,
           0.25136019847061913,
           0.11661852483105559,
           0.21064885207517692,
           0.10890654943090537,
           0.048245247883584544,
           0.10804663902991209,
           -0.034712882585064754
          ],
          [
           0.020728248287895275,
           -0.015233229041393424,
           0.002759721482153803,
           0.04132754413664121,
           0.029638394945205013,
           0.02195052883142534,
           0.03013932400124241,
           0.11515993799539095,
           0.003949177057719133,
           0.009039300682583748,
           -0.002836825237181664,
           0.00022595530924144063,
           0.057253380814684396,
           0.12753158525842148,
           0.042221382880769744,
           -0.001398724847399598,
           -0.0041297649577806085,
           0.0020543542638882745,
           0.02668633722711934,
           -0.006017294324750368,
           0.007166811858396961,
           -0.010296588738321477,
           0.04911146705368587,
           0.01640388998743268,
           0.13627040121112163,
           0.037933890480212444,
           0.20293560262361449,
           0.3966770947758288,
           0.4317008770775391,
           1,
           -0.34918431759741875,
           0.07391440506475298,
           -0.15326763799795884,
           0.21492781148418927,
           0.1207775734887835,
           -0.022287472797393078,
           0.12060194487836183,
           0.14813689889431317,
           0.15768247646704187,
           -0.021145158648052387,
           0.06651182651389395,
           0.11096459029225046,
           -0.08912130159789537,
           0.10176596070140545,
           0.2358129194127048,
           0.2738634024086589,
           -0.22682955849661204,
           0.11927673506740079,
           -0.07347606530304242,
           0.16805004345735922,
           0.020154848253706715,
           0.13227350095452622,
           0.05226928788057051,
           0.053276844225658086,
           0.023780397709317707,
           0.020342938344437723
          ],
          [
           0.17405776567189132,
           0.04850212010719168,
           0.09501761241835162,
           -0.012320542563059804,
           0.027075653646306472,
           0.035655845674097936,
           0.15835725164241804,
           0.2381776548621473,
           0.042237936821644166,
           0.059457044887201584,
           0.025911927430619642,
           0.00011640107190988665,
           0.16280590827735067,
           0.21465874676277452,
           0.1417341536750063,
           0.007507673946448987,
           0.05546771383301571,
           0.00565480732758031,
           0.05969632909564368,
           0.023661507275328685,
           0.040275235754677376,
           -0.07117177197988575,
           -0.009728950409117478,
           0.034199247219551025,
           -0.4191689104973375,
           -0.13929386967371288,
           -0.5288879796856472,
           0.4389575839037729,
           -0.9785449790060089,
           -0.34918431759741875,
           1,
           0.658924490533793,
           0.7863313455574057,
           -0.6116760624105366,
           0.6324525151293476,
           0.7230106583345582,
           0.6350309682853089,
           0.5701552336745049,
           0.534967609570076,
           0.7046190038230695,
           0.09304372772086318,
           -0.010593008527706936,
           0.13936932562215765,
           -0.031047753267128473,
           -0.05884291883995694,
           0.09697164776644776,
           0.16640276679307414,
           -0.12842864952020513,
           -0.12562322644054938,
           -0.1939150298833387,
           -0.09922995074131712,
           -0.2051021367149951,
           -0.0868655825321496,
           -0.030898809095735284,
           -0.08624589083173821,
           0.03514340046693614
          ],
          [
           0.21523916331755188,
           0.04277215661095484,
           0.13002644182809137,
           0.01743717465551732,
           0.06920073595342086,
           0.10454687554818487,
           0.2364291541645233,
           0.5755096708250129,
           0.08127348522522176,
           0.08856988751618453,
           0.056753516480384784,
           -0.00045291324350510626,
           0.28005718745126057,
           0.5671518149371741,
           0.2378146691261771,
           0.008776232091046717,
           0.08163263009899414,
           0.008608961651620369,
           0.12956748963077683,
           0.04400992491058606,
           0.08735676251789809,
           -0.04714824520432729,
           0.08681994456212859,
           0.07447119513804648,
           -0.06176930230689434,
           -0.346723651090091,
           -0.4043825468749695,
           0.8567869662428633,
           -0.6147891863446056,
           0.07391440506475298,
           0.658924490533793,
           1,
           0.9327473020320688,
           -0.29564352223831575,
           0.9697167695215843,
           0.9766065721870845,
           0.9776461154718555,
           0.9707704871356305,
           0.9299139760386107,
           0.9455026252346257,
           0.22153051141977098,
           -0.09578405025434278,
           0.11440833749899046,
           0.09067085426145895,
           0.14839572521025657,
           0.10231730438317317,
           -0.1083497498221034,
           0.11993661678551727,
           0.027143903252742848,
           0.009329840972500033,
           0.13274851656358705,
           0.07715727144248424,
           0.13413270642761155,
           0.21926642930846574,
           0.14182129754172268,
           0.07510579761879738
          ],
          [
           0.214222067936488,
           0.058186839758136176,
           0.12548172698643945,
           -0.020688290817673243,
           0.05179178183223123,
           0.0807504605610682,
           0.21536068913273101,
           0.45849172375946384,
           0.07602277165083447,
           0.08225272268319163,
           0.053221810172827115,
           0.00001042347936008765,
           0.25045946807852526,
           0.44123885583732114,
           0.21131721324060315,
           0.009166276681429768,
           0.0813403943766023,
           0.007812650201691963,
           0.11525665073321731,
           0.04205064618393252,
           0.0824000570437074,
           -0.04905885052854838,
           0.05908489327677447,
           0.06407046027858136,
           -0.13644080100893294,
           -0.43853464131516495,
           -0.5119900708116529,
           0.6666492751714512,
           -0.7779799134139744,
           -0.15326763799795884,
           0.7863313455574057,
           0.9327473020320688,
           1,
           -0.28048460377235307,
           0.9247952567550097,
           0.95745938242882,
           0.9381333919180525,
           0.8692656386500135,
           0.8943656152698456,
           0.9482687626585667,
           0.19329106721682388,
           -0.1496686987131188,
           0.12982683674202447,
           0.06801931647312001,
           0.13857677825767065,
           0.18612839674584317,
           -0.07594709815591573,
           0.081426514217856,
           0.04445921691022119,
           0.03026350207553038,
           0.1655650897916457,
           0.0504891696775469,
           0.15347618269278634,
           0.19137988867429892,
           0.18128725547324487,
           0.09594323301282734
          ],
          [
           -0.05605604976988191,
           0.030714082830391315,
           -0.05579994107391595,
           -0.06584778377910637,
           -0.031678879527146216,
           -0.03904006501743025,
           -0.09995865649911868,
           -0.2430273931814662,
           -0.0026763248254370307,
           -0.03290849242843754,
           0.0013509025355115463,
           -0.000007266868267057065,
           -0.06424792663631743,
           -0.2317488918352337,
           -0.062310041136743,
           -0.0038795913109410454,
           -0.01599250394631731,
           -0.004070362179693465,
           0.0025094393477845312,
           -0.0024151681336341377,
           0.014522521320475852,
           0.0760129144648182,
           0.03930266937080006,
           -0.01764744150216741,
           0.13636066524703577,
           -0.3698530982921998,
           -0.14364274605307,
           -0.3829862499698254,
           0.5386287886647586,
           0.21492781148418927,
           -0.6116760624105366,
           -0.29564352223831575,
           -0.28048460377235307,
           1,
           -0.20829447873328988,
           -0.2598965022987127,
           -0.192575319956597,
           -0.3341605285720811,
           -0.03347663083301727,
           -0.38004705981911185,
           -0.18053801414444687,
           -0.2520771818875507,
           -0.274461806182978,
           0.05598654216439941,
           0.12931935520001642,
           0.01633503029089159,
           -0.14360347271974075,
           0.23688500610096658,
           0.1406059186731973,
           0.37999344731705786,
           0.26079232201468006,
           0.3143806200306976,
           0.26125369288433736,
           0.13412077520885166,
           0.30168966698955146,
           -0.02689410216032532
          ],
          [
           0.22123424063864597,
           0.049043670044639574,
           0.12727480262554025,
           0.00947713084548406,
           0.06800645465435586,
           0.09513221353187987,
           0.23174425710284394,
           0.5255899500133793,
           0.07904944115091408,
           0.08730288551978482,
           0.05429827293925442,
           0.0003346262597932895,
           0.27699153729037285,
           0.5161820304714851,
           0.22895984177838283,
           0.00941407816742992,
           0.07863770064450064,
           0.008589368035587202,
           0.12663679062588024,
           0.038979974101920147,
           0.08612713597474983,
           -0.04633768314173026,
           0.07943881771528555,
           0.07167078666731302,
           -0.09553402787418426,
           -0.4124793485936792,
           -0.4467636941148178,
           0.8096228023360402,
           -0.5979705261966006,
           0.1207775734887835,
           0.6324525151293476,
           0.9697167695215843,
           0.9247952567550097,
           -0.20829447873328988,
           1,
           0.9602658144815295,
           0.9916265358109618,
           0.9384557030867265,
           0.9701429759600866,
           0.9164260829551436,
           0.13436014465772167,
           -0.07871275546487669,
           0.0540003949631003,
           0.08058234107871272,
           0.1544964347978122,
           0.15692372020847173,
           -0.1064177579464136,
           0.10416103124733014,
           -0.033502025619068856,
           -0.0028912479356132666,
           0.17327183166734056,
           0.07191744020820229,
           0.12882320441598727,
           0.1910390186718229,
           0.1456755130874423,
           0.013117378934700706
          ],
          [
           0.2196591416020006,
           0.05436353984860677,
           0.1269821462714237,
           0.002692746800971107,
           0.06134631946425617,
           0.0951731359223699,
           0.2272710384809254,
           0.5287768831855216,
           0.08200639275498162,
           0.08616579041321265,
           0.05809480379216063,
           -0.0005182861653578059,
           0.2703713723523952,
           0.5184930639424177,
           0.2307728620891947,
           0.008604423322011707,
           0.08321116566784321,
           0.00801911179205245,
           0.1273142317619763,
           0.04515319956015765,
           0.08862576270801625,
           -0.0446592274927523,
           0.07519713367570548,
           0.07043886806775787,
           -0.14921858254907056,
           -0.35727367958822637,
           -0.5153869027207505,
           0.7811003550643558,
           -0.7052809181059466,
           -0.022287472797393078,
           0.7230106583345582,
           0.9766065721870845,
           0.95745938242882,
           -0.2598965022987127,
           0.9602658144815295,
           1,
           0.968329750848174,
           0.9259429362558955,
           0.9313026441864108,
           0.9300789889802077,
           0.1461736639563987,
           -0.08157022099896775,
           0.10635203904977152,
           0.07795128490158608,
           0.10476908704129276,
           0.07297910576269871,
           -0.04882063248177497,
           0.07793864025907349,
           -0.0026020600052920214,
           -0.030749283701675727,
           0.10835447020633458,
           0.07825644107452831,
           0.10432429639584163,
           0.1965389302827853,
           0.12035020155496952,
           0.036551793985369445
          ],
          [
           0.2203919861969848,
           0.050046211836822084,
           0.12756328711370915,
           0.003736251165632428,
           0.06636271888239749,
           0.09518953331566979,
           0.2307899320455023,
           0.5294174638417082,
           0.08132081759495405,
           0.08709844291141079,
           0.05612265236126545,
           -0.00003174304143950264,
           0.27579346563097196,
           0.5190109849484688,
           0.23174879440138035,
           0.00866695270213318,
           0.08154379883557229,
           0.00878059867286731,
           0.12947618789237011,
           0.04278507726292092,
           0.08914537616999445,
           -0.046908451178234334,
           0.08242504805394245,
           0.0727387860902134,
           -0.07338777444352214,
           -0.43609251604862936,
           -0.4420361010048875,
           0.8196421043952123,
           -0.5996170743718556,
           0.12060194487836183,
           0.6350309682853089,
           0.9776461154718555,
           0.9381333919180525,
           -0.192575319956597,
           0.9916265358109618,
           0.968329750848174,
           1,
           0.9467223196579155,
           0.9770045618438085,
           0.9327492735192651,
           0.17824698593598656,
           -0.11991726394893915,
           0.10005648151931025,
           0.09949031099525438,
           0.1865986591505027,
           0.14672033723688524,
           -0.12483711421547444,
           0.13630913472855768,
           0.018500424431131382,
           0.043814353947952474,
           0.1806744324679263,
           0.1025421804019965,
           0.1765917445037175,
           0.22989778227031396,
           0.18848999573159175,
           0.06476186621719132
          ],
          [
           0.1980457097321248,
           0.028371741350146366,
           0.1251960322363448,
           0.03385414870308001,
           0.07245172342529724,
           0.10427125891233889,
           0.23039443257050182,
           0.5832737839185593,
           0.08670793877758158,
           0.08471028211030428,
           0.06458545663499728,
           -0.001729962564359082,
           0.2677905477779262,
           0.576146528819173,
           0.23132810902269507,
           0.006225811532693846,
           0.08413257463018523,
           0.00828431634907061,
           0.13125571871225175,
           0.051323060737403996,
           0.08955517294859824,
           -0.05040110102668377,
           0.08345458495128134,
           0.08269037504049404,
           0.036939648028959104,
           -0.30047524920292484,
           -0.21738346794254929,
           0.9020075577701704,
           -0.5077508938133616,
           0.14813689889431317,
           0.5701552336745049,
           0.9707704871356305,
           0.8692656386500135,
           -0.3341605285720811,
           0.9384557030867265,
           0.9259429362558955,
           0.9467223196579155,
           1,
           0.9045418936617555,
           0.9276072405873633,
           0.2489951484479052,
           -0.07290680702827755,
           0.1663117076976887,
           0.16591605406047735,
           0.1829296308774615,
           0.0950069167758476,
           -0.13536723229135722,
           0.1991891193561048,
           0.0844490756415614,
           0.029045625753693256,
           0.18393151537069105,
           0.16459364593473394,
           0.18036476670511725,
           0.32579688518636335,
           0.17843816934707132,
           0.10004375503877
          ],
          [
           0.21408915895942204,
           0.05440170963806342,
           0.119176374825793,
           -0.006033322599852827,
           0.06243543328621054,
           0.0872586950007477,
           0.21552777828161315,
           0.48271389234138606,
           0.08236333264217684,
           0.08192548750007889,
           0.058245539709238736,
           -0.00006203325860943357,
           0.2646717188282555,
           0.4749579969619134,
           0.22002185832600432,
           0.00791273650293216,
           0.08041984378525291,
           0.008422004118264317,
           0.13070632673193122,
           0.04371831326874511,
           0.09224135363935175,
           -0.037094133469945244,
           0.08536268718883964,
           0.0726109623840008,
           -0.0458474224453918,
           -0.5120872343995113,
           -0.42904228412093154,
           0.7623196327504825,
           -0.5114789250472294,
           0.15768247646704187,
           0.534967609570076,
           0.9299139760386107,
           0.8943656152698456,
           -0.03347663083301727,
           0.9701429759600866,
           0.9313026441864108,
           0.9770045618438085,
           0.9045418936617555,
           1,
           0.8774879007407469,
           0.12008611435327939,
           -0.15387002833721652,
           0.09758665409405735,
           0.11346767569398382,
           0.21366277663705593,
           0.13300791096823716,
           -0.14969214375901777,
           0.17256288778067658,
           0.028850768972950903,
           0.0925919255703313,
           0.21332605848568834,
           0.1670963375082425,
           0.19657498664262427,
           0.25632254944239985,
           0.25014360615845527,
           0.035634321169622285
          ],
          [
           0.20973454805427058,
           0.04518977397054094,
           0.13216959296555744,
           0.011147361231729613,
           0.06687538477099265,
           0.10025061821630528,
           0.22548120570341706,
           0.55719355765835,
           0.06894893909530812,
           0.08624745502174554,
           0.04569584782750142,
           0.000053577315480782955,
           0.2594756573035441,
           0.5443285476388379,
           0.22191320520309119,
           0.009532619113872907,
           0.07612540200277036,
           0.00794040755881214,
           0.10962815153883401,
           0.03617664217545269,
           0.07161579885762054,
           -0.055117842793691675,
           0.07412288924067655,
           0.06543012594895929,
           0.01380058324167279,
           -0.3440423185860083,
           -0.30656650011309805,
           0.7855919915695732,
           -0.6738369277850481,
           -0.021145158648052387,
           0.7046190038230695,
           0.9455026252346257,
           0.9482687626585667,
           -0.38004705981911185,
           0.9164260829551436,
           0.9300789889802077,
           0.9327492735192651,
           0.9276072405873633,
           0.8774879007407469,
           1,
           0.2644883730693056,
           -0.09954486243191601,
           0.21255697723439598,
           0.03958696695098543,
           0.13981892242373334,
           0.13732977384459755,
           -0.07997872615217197,
           0.03431748200350414,
           0.048432459025094285,
           0.012852646417526975,
           0.10587455679027498,
           0.008983165470838468,
           0.09553850566787853,
           0.15234424221655568,
           0.12303808873346322,
           0.1841078307897975
          ],
          [
           0.01484224259417869,
           -0.017671948903775683,
           0.02819347362035758,
           -0.02024628436734244,
           0.018965149251168132,
           0.027074313503602364,
           0.036528272428582836,
           0.1550949026116112,
           0.01828946561507515,
           0.020781117219353707,
           0.012252381567450251,
           0.014752401399160541,
           0.04100822605811792,
           0.14975956215526648,
           0.04149190941934147,
           0.014787524756816866,
           0.019053689763067982,
           0.010782671532921293,
           0.0210604834316476,
           0.01800024468282453,
           0.017173305081369928,
           -0.011500240650629047,
           0.04315231286257555,
           0.012979436240186638,
           0.2641154038445176,
           -0.10793726145692246,
           0.04450572972266792,
           0.2923759400904117,
           -0.05136325645177159,
           0.06651182651389395,
           0.09304372772086318,
           0.22153051141977098,
           0.19329106721682388,
           -0.18053801414444687,
           0.13436014465772167,
           0.1461736639563987,
           0.17824698593598656,
           0.2489951484479052,
           0.12008611435327939,
           0.2644883730693056,
           1,
           -0.24747424632820733,
           0.11499048044680506,
           0.09598182082813901,
           0.12280472641187666,
           0.0807720962377329,
           -0.07053468599792351,
           0.28362458513468486,
           0.4765639616421137,
           0.256767226569736,
           0.12496248301585162,
           0.1715109479213662,
           0.3224441885325063,
           0.2949595287946225,
           0.26995305781790774,
           0.5136368903530047
          ],
          [
           -0.003342866092776462,
           -0.010208921986455215,
           -0.004238290560153136,
           0.04899244299203558,
           0.009087444520673211,
           0.02372049982950889,
           -0.008827561785280828,
           0.1254922738683701,
           -0.00289260308546518,
           0.005132697836497742,
           -0.001036260614158971,
           -0.00027712570250665677,
           -0.01870822104812203,
           0.13186444415900966,
           -0.015394952757898699,
           0.00036588600825235117,
           -0.0067653139761942715,
           0.0003085398510986077,
           -0.016971435780155475,
           -0.0046786178907434976,
           -0.012941417125253705,
           -0.011836686965584979,
           -0.02636106994424967,
           -0.006419945901678516,
           -0.0897661640745926,
           0.7588845132333762,
           0.04871862242353827,
           0.06406184345520721,
           -0.026527350869294468,
           0.11096459029225046,
           -0.010593008527706936,
           -0.09578405025434278,
           -0.1496686987131188,
           -0.2520771818875507,
           -0.07871275546487669,
           -0.08157022099896775,
           -0.11991726394893915,
           -0.07290680702827755,
           -0.15387002833721652,
           -0.09954486243191601,
           -0.24747424632820733,
           1,
           0.07609923547888063,
           -0.03704467223652235,
           -0.3843695046493461,
           0.007895500151282145,
           0.30975193281713403,
           -0.48910877109823575,
           -0.5252350893608644,
           -0.7271663176433638,
           -0.5543829844980652,
           -0.45249269546742144,
           -0.6780580101908698,
           -0.3018563544556886,
           -0.7505912438924853,
           -0.23763226433700274
          ],
          [
           0.02192118439307835,
           0.004574288425679268,
           0.021347859872489924,
           -0.016366759229025012,
           0.012546450487575711,
           0.00390744488541775,
           0.00833460691373781,
           0.055024734526707295,
           -0.0024411846788910925,
           0.009651524284567893,
           -0.007103924524114475,
           -0.0006440174894136099,
           0.006567917223040462,
           0.05296544069586066,
           0.03101663446619098,
           -0.0006601250164623818,
           0.009899138978960114,
           0.0020824074181408866,
           -0.004011531998912033,
           0.00532716084979277,
           -0.008062369613235365,
           -0.03345893641919585,
           -0.012406887324206747,
           -0.0004720770722885624,
           0.06054454541368402,
           0.07204915416438476,
           0.31324767026359734,
           0.17775027415223135,
           -0.11242939563161047,
           -0.08912130159789537,
           0.13936932562215765,
           0.11440833749899046,
           0.12982683674202447,
           -0.274461806182978,
           0.0540003949631003,
           0.10635203904977152,
           0.10005648151931025,
           0.1663117076976887,
           0.09758665409405735,
           0.21255697723439598,
           0.11499048044680506,
           0.07609923547888063,
           1,
           0.06324276280031897,
           0.1753742545818035,
           -0.04299485008605388,
           -0.17228852031591851,
           -0.02056035323993812,
           0.09199323169652684,
           -0.042173888240339484,
           -0.18606734535325178,
           -0.009762017417211742,
           -0.06010670896155994,
           0.11917739142087494,
           0.05688890486330214,
           0.312447239229218
          ],
          [
           -0.013240205609424042,
           -0.014989918647526256,
           -0.00453075176311033,
           0.022816914316251723,
           -0.008245823825335818,
           -0.026571880450256637,
           -0.0043837885634771855,
           -0.07008197052718575,
           0.11776853467850759,
           -0.025016385836222177,
           0.11737808756666541,
           -0.01229541138433293,
           -0.0014105665768239524,
           -0.07186754026764917,
           -0.0017750630181504437,
           -0.018609834709993795,
           0.08789466854672587,
           -0.023451763461548487,
           0.11044433203755546,
           0.11219519054222009,
           0.11572468633835511,
           -0.017396773589844775,
           -0.002570270096137871,
           0.08903491764448616,
           0.027188777512967565,
           -0.026503972550133176,
           0.07809586100901798,
           0.16283578889433092,
           0.03240202009384742,
           0.10176596070140545,
           -0.031047753267128473,
           0.09067085426145895,
           0.06801931647312001,
           0.05598654216439941,
           0.08058234107871272,
           0.07795128490158608,
           0.09949031099525438,
           0.16591605406047735,
           0.11346767569398382,
           0.03958696695098543,
           0.09598182082813901,
           -0.03704467223652235,
           0.06324276280031897,
           1,
           0.03605745210012348,
           0.1447768893607905,
           -0.019105526030903968,
           0.6874993488379493,
           0.4659625984288942,
           0.07590969857695559,
           0.5169035202793575,
           0.6218238997826243,
           0.5439664638631324,
           0.7842132988160566,
           0.3989431303747742,
           0.30351732978772744
          ],
          [
           0.021308921855466872,
           -0.0007663471459661916,
           0.01958555987158994,
           -0.03258596185156092,
           0.00650646685814862,
           -0.023845927494004673,
           0.023824428210294495,
           -0.10567035975530474,
           0.007295810040806446,
           0.0057877620742679305,
           -0.002179134317563191,
           -0.00013121205812792095,
           0.043027451408785854,
           -0.1086725676152516,
           0.03926510864712119,
           -0.001852596533776769,
           0.007846096465308343,
           0.0006752437886069413,
           0.017353174706591836,
           -0.00006908122321930606,
           0.012948840998605516,
           -0.022448837512780224,
           0.016666047976430794,
           0.010851003493069474,
           0.02039576223600634,
           -0.37805429974768195,
           0.1768116026289732,
           0.1432640273945476,
           0.14035839171204076,
           0.2358129194127048,
           -0.05884291883995694,
           0.14839572521025657,
           0.13857677825767065,
           0.12931935520001642,
           0.1544964347978122,
           0.10476908704129276,
           0.1865986591505027,
           0.1829296308774615,
           0.21366277663705593,
           0.13981892242373334,
           0.12280472641187666,
           -0.3843695046493461,
           0.1753742545818035,
           0.03605745210012348,
           1,
           0.45548432672366496,
           -0.9007675528337117,
           0.2868745594379507,
           0.07569306639107341,
           0.7439296468618886,
           0.3561863807566348,
           0.1639731513045619,
           0.35827004968625603,
           0.24511287665394924,
           0.4580974418463966,
           -0.09158323299011031
          ],
          [
           0.039698078465774445,
           0.019420517760908886,
           0.014192513170354996,
           -0.04071131634359541,
           -0.015199477915651363,
           -0.03258994489627726,
           0.05025635243095818,
           -0.1472536794139184,
           0.005450680723231817,
           0.005634339081640727,
           0.0032130157972044596,
           -0.0004404069641473495,
           0.02874108750380335,
           -0.16093370015678019,
           0.007545279777187934,
           -0.0008617088317452627,
           0.009330724577449502,
           -0.0010142391845376275,
           0.008260083614467808,
           -0.00715805932009112,
           0.00657359297647599,
           -0.025302210533206377,
           -0.033421759454455566,
           0.01068543348487271,
           -0.06302157361333345,
           -0.09889587498205865,
           -0.05215217894334675,
           0.01826849662464291,
           -0.07960971971587794,
           0.2738634024086589,
           0.09697164776644776,
           0.10231730438317317,
           0.18612839674584317,
           0.01633503029089159,
           0.15692372020847173,
           0.07297910576269871,
           0.14672033723688524,
           0.0950069167758476,
           0.13300791096823716,
           0.13732977384459755,
           0.0807720962377329,
           0.007895500151282145,
           -0.04299485008605388,
           0.1447768893607905,
           0.45548432672366496,
           1,
           -0.40319137219934226,
           0.10922446202639005,
           -0.020677871939682138,
           0.32223174480205063,
           0.2630956164523429,
           -0.008787240508971889,
           0.2131147901800924,
           0.08468441267419952,
           0.20417458248899795,
           0.08661608259172592
          ],
          [
           -0.0061619190199018545,
           0.006750749472342707,
           -0.016481048168796517,
           0.02253409788596724,
           -0.009917538808664745,
           0.00956064433140609,
           -0.006696124780421833,
           0.038190008348639816,
           -0.005864744360185743,
           -0.005159270789444962,
           0.0019212315072620275,
           -0.00014824737081160975,
           -0.036645354301327,
           0.03558415191554809,
           -0.02915151935333705,
           0.0007555059718743442,
           -0.004545546580670676,
           -0.0003175278337418933,
           -0.01479197971411378,
           -0.0003332164749933805,
           -0.010072609159906744,
           0.015962405896697753,
           -0.021421078419625473,
           -0.008266675875604708,
           -0.03426086606716991,
           0.31752074194945207,
           -0.21151163623726518,
           -0.12635118287438812,
           -0.22992600822312032,
           -0.22682955849661204,
           0.16640276679307414,
           -0.1083497498221034,
           -0.07594709815591573,
           -0.14360347271974075,
           -0.1064177579464136,
           -0.04882063248177497,
           -0.12483711421547444,
           -0.13536723229135722,
           -0.14969214375901777,
           -0.07997872615217197,
           -0.07053468599792351,
           0.30975193281713403,
           -0.17228852031591851,
           -0.019105526030903968,
           -0.9007675528337117,
           -0.40319137219934226,
           1,
           -0.22790271977990664,
           -0.03572762364922707,
           -0.6671081079856744,
           -0.3158066892182909,
           -0.15285719696366107,
           -0.2910351422026774,
           -0.19740386198342216,
           -0.4220754230906354,
           0.16987704760202263
          ],
          [
           -0.009661321905655648,
           -0.011452017281917476,
           -0.008524934226266975,
           -0.013824051455925215,
           -0.007293994738665724,
           -0.034849815844088486,
           -0.0010660599589879004,
           -0.16007931820663254,
           0.06442111520183892,
           -0.01572513335352094,
           0.06398234615456934,
           -0.004185356744100419,
           0.011909854018751602,
           -0.1643461954064535,
           0.007780567960430482,
           -0.008523292950110532,
           0.046038219302715344,
           -0.0030013523800118407,
           0.07059114207866082,
           0.05504483663161365,
           0.06842903820139813,
           0.008150303359763349,
           0.017338710841185937,
           0.0528977317692586,
           0.1360357294514458,
           -0.34788527774308153,
           0.1085173737359561,
           0.14716408921963745,
           0.15931885460496495,
           0.11927673506740079,
           -0.12842864952020513,
           0.11993661678551727,
           0.081426514217856,
           0.23688500610096658,
           0.10416103124733014,
           0.07793864025907349,
           0.13630913472855768,
           0.1991891193561048,
           0.17256288778067658,
           0.03431748200350414,
           0.28362458513468486,
           -0.48910877109823575,
           -0.02056035323993812,
           0.6874993488379493,
           0.2868745594379507,
           0.10922446202639005,
           -0.22790271977990664,
           1,
           0.7755773915534113,
           0.5160578719507507,
           0.7899861009083055,
           0.8581115630956572,
           0.8984097252721996,
           0.9137800094838956,
           0.776613281076267,
           0.34308019087422387
          ],
          [
           -0.029884641455015522,
           -0.010640074957539544,
           -0.01271394248038454,
           -0.014422480541239187,
           -0.00977542325137606,
           -0.026797747106930225,
           -0.023927148793197493,
           -0.14000501346261762,
           0.019137238478933727,
           -0.01949230065441609,
           0.022817868467389257,
           -0.0018867072957929481,
           -0.02270592142829439,
           -0.14203821275094067,
           -0.025239873670873048,
           -0.005500255998286456,
           0.015685027572562727,
           -0.0019524088104962405,
           0.01793221200925467,
           0.022341445716498907,
           0.019340665511081413,
           0.013620637164737765,
           0.02389200764444259,
           0.014523476366802547,
           0.27227219159349436,
           -0.28893376896377315,
           0.1466031602183782,
           0.04165956093788422,
           0.1299500844933461,
           -0.07347606530304242,
           -0.12562322644054938,
           0.027143903252742848,
           0.04445921691022119,
           0.1406059186731973,
           -0.033502025619068856,
           -0.0026020600052920214,
           0.018500424431131382,
           0.0844490756415614,
           0.028850768972950903,
           0.048432459025094285,
           0.4765639616421137,
           -0.5252350893608644,
           0.09199323169652684,
           0.4659625984288942,
           0.07569306639107341,
           -0.020677871939682138,
           -0.03572762364922707,
           0.7755773915534113,
           1,
           0.43480667868291967,
           0.581799974451054,
           0.7574602196942488,
           0.7774822275863166,
           0.6776526644596313,
           0.662498186473516,
           0.6764700524574971
          ],
          [
           -0.014402874416782258,
           0.006220338543099035,
           -0.005999125838318191,
           -0.03735058011746879,
           -0.01177551222115849,
           -0.03154534106614697,
           -0.014122846043596099,
           -0.16644997782208437,
           -0.001027720451470733,
           -0.010455338075208648,
           -0.0033206594442549086,
           -0.00010497566864086708,
           0.004583575472181388,
           -0.16790805280879398,
           -0.002236307312637326,
           -0.0016655711098748165,
           -0.0007685020919298283,
           -0.0015818622783690206,
           0.009820495505211616,
           -0.004471407658047546,
           0.0078222800028322,
           0.009630322717504003,
           0.023319695016431777,
           0.0019123676064138177,
           0.15358149398143697,
           -0.5554242288810061,
           0.13967167914886391,
           -0.02276045418929416,
           0.25136019847061913,
           0.16805004345735922,
           -0.1939150298833387,
           0.009329840972500033,
           0.03026350207553038,
           0.37999344731705786,
           -0.0028912479356132666,
           -0.030749283701675727,
           0.043814353947952474,
           0.029045625753693256,
           0.0925919255703313,
           0.012852646417526975,
           0.256767226569736,
           -0.7271663176433638,
           -0.042173888240339484,
           0.07590969857695559,
           0.7439296468618886,
           0.32223174480205063,
           -0.6671081079856744,
           0.5160578719507507,
           0.43480667868291967,
           1,
           0.5562440801438336,
           0.4316862211559792,
           0.6466511473156933,
           0.3673687451776767,
           0.7579326416141808,
           0.16654592097480486
          ],
          [
           -0.00019903505225027146,
           -0.005904586199871624,
           0.006122021087149214,
           -0.030467070750304935,
           -0.004561007634398637,
           -0.01976875418184168,
           0.024860983705798166,
           -0.09844877303348375,
           0.04219704425659732,
           -0.005872240686077308,
           0.0422942574940295,
           -0.0021190393965980466,
           0.032489627351544974,
           -0.11103706456007086,
           0.005813373310463084,
           -0.004359177836890809,
           0.03037290077640823,
           -0.003235550135673405,
           0.049906829668761546,
           0.025654905222241967,
           0.047175258522891846,
           0.012579009244469419,
           0.017850470586752976,
           0.04111984270461193,
           0.14320164196600474,
           -0.5279869423443261,
           0.07062250966159672,
           0.06798918022221863,
           0.11661852483105559,
           0.020154848253706715,
           -0.09922995074131712,
           0.13274851656358705,
           0.1655650897916457,
           0.26079232201468006,
           0.17327183166734056,
           0.10835447020633458,
           0.1806744324679263,
           0.18393151537069105,
           0.21332605848568834,
           0.10587455679027498,
           0.12496248301585162,
           -0.5543829844980652,
           -0.18606734535325178,
           0.5169035202793575,
           0.3561863807566348,
           0.2630956164523429,
           -0.3158066892182909,
           0.7899861009083055,
           0.581799974451054,
           0.5562440801438336,
           1,
           0.6836404207848484,
           0.8721226029723798,
           0.6888317253096882,
           0.817858854794182,
           0.13725320317705814
          ],
          [
           -0.026304472555636452,
           -0.013077435262994938,
           -0.013910586855077645,
           -0.025392011725204,
           -0.006425096854187829,
           -0.020848003688982653,
           -0.013491945816981443,
           -0.09659285170717641,
           0.06085389261655755,
           -0.017592580608633987,
           0.06330001273480196,
           -0.003611728959811668,
           -0.00023266663544093614,
           -0.0960554978539139,
           0.003349831564559948,
           -0.006717753296249841,
           0.041999873910593004,
           -0.0025809939460106355,
           0.06543579657396785,
           0.05590274897333009,
           0.06332142980411716,
           0.018418160519771905,
           0.018679349220232285,
           0.048505432718014,
           0.19162675036919333,
           -0.3683336530860664,
           0.12294561302067354,
           0.08921468086936798,
           0.21064885207517692,
           0.13227350095452622,
           -0.2051021367149951,
           0.07715727144248424,
           0.0504891696775469,
           0.3143806200306976,
           0.07191744020820229,
           0.07825644107452831,
           0.1025421804019965,
           0.16459364593473394,
           0.1670963375082425,
           0.008983165470838468,
           0.1715109479213662,
           -0.45249269546742144,
           -0.009762017417211742,
           0.6218238997826243,
           0.1639731513045619,
           -0.008787240508971889,
           -0.15285719696366107,
           0.8581115630956572,
           0.7574602196942488,
           0.4316862211559792,
           0.6836404207848484,
           1,
           0.796004598853022,
           0.8341965682685858,
           0.7282966710796253,
           0.3186972758508123
          ],
          [
           -0.004761710210729915,
           -0.004700426808261579,
           -0.0004087927341493351,
           -0.03609347212204058,
           -0.011128519823351368,
           -0.024063325532010654,
           0.019527067418685682,
           -0.11000779143346759,
           0.04469546331052669,
           -0.008930632046425422,
           0.04429569193119272,
           -0.002636238153640449,
           0.019641962409467334,
           -0.12170813084872821,
           0.010682993725853566,
           -0.0054662469475318665,
           0.034901781346080456,
           -0.002645752295054137,
           0.05321498700348118,
           0.034946234245623535,
           0.050486093552641005,
           0.00876364967080746,
           0.018546044891099206,
           0.04095806845674561,
           0.16816977088910287,
           -0.5374462556698194,
           0.0334311927427403,
           0.09362744610542728,
           0.10890654943090537,
           0.05226928788057051,
           -0.0868655825321496,
           0.13413270642761155,
           0.15347618269278634,
           0.26125369288433736,
           0.12882320441598727,
           0.10432429639584163,
           0.1765917445037175,
           0.18036476670511725,
           0.19657498664262427,
           0.09553850566787853,
           0.3224441885325063,
           -0.6780580101908698,
           -0.06010670896155994,
           0.5439664638631324,
           0.35827004968625603,
           0.2131147901800924,
           -0.2910351422026774,
           0.8984097252721996,
           0.7774822275863166,
           0.6466511473156933,
           0.8721226029723798,
           0.796004598853022,
           1,
           0.7857694719236171,
           0.8992175185542951,
           0.37329754678105415
          ],
          [
           0.005898376841279733,
           -0.006354683638137804,
           0.004952807977427503,
           -0.012419366819635489,
           -0.0031009246880221663,
           -0.017683149596033287,
           0.012229947856316448,
           -0.045371030879774926,
           0.10066251474790867,
           -0.006339524558146231,
           0.09978940536400055,
           -0.006758970914903812,
           0.024252441643988397,
           -0.05153910627177115,
           0.03254229736169695,
           -0.010892949258435642,
           0.07706806364099333,
           -0.002377831149621098,
           0.10145393149389736,
           0.09147678171246036,
           0.10106064567962264,
           -0.006050192766825649,
           0.012924756741626239,
           0.0768010551418292,
           0.12243571774092732,
           -0.23603354350553982,
           0.09080136712749994,
           0.24166699578051443,
           0.048245247883584544,
           0.053276844225658086,
           -0.030898809095735284,
           0.21926642930846574,
           0.19137988867429892,
           0.13412077520885166,
           0.1910390186718229,
           0.1965389302827853,
           0.22989778227031396,
           0.32579688518636335,
           0.25632254944239985,
           0.15234424221655568,
           0.2949595287946225,
           -0.3018563544556886,
           0.11917739142087494,
           0.7842132988160566,
           0.24511287665394924,
           0.08468441267419952,
           -0.19740386198342216,
           0.9137800094838956,
           0.6776526644596313,
           0.3673687451776767,
           0.6888317253096882,
           0.8341965682685858,
           0.7857694719236171,
           1,
           0.6691291274904431,
           0.3452042224192315
          ],
          [
           0.004693298428085511,
           0.0029729629221145097,
           0.004880707062188499,
           -0.04634672169467193,
           -0.008773424020452062,
           -0.022473640761804893,
           0.022082512027255183,
           -0.11172518878033419,
           0.030478311831685426,
           -0.00389384954542584,
           0.02995498746034487,
           -0.001493942114388403,
           0.025816393067269874,
           -0.11967333330314274,
           0.012682677799700282,
           -0.003185979960957675,
           0.026413948092432117,
           -0.001851309455635211,
           0.0422663183145503,
           0.021613642460130745,
           0.03711077599085009,
           0.008388494672463259,
           0.021522960884135755,
           0.03061626014660704,
           0.17346481425184884,
           -0.6512453554738035,
           0.05314093057312371,
           0.08002640399013289,
           0.10804663902991209,
           0.023780397709317707,
           -0.08624589083173821,
           0.14182129754172268,
           0.18128725547324487,
           0.30168966698955146,
           0.1456755130874423,
           0.12035020155496952,
           0.18848999573159175,
           0.17843816934707132,
           0.25014360615845527,
           0.12303808873346322,
           0.26995305781790774,
           -0.7505912438924853,
           0.05688890486330214,
           0.3989431303747742,
           0.4580974418463966,
           0.20417458248899795,
           -0.4220754230906354,
           0.776613281076267,
           0.662498186473516,
           0.7579326416141808,
           0.817858854794182,
           0.7282966710796253,
           0.8992175185542951,
           0.6691291274904431,
           1,
           0.2384130553711177
          ],
          [
           0.003992881619631378,
           0.008514074924936183,
           0.0024820047808740107,
           -0.023165887229700595,
           -0.0031774791447154054,
           -0.005104505009606013,
           -0.012151855586079358,
           -0.017480581007640376,
           0.010220835121424498,
           -0.004839973992381817,
           0.00716592228209776,
           -0.0011170199638208826,
           -0.010644984881431056,
           -0.021548458879295285,
           -0.0017910170172579995,
           -0.0018755873998766994,
           0.014260080284706335,
           0.0005783593320193341,
           0.006410765624076502,
           0.015526279669654123,
           0.005606756809990386,
           -0.005843848579420259,
           0.028373072487753396,
           0.0038681540476782957,
           0.207565914719494,
           -0.07780672475004331,
           0.06558529358289407,
           0.11797794244387573,
           -0.034712882585064754,
           0.020342938344437723,
           0.03514340046693614,
           0.07510579761879738,
           0.09594323301282734,
           -0.02689410216032532,
           0.013117378934700706,
           0.036551793985369445,
           0.06476186621719132,
           0.10004375503877,
           0.035634321169622285,
           0.1841078307897975,
           0.5136368903530047,
           -0.23763226433700274,
           0.312447239229218,
           0.30351732978772744,
           -0.09158323299011031,
           0.08661608259172592,
           0.16987704760202263,
           0.34308019087422387,
           0.6764700524574971,
           0.16654592097480486,
           0.13725320317705814,
           0.3186972758508123,
           0.37329754678105415,
           0.3452042224192315,
           0.2384130553711177,
           1
          ]
         ],
         "zmax": 1,
         "zmin": -1
        }
       ],
       "layout": {
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Correlation Matrix"
        },
        "width": 900,
        "xaxis": {
         "side": "bottom",
         "title": {
          "text": "Variables"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Variables"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = x_values.corr()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   z=corr.values,\n",
    "                   x=corr.columns,\n",
    "                   y=corr.columns,\n",
    "                   text=corr.round(2).values,\n",
    "                   texttemplate=\"%{text}\",\n",
    "                   colorscale='Viridis',\n",
    "                   zmin=-1, zmax=1))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix',\n",
    "    xaxis_title=\"Variables\",\n",
    "    yaxis_title=\"Variables\",\n",
    "    xaxis=dict(side='bottom'),\n",
    "    yaxis=dict(autorange='reversed'),\n",
    "    width=900,  # or any width you desire\n",
    "    height=800,  # or any height you desire\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ],
         "text": [
          [
           1,
           0.95,
           0.24,
           0.56,
           0.56,
           0.46,
           0.53,
           0.58,
           0.48,
           0.58,
           0.53,
           0.53,
           -0.11,
           -0.17,
           -0.1,
           -0.11,
           -0.11,
           -0.07,
           -0.14,
           -0.05,
           -0.16,
           -0.1,
           0.04,
           -0.02,
           0.13,
           -0.24,
           -0.09,
           -0.22,
           0.11,
           0.12,
           0.02,
           0.07,
           -0.01,
           -0.04,
           0.13,
           0.16,
           -0.02,
           0.12,
           -0.15,
           0.18,
           0.06,
           0.04,
           0.03,
           0.02,
           0.01,
           0.06,
           0.03,
           0.03,
           0.14,
           0.09,
           0.16,
           0.15,
           0.05,
           0.04,
           -0.02,
           -0.01
          ],
          [
           0.95,
           1,
           0.21,
           0.56,
           0.54,
           0.44,
           0.52,
           0.58,
           0.47,
           0.57,
           0.52,
           0.52,
           -0.11,
           -0.17,
           -0.11,
           -0.12,
           -0.12,
           -0.07,
           -0.14,
           -0.05,
           -0.16,
           -0.1,
           0.04,
           -0.01,
           0.13,
           -0.23,
           -0.08,
           -0.2,
           0.1,
           0.11,
           0.01,
           0.09,
           0,
           -0.04,
           0.13,
           0.15,
           -0.02,
           0.13,
           -0.16,
           0.17,
           0.05,
           0.06,
           0.04,
           0.02,
           0.01,
           0.06,
           0.04,
           0.04,
           0.14,
           0.07,
           0.15,
           0.13,
           0.04,
           0.05,
           -0.03,
           -0.02
          ],
          [
           0.24,
           0.21,
           1,
           0.44,
           0.7,
           0.79,
           0.72,
           0.57,
           0.53,
           0.66,
           0.63,
           0.64,
           -0.06,
           -0.19,
           -0.1,
           -0.09,
           -0.09,
           -0.03,
           -0.13,
           -0.03,
           -0.13,
           -0.21,
           0.17,
           -0.14,
           -0.01,
           -0.61,
           -0.53,
           -0.98,
           -0.01,
           0.17,
           0.05,
           -0.01,
           0.01,
           -0.07,
           -0.42,
           0.09,
           0.04,
           -0.35,
           0.1,
           0.04,
           0.14,
           0.03,
           0.06,
           0.03,
           0.02,
           0.06,
           0.04,
           0.04,
           0.16,
           0.1,
           0.16,
           0.14,
           0.06,
           0.03,
           0,
           0.01
          ],
          [
           0.56,
           0.56,
           0.44,
           1,
           0.79,
           0.67,
           0.78,
           0.9,
           0.76,
           0.86,
           0.81,
           0.82,
           0.14,
           -0.02,
           0.07,
           0.09,
           0.08,
           0.16,
           0.04,
           0.24,
           0.15,
           0.09,
           -0.13,
           -0.14,
           0.06,
           -0.38,
           -0.09,
           -0.34,
           0.05,
           0.17,
           0.01,
           0.09,
           0.01,
           -0.05,
           0.03,
           0.29,
           0.12,
           0.4,
           0.02,
           0.1,
           0.18,
           0.08,
           0.07,
           0.05,
           0.04,
           0.12,
           0.08,
           0.08,
           0.21,
           0.11,
           0.25,
           0.21,
           0.08,
           0.08,
           0,
           0.01
          ],
          [
           0.56,
           0.54,
           0.7,
           0.79,
           1,
           0.95,
           0.93,
           0.93,
           0.88,
           0.95,
           0.92,
           0.93,
           0.14,
           0.01,
           0.11,
           0.1,
           0.12,
           0.04,
           0.05,
           0.15,
           0.03,
           0.01,
           -0.08,
           -0.34,
           -0.1,
           -0.38,
           -0.31,
           -0.67,
           0.01,
           0.21,
           0.05,
           0.07,
           0.01,
           -0.06,
           0.01,
           0.26,
           0.18,
           -0.02,
           0.14,
           0.1,
           0.21,
           0.07,
           0.08,
           0.05,
           0.04,
           0.11,
           0.07,
           0.07,
           0.23,
           0.13,
           0.26,
           0.22,
           0.09,
           0.07,
           0,
           0.01
          ],
          [
           0.46,
           0.44,
           0.79,
           0.67,
           0.95,
           1,
           0.96,
           0.87,
           0.89,
           0.93,
           0.92,
           0.94,
           0.14,
           0.03,
           0.17,
           0.15,
           0.18,
           0.07,
           0.04,
           0.19,
           0.08,
           0.05,
           -0.08,
           -0.44,
           -0.15,
           -0.28,
           -0.51,
           -0.78,
           -0.02,
           0.21,
           0.06,
           0.06,
           0.01,
           -0.05,
           -0.14,
           0.19,
           0.1,
           -0.15,
           0.19,
           0.08,
           0.13,
           0.06,
           0.08,
           0.05,
           0.04,
           0.12,
           0.08,
           0.08,
           0.22,
           0.13,
           0.25,
           0.21,
           0.08,
           0.05,
           0,
           0.01
          ],
          [
           0.53,
           0.52,
           0.72,
           0.78,
           0.93,
           0.96,
           1,
           0.93,
           0.93,
           0.98,
           0.96,
           0.97,
           0.1,
           -0.03,
           0.11,
           0.1,
           0.12,
           0.08,
           0,
           0.2,
           0.08,
           0.08,
           -0.05,
           -0.36,
           -0.08,
           -0.26,
           -0.52,
           -0.71,
           0,
           0.22,
           0.05,
           0.08,
           0.01,
           -0.04,
           -0.15,
           0.15,
           0.04,
           -0.02,
           0.07,
           0.1,
           0.11,
           0.07,
           0.08,
           0.06,
           0.05,
           0.13,
           0.08,
           0.09,
           0.23,
           0.13,
           0.27,
           0.23,
           0.09,
           0.06,
           0,
           0.01
          ],
          [
           0.58,
           0.58,
           0.57,
           0.9,
           0.93,
           0.87,
           0.93,
           1,
           0.9,
           0.97,
           0.94,
           0.95,
           0.18,
           0.03,
           0.18,
           0.18,
           0.18,
           0.17,
           0.08,
           0.33,
           0.2,
           0.16,
           -0.14,
           -0.3,
           -0.07,
           -0.33,
           -0.22,
           -0.51,
           0.03,
           0.2,
           0.03,
           0.08,
           0.01,
           -0.05,
           0.04,
           0.25,
           0.1,
           0.15,
           0.1,
           0.1,
           0.17,
           0.08,
           0.08,
           0.06,
           0.05,
           0.13,
           0.09,
           0.09,
           0.23,
           0.13,
           0.27,
           0.23,
           0.08,
           0.07,
           0,
           0.01
          ],
          [
           0.48,
           0.47,
           0.53,
           0.76,
           0.88,
           0.89,
           0.93,
           0.9,
           1,
           0.93,
           0.97,
           0.98,
           0.21,
           0.09,
           0.21,
           0.2,
           0.25,
           0.11,
           0.03,
           0.26,
           0.17,
           0.17,
           -0.15,
           -0.51,
           -0.15,
           -0.03,
           -0.43,
           -0.51,
           -0.01,
           0.21,
           0.05,
           0.09,
           0.01,
           -0.04,
           -0.05,
           0.12,
           0.04,
           0.16,
           0.13,
           0.09,
           0.1,
           0.07,
           0.08,
           0.06,
           0.04,
           0.13,
           0.08,
           0.09,
           0.22,
           0.12,
           0.26,
           0.22,
           0.08,
           0.06,
           0,
           0.01
          ],
          [
           0.58,
           0.57,
           0.66,
           0.86,
           0.95,
           0.93,
           0.98,
           0.97,
           0.93,
           1,
           0.97,
           0.98,
           0.15,
           0.01,
           0.13,
           0.13,
           0.14,
           0.09,
           0.03,
           0.22,
           0.12,
           0.08,
           -0.11,
           -0.35,
           -0.1,
           -0.3,
           -0.4,
           -0.61,
           0.02,
           0.22,
           0.04,
           0.09,
           0.01,
           -0.05,
           -0.06,
           0.22,
           0.08,
           0.07,
           0.1,
           0.1,
           0.11,
           0.07,
           0.08,
           0.06,
           0.04,
           0.13,
           0.08,
           0.09,
           0.24,
           0.13,
           0.28,
           0.24,
           0.09,
           0.07,
           0,
           0.01
          ],
          [
           0.53,
           0.52,
           0.63,
           0.81,
           0.92,
           0.92,
           0.96,
           0.94,
           0.97,
           0.97,
           1,
           0.99,
           0.15,
           0,
           0.17,
           0.13,
           0.15,
           0.08,
           -0.03,
           0.19,
           0.1,
           0.07,
           -0.11,
           -0.41,
           -0.08,
           -0.21,
           -0.45,
           -0.6,
           0.01,
           0.22,
           0.05,
           0.08,
           0.01,
           -0.05,
           -0.1,
           0.13,
           0.01,
           0.12,
           0.16,
           0.1,
           0.05,
           0.07,
           0.08,
           0.05,
           0.04,
           0.13,
           0.08,
           0.09,
           0.23,
           0.13,
           0.28,
           0.23,
           0.09,
           0.07,
           0,
           0.01
          ],
          [
           0.53,
           0.52,
           0.64,
           0.82,
           0.93,
           0.94,
           0.97,
           0.95,
           0.98,
           0.98,
           0.99,
           1,
           0.19,
           0.04,
           0.18,
           0.18,
           0.19,
           0.1,
           0.02,
           0.23,
           0.14,
           0.1,
           -0.12,
           -0.44,
           -0.12,
           -0.19,
           -0.44,
           -0.6,
           0,
           0.22,
           0.05,
           0.08,
           0.01,
           -0.05,
           -0.07,
           0.18,
           0.06,
           0.12,
           0.15,
           0.1,
           0.1,
           0.07,
           0.08,
           0.06,
           0.04,
           0.13,
           0.08,
           0.09,
           0.23,
           0.13,
           0.28,
           0.23,
           0.09,
           0.07,
           0,
           0.01
          ],
          [
           -0.11,
           -0.11,
           -0.06,
           0.14,
           0.14,
           0.14,
           0.1,
           0.18,
           0.21,
           0.15,
           0.15,
           0.19,
           1,
           0.74,
           0.36,
           0.36,
           0.46,
           0.04,
           0.08,
           0.25,
           0.29,
           0.16,
           -0.9,
           -0.38,
           -0.38,
           0.13,
           0.18,
           0.14,
           -0.03,
           0.02,
           0,
           0.02,
           0,
           -0.02,
           0.02,
           0.12,
           -0.09,
           0.24,
           0.46,
           -0.02,
           0.18,
           0.01,
           0.01,
           0,
           0,
           0.02,
           0.01,
           0.01,
           0.02,
           0.02,
           0.04,
           0.04,
           0.01,
           0.01,
           0,
           0
          ],
          [
           -0.17,
           -0.17,
           -0.19,
           -0.02,
           0.01,
           0.03,
           -0.03,
           0.03,
           0.09,
           0.01,
           0,
           0.04,
           0.74,
           1,
           0.56,
           0.65,
           0.76,
           0.08,
           0.43,
           0.37,
           0.52,
           0.43,
           -0.67,
           -0.56,
           -0.73,
           0.38,
           0.14,
           0.25,
           -0.04,
           -0.01,
           0.01,
           0.02,
           0,
           0.01,
           0.15,
           0.26,
           0.17,
           0.17,
           0.32,
           -0.03,
           -0.04,
           0,
           0,
           0,
           0,
           0.01,
           0,
           0.01,
           -0.01,
           -0.01,
           0,
           0,
           -0.01,
           -0.01,
           0,
           0
          ],
          [
           -0.1,
           -0.11,
           -0.1,
           0.07,
           0.11,
           0.17,
           0.11,
           0.18,
           0.21,
           0.13,
           0.17,
           0.18,
           0.36,
           0.56,
           1,
           0.87,
           0.82,
           0.52,
           0.58,
           0.69,
           0.79,
           0.68,
           -0.32,
           -0.53,
           -0.55,
           0.26,
           0.07,
           0.12,
           -0.03,
           0,
           -0.01,
           0.02,
           0,
           0.01,
           0.14,
           0.12,
           0.14,
           0.02,
           0.26,
           -0.02,
           -0.19,
           0.04,
           0.03,
           0.04,
           0.03,
           0.05,
           0.04,
           0.05,
           0.02,
           0.01,
           0.03,
           0.01,
           -0.01,
           0,
           0,
           0
          ],
          [
           -0.11,
           -0.12,
           -0.09,
           0.09,
           0.1,
           0.15,
           0.1,
           0.18,
           0.2,
           0.13,
           0.13,
           0.18,
           0.36,
           0.65,
           0.87,
           1,
           0.9,
           0.54,
           0.78,
           0.79,
           0.9,
           0.8,
           -0.29,
           -0.54,
           -0.68,
           0.26,
           0.03,
           0.11,
           -0.04,
           0,
           0,
           0.02,
           -0.01,
           0.01,
           0.17,
           0.32,
           0.37,
           0.05,
           0.21,
           -0.02,
           -0.06,
           0.04,
           0.03,
           0.04,
           0.03,
           0.05,
           0.04,
           0.05,
           0.02,
           0,
           0.02,
           0.01,
           -0.01,
           -0.01,
           0,
           0
          ],
          [
           -0.11,
           -0.12,
           -0.09,
           0.08,
           0.12,
           0.18,
           0.12,
           0.18,
           0.25,
           0.14,
           0.15,
           0.19,
           0.46,
           0.76,
           0.82,
           0.9,
           1,
           0.4,
           0.66,
           0.67,
           0.78,
           0.73,
           -0.42,
           -0.65,
           -0.75,
           0.3,
           0.05,
           0.11,
           -0.05,
           0,
           0,
           0.02,
           0,
           0.01,
           0.17,
           0.27,
           0.24,
           0.02,
           0.2,
           -0.02,
           0.06,
           0.03,
           0.03,
           0.03,
           0.02,
           0.04,
           0.03,
           0.04,
           0.02,
           0,
           0.03,
           0.01,
           0,
           -0.01,
           0,
           0
          ],
          [
           -0.07,
           -0.07,
           -0.03,
           0.16,
           0.04,
           0.07,
           0.08,
           0.17,
           0.11,
           0.09,
           0.08,
           0.1,
           0.04,
           0.08,
           0.52,
           0.54,
           0.4,
           1,
           0.47,
           0.78,
           0.69,
           0.62,
           -0.02,
           -0.03,
           -0.04,
           0.06,
           0.08,
           0.03,
           0.02,
           -0.01,
           -0.01,
           0,
           -0.02,
           -0.02,
           0.03,
           0.1,
           0.3,
           0.1,
           0.14,
           -0.03,
           0.06,
           0.09,
           0.09,
           0.12,
           0.11,
           0.11,
           0.12,
           0.12,
           0,
           0,
           0,
           0,
           -0.03,
           -0.01,
           -0.01,
           -0.02
          ],
          [
           -0.14,
           -0.14,
           -0.13,
           0.04,
           0.05,
           0.04,
           0,
           0.08,
           0.03,
           0.03,
           -0.03,
           0.02,
           0.08,
           0.43,
           0.58,
           0.78,
           0.66,
           0.47,
           1,
           0.68,
           0.78,
           0.76,
           -0.04,
           -0.29,
           -0.53,
           0.14,
           0.15,
           0.13,
           -0.01,
           -0.03,
           -0.01,
           0.02,
           -0.01,
           0.01,
           0.27,
           0.48,
           0.68,
           -0.07,
           -0.02,
           -0.03,
           0.09,
           0.01,
           0.02,
           0.02,
           0.02,
           0.02,
           0.02,
           0.02,
           -0.02,
           -0.01,
           -0.02,
           -0.03,
           -0.02,
           -0.01,
           0,
           0
          ],
          [
           -0.05,
           -0.05,
           -0.03,
           0.24,
           0.15,
           0.19,
           0.2,
           0.33,
           0.26,
           0.22,
           0.19,
           0.23,
           0.25,
           0.37,
           0.69,
           0.79,
           0.67,
           0.78,
           0.68,
           1,
           0.91,
           0.83,
           -0.2,
           -0.24,
           -0.3,
           0.13,
           0.09,
           0.05,
           -0.01,
           0.01,
           -0.01,
           0.01,
           -0.01,
           -0.01,
           0.12,
           0.29,
           0.35,
           0.05,
           0.08,
           -0.02,
           0.12,
           0.08,
           0.08,
           0.1,
           0.09,
           0.1,
           0.1,
           0.1,
           0.01,
           0,
           0.02,
           0.03,
           -0.01,
           0,
           -0.01,
           0
          ],
          [
           -0.16,
           -0.16,
           -0.13,
           0.15,
           0.03,
           0.08,
           0.08,
           0.2,
           0.17,
           0.12,
           0.1,
           0.14,
           0.29,
           0.52,
           0.79,
           0.9,
           0.78,
           0.69,
           0.78,
           0.91,
           1,
           0.86,
           -0.23,
           -0.35,
           -0.49,
           0.24,
           0.11,
           0.16,
           -0.01,
           -0.01,
           -0.01,
           0.02,
           -0.01,
           0.01,
           0.14,
           0.28,
           0.34,
           0.12,
           0.11,
           -0.03,
           -0.02,
           0.05,
           0.05,
           0.06,
           0.06,
           0.07,
           0.06,
           0.07,
           0,
           -0.01,
           0.01,
           0.01,
           -0.02,
           -0.01,
           0,
           0
          ],
          [
           -0.1,
           -0.1,
           -0.21,
           0.09,
           0.01,
           0.05,
           0.08,
           0.16,
           0.17,
           0.08,
           0.07,
           0.1,
           0.16,
           0.43,
           0.68,
           0.8,
           0.73,
           0.62,
           0.76,
           0.83,
           0.86,
           1,
           -0.15,
           -0.37,
           -0.45,
           0.31,
           0.12,
           0.21,
           -0.03,
           -0.03,
           -0.01,
           0.02,
           -0.01,
           0.02,
           0.19,
           0.17,
           0.32,
           0.13,
           -0.01,
           -0.02,
           -0.01,
           0.05,
           0.04,
           0.06,
           0.06,
           0.07,
           0.06,
           0.06,
           -0.01,
           -0.01,
           0,
           0,
           -0.02,
           -0.01,
           0,
           0
          ],
          [
           0.04,
           0.04,
           0.17,
           -0.13,
           -0.08,
           -0.08,
           -0.05,
           -0.14,
           -0.15,
           -0.11,
           -0.11,
           -0.12,
           -0.9,
           -0.67,
           -0.32,
           -0.29,
           -0.42,
           -0.02,
           -0.04,
           -0.2,
           -0.23,
           -0.15,
           1,
           0.32,
           0.31,
           -0.14,
           -0.21,
           -0.23,
           0.02,
           -0.01,
           0.01,
           -0.02,
           0,
           0.02,
           -0.03,
           -0.07,
           0.17,
           -0.23,
           -0.4,
           0.01,
           -0.17,
           -0.01,
           0,
           0,
           0,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           -0.02,
           -0.04,
           -0.03,
           -0.01,
           -0.01,
           0,
           0
          ],
          [
           -0.02,
           -0.01,
           -0.14,
           -0.14,
           -0.34,
           -0.44,
           -0.36,
           -0.3,
           -0.51,
           -0.35,
           -0.41,
           -0.44,
           -0.38,
           -0.56,
           -0.53,
           -0.54,
           -0.65,
           -0.03,
           -0.29,
           -0.24,
           -0.35,
           -0.37,
           0.32,
           1,
           0.76,
           -0.37,
           0.22,
           0.11,
           0.09,
           -0.08,
           -0.04,
           -0.05,
           -0.01,
           -0.01,
           -0.03,
           -0.11,
           -0.08,
           0.04,
           -0.1,
           0,
           0.07,
           -0.03,
           -0.03,
           -0.01,
           -0.01,
           -0.05,
           -0.03,
           -0.04,
           -0.07,
           -0.04,
           -0.1,
           -0.08,
           -0.02,
           -0.01,
           0,
           0
          ],
          [
           0.13,
           0.13,
           -0.01,
           0.06,
           -0.1,
           -0.15,
           -0.08,
           -0.07,
           -0.15,
           -0.1,
           -0.08,
           -0.12,
           -0.38,
           -0.73,
           -0.55,
           -0.68,
           -0.75,
           -0.04,
           -0.53,
           -0.3,
           -0.49,
           -0.45,
           0.31,
           0.76,
           1,
           -0.25,
           0.05,
           -0.03,
           0.05,
           0,
           -0.01,
           -0.03,
           0,
           -0.01,
           -0.09,
           -0.25,
           -0.24,
           0.11,
           0.01,
           0.02,
           0.08,
           -0.01,
           -0.01,
           0,
           0,
           -0.02,
           0,
           -0.01,
           -0.01,
           0,
           -0.02,
           -0.02,
           0.01,
           0.01,
           0,
           0
          ],
          [
           -0.24,
           -0.23,
           -0.61,
           -0.38,
           -0.38,
           -0.28,
           -0.26,
           -0.33,
           -0.03,
           -0.3,
           -0.21,
           -0.19,
           0.13,
           0.38,
           0.26,
           0.26,
           0.3,
           0.06,
           0.14,
           0.13,
           0.24,
           0.31,
           -0.14,
           -0.37,
           -0.25,
           1,
           -0.14,
           0.54,
           -0.07,
           -0.06,
           0.03,
           0.04,
           0,
           0.08,
           0.14,
           -0.18,
           -0.03,
           0.21,
           0.02,
           -0.04,
           -0.27,
           -0.02,
           -0.02,
           0,
           0,
           0,
           0,
           0.01,
           -0.1,
           -0.06,
           -0.06,
           -0.06,
           -0.03,
           -0.03,
           0,
           0
          ],
          [
           -0.09,
           -0.08,
           -0.53,
           -0.09,
           -0.31,
           -0.51,
           -0.52,
           -0.22,
           -0.43,
           -0.4,
           -0.45,
           -0.44,
           0.18,
           0.14,
           0.07,
           0.03,
           0.05,
           0.08,
           0.15,
           0.09,
           0.11,
           0.12,
           -0.21,
           0.22,
           0.05,
           -0.14,
           1,
           0.62,
           0.04,
           -0.13,
           -0.07,
           -0.02,
           -0.01,
           0.01,
           0.44,
           0.04,
           0.07,
           0.2,
           -0.05,
           -0.02,
           0.31,
           0,
           -0.04,
           -0.02,
           -0.02,
           -0.05,
           -0.03,
           -0.04,
           -0.07,
           -0.05,
           -0.11,
           -0.09,
           -0.04,
           0,
           0,
           0
          ],
          [
           -0.22,
           -0.2,
           -0.98,
           -0.34,
           -0.67,
           -0.78,
           -0.71,
           -0.51,
           -0.51,
           -0.61,
           -0.6,
           -0.6,
           0.14,
           0.25,
           0.12,
           0.11,
           0.11,
           0.03,
           0.13,
           0.05,
           0.16,
           0.21,
           -0.23,
           0.11,
           -0.03,
           0.54,
           0.62,
           1,
           0.02,
           -0.17,
           -0.05,
           0.01,
           -0.01,
           0.06,
           0.42,
           -0.05,
           -0.03,
           0.43,
           -0.08,
           -0.03,
           -0.11,
           -0.03,
           -0.06,
           -0.03,
           -0.03,
           -0.06,
           -0.04,
           -0.04,
           -0.14,
           -0.09,
           -0.15,
           -0.13,
           -0.06,
           -0.02,
           0,
           0
          ],
          [
           0.11,
           0.1,
           -0.01,
           0.05,
           0.01,
           -0.02,
           0,
           0.03,
           -0.01,
           0.02,
           0.01,
           0,
           -0.03,
           -0.04,
           -0.03,
           -0.04,
           -0.05,
           0.02,
           -0.01,
           -0.01,
           -0.01,
           -0.03,
           0.02,
           0.09,
           0.05,
           -0.07,
           0.04,
           0.02,
           1,
           0.33,
           0.24,
           -0.02,
           -0.08,
           -0.09,
           0.09,
           -0.02,
           -0.02,
           0.04,
           -0.04,
           0.06,
           -0.02,
           0.25,
           0.11,
           0.1,
           0.06,
           0.08,
           0.12,
           0.08,
           0.19,
           0.3,
           0.25,
           0.2,
           0.06,
           0,
           0.02,
           0.06
          ],
          [
           0.12,
           0.11,
           0.17,
           0.17,
           0.21,
           0.21,
           0.22,
           0.2,
           0.21,
           0.22,
           0.22,
           0.22,
           0.02,
           -0.01,
           0,
           0,
           0,
           -0.01,
           -0.03,
           0.01,
           -0.01,
           -0.03,
           -0.01,
           -0.08,
           0,
           -0.06,
           -0.13,
           -0.17,
           0.33,
           1,
           0.81,
           0.02,
           -0.03,
           -0.06,
           -0.01,
           0.01,
           0,
           0.02,
           0.04,
           0.01,
           0.02,
           0.24,
           0.1,
           0.03,
           0.03,
           0.08,
           0.06,
           0.06,
           0.44,
           0.47,
           0.5,
           0.44,
           0.23,
           0.03,
           0.08,
           0.09
          ],
          [
           0.02,
           0.01,
           0.05,
           0.01,
           0.05,
           0.06,
           0.05,
           0.03,
           0.05,
           0.04,
           0.05,
           0.05,
           0,
           0.01,
           -0.01,
           0,
           0,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           0.01,
           -0.04,
           -0.01,
           0.03,
           -0.07,
           -0.05,
           0.24,
           0.81,
           1,
           -0.03,
           -0.11,
           -0.13,
           0.02,
           -0.02,
           0.01,
           -0.02,
           0.02,
           -0.08,
           0,
           0.2,
           0.06,
           -0.01,
           -0.02,
           0.02,
           0.01,
           0,
           0.11,
           0.22,
           0.16,
           0.12,
           -0.06,
           -0.03,
           -0.02,
           0.03
          ],
          [
           0.07,
           0.09,
           -0.01,
           0.09,
           0.07,
           0.06,
           0.08,
           0.08,
           0.09,
           0.09,
           0.08,
           0.08,
           0.02,
           0.02,
           0.02,
           0.02,
           0.02,
           0,
           0.02,
           0.01,
           0.02,
           0.02,
           -0.02,
           -0.05,
           -0.03,
           0.04,
           -0.02,
           0.01,
           -0.02,
           0.02,
           -0.03,
           1,
           0.18,
           0.28,
           0.05,
           0.04,
           0.03,
           0.05,
           -0.03,
           0,
           -0.01,
           -0.03,
           -0.07,
           0,
           -0.01,
           0.02,
           0,
           0.01,
           -0.02,
           -0.1,
           0.02,
           0.01,
           0.04,
           -0.02,
           0.02,
           -0.01
          ],
          [
           -0.01,
           0,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0,
           0,
           0,
           -0.01,
           0,
           -0.02,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           0,
           -0.01,
           0,
           0,
           -0.01,
           -0.01,
           -0.08,
           -0.03,
           -0.11,
           0.18,
           1,
           0.5,
           -0.12,
           0.01,
           0,
           0,
           0,
           -0.11,
           0,
           -0.22,
           -0.22,
           -0.12,
           -0.09,
           -0.11,
           -0.12,
           -0.11,
           -0.08,
           -0.29,
           -0.08,
           -0.07,
           0.17,
           -0.1,
           0.02,
           -0.02
          ],
          [
           -0.04,
           -0.04,
           -0.07,
           -0.05,
           -0.06,
           -0.05,
           -0.04,
           -0.05,
           -0.04,
           -0.05,
           -0.05,
           -0.05,
           -0.02,
           0.01,
           0.01,
           0.01,
           0.01,
           -0.02,
           0.01,
           -0.01,
           0.01,
           0.02,
           0.02,
           -0.01,
           -0.01,
           0.08,
           0.01,
           0.06,
           -0.09,
           -0.06,
           -0.13,
           0.28,
           0.5,
           1,
           -0.04,
           -0.01,
           -0.01,
           -0.01,
           -0.03,
           -0.11,
           -0.03,
           -0.23,
           -0.24,
           -0.13,
           -0.11,
           -0.1,
           -0.12,
           -0.11,
           -0.07,
           -0.33,
           -0.13,
           -0.11,
           0.11,
           -0.11,
           0.01,
           -0.03
          ],
          [
           0.13,
           0.13,
           -0.42,
           0.03,
           0.01,
           -0.14,
           -0.15,
           0.04,
           -0.05,
           -0.06,
           -0.1,
           -0.07,
           0.02,
           0.15,
           0.14,
           0.17,
           0.17,
           0.03,
           0.27,
           0.12,
           0.14,
           0.19,
           -0.03,
           -0.03,
           -0.09,
           0.14,
           0.44,
           0.42,
           0.09,
           -0.01,
           0.02,
           0.05,
           -0.12,
           -0.04,
           1,
           0.26,
           0.21,
           0.14,
           -0.06,
           0,
           0.06,
           0.06,
           -0.01,
           0,
           -0.02,
           0,
           -0.01,
           -0.01,
           0,
           0,
           0.01,
           0,
           -0.03,
           -0.02,
           -0.09,
           -0.05
          ],
          [
           0.16,
           0.15,
           0.09,
           0.29,
           0.26,
           0.19,
           0.15,
           0.25,
           0.12,
           0.22,
           0.13,
           0.18,
           0.12,
           0.26,
           0.12,
           0.32,
           0.27,
           0.1,
           0.48,
           0.29,
           0.28,
           0.17,
           -0.07,
           -0.11,
           -0.25,
           -0.18,
           0.04,
           -0.05,
           -0.02,
           0.01,
           -0.02,
           0.04,
           0.01,
           -0.01,
           0.26,
           1,
           0.51,
           0.07,
           0.08,
           0.03,
           0.11,
           0.01,
           0.02,
           0.01,
           0.02,
           0.02,
           0.02,
           0.02,
           0.04,
           0.03,
           0.04,
           0.04,
           0.02,
           0.02,
           0.01,
           0.01
          ],
          [
           -0.02,
           -0.02,
           0.04,
           0.12,
           0.18,
           0.1,
           0.04,
           0.1,
           0.04,
           0.08,
           0.01,
           0.06,
           -0.09,
           0.17,
           0.14,
           0.37,
           0.24,
           0.3,
           0.68,
           0.35,
           0.34,
           0.32,
           0.17,
           -0.08,
           -0.24,
           -0.03,
           0.07,
           -0.03,
           -0.02,
           0,
           0.01,
           0.03,
           0,
           -0.01,
           0.21,
           0.51,
           1,
           0.02,
           0.09,
           -0.01,
           0.31,
           0,
           0.01,
           0.01,
           0.02,
           0.01,
           0.01,
           0.01,
           -0.01,
           0,
           -0.01,
           0,
           0,
           0,
           0,
           0
          ],
          [
           0.12,
           0.13,
           -0.35,
           0.4,
           -0.02,
           -0.15,
           -0.02,
           0.15,
           0.16,
           0.07,
           0.12,
           0.12,
           0.24,
           0.17,
           0.02,
           0.05,
           0.02,
           0.1,
           -0.07,
           0.05,
           0.12,
           0.13,
           -0.23,
           0.04,
           0.11,
           0.21,
           0.2,
           0.43,
           0.04,
           0.02,
           -0.02,
           0.05,
           0,
           -0.01,
           0.14,
           0.07,
           0.02,
           1,
           0.27,
           0.02,
           -0.09,
           0.02,
           0,
           0,
           -0.01,
           0.03,
           0,
           0.01,
           0.03,
           0,
           0.06,
           0.04,
           0.01,
           0.03,
           0,
           0
          ],
          [
           -0.15,
           -0.16,
           0.1,
           0.02,
           0.14,
           0.19,
           0.07,
           0.1,
           0.13,
           0.1,
           0.16,
           0.15,
           0.46,
           0.32,
           0.26,
           0.21,
           0.2,
           0.14,
           -0.02,
           0.08,
           0.11,
           -0.01,
           -0.4,
           -0.1,
           0.01,
           0.02,
           -0.05,
           -0.08,
           -0.04,
           0.04,
           0.02,
           -0.03,
           0,
           -0.03,
           -0.06,
           0.08,
           0.09,
           0.27,
           1,
           -0.03,
           -0.04,
           0.01,
           0.01,
           0,
           -0.01,
           0.01,
           0.01,
           0.01,
           0.05,
           0.01,
           0.03,
           0.01,
           0.01,
           -0.02,
           0,
           0
          ],
          [
           0.18,
           0.17,
           0.04,
           0.1,
           0.1,
           0.08,
           0.1,
           0.1,
           0.09,
           0.1,
           0.1,
           0.1,
           -0.02,
           -0.03,
           -0.02,
           -0.02,
           -0.02,
           -0.03,
           -0.03,
           -0.02,
           -0.03,
           -0.02,
           0.01,
           0,
           0.02,
           -0.04,
           -0.02,
           -0.03,
           0.06,
           0.01,
           -0.08,
           0,
           -0.11,
           -0.11,
           0,
           0.03,
           -0.01,
           0.02,
           -0.03,
           1,
           0,
           -0.07,
           -0.06,
           -0.07,
           -0.06,
           -0.06,
           -0.06,
           -0.06,
           0.12,
           0.14,
           0.16,
           0.12,
           -0.03,
           0.01,
           -0.03,
           -0.06
          ],
          [
           0.06,
           0.05,
           0.14,
           0.18,
           0.21,
           0.13,
           0.11,
           0.17,
           0.1,
           0.11,
           0.05,
           0.1,
           0.18,
           -0.04,
           -0.19,
           -0.06,
           0.06,
           0.06,
           0.09,
           0.12,
           -0.02,
           -0.01,
           -0.17,
           0.07,
           0.08,
           -0.27,
           0.31,
           -0.11,
           -0.02,
           0.02,
           0,
           -0.01,
           0,
           -0.03,
           0.06,
           0.11,
           0.31,
           -0.09,
           -0.04,
           0,
           1,
           0,
           0.01,
           -0.01,
           0.01,
           0,
           0,
           -0.01,
           0.01,
           0.02,
           0.01,
           0.03,
           0.01,
           0.01,
           0,
           0
          ],
          [
           0.04,
           0.06,
           0.03,
           0.08,
           0.07,
           0.06,
           0.07,
           0.08,
           0.07,
           0.07,
           0.07,
           0.07,
           0.01,
           0,
           0.04,
           0.04,
           0.03,
           0.09,
           0.01,
           0.08,
           0.05,
           0.05,
           -0.01,
           -0.03,
           -0.01,
           -0.02,
           0,
           -0.03,
           0.25,
           0.24,
           0.2,
           -0.03,
           -0.22,
           -0.23,
           0.06,
           0.01,
           0,
           0.02,
           0.01,
           -0.07,
           0,
           1,
           0.83,
           0.85,
           0.81,
           0.85,
           0.86,
           0.84,
           0.11,
           0.3,
           0.27,
           0.19,
           0.01,
           0.2,
           0.15,
           0.17
          ],
          [
           0.03,
           0.04,
           0.06,
           0.07,
           0.08,
           0.08,
           0.08,
           0.08,
           0.08,
           0.08,
           0.08,
           0.08,
           0.01,
           0,
           0.03,
           0.03,
           0.03,
           0.09,
           0.02,
           0.08,
           0.05,
           0.04,
           0,
           -0.03,
           -0.01,
           -0.02,
           -0.04,
           -0.06,
           0.11,
           0.1,
           0.06,
           -0.07,
           -0.22,
           -0.24,
           -0.01,
           0.02,
           0.01,
           0,
           0.01,
           -0.06,
           0.01,
           0.83,
           1,
           0.93,
           0.94,
           0.93,
           0.93,
           0.94,
           0.15,
           0.39,
           0.3,
           0.24,
           0.14,
           0.3,
           0.28,
           0.25
          ],
          [
           0.02,
           0.02,
           0.03,
           0.05,
           0.05,
           0.05,
           0.06,
           0.06,
           0.06,
           0.06,
           0.05,
           0.06,
           0,
           0,
           0.04,
           0.04,
           0.03,
           0.12,
           0.02,
           0.1,
           0.06,
           0.06,
           0,
           -0.01,
           0,
           0,
           -0.02,
           -0.03,
           0.1,
           0.03,
           -0.01,
           0,
           -0.12,
           -0.13,
           0,
           0.01,
           0.01,
           0,
           0,
           -0.07,
           -0.01,
           0.85,
           0.93,
           1,
           0.97,
           0.97,
           0.98,
           0.98,
           0.09,
           0.25,
           0.25,
           0.19,
           0.11,
           0.25,
           0.22,
           0.21
          ],
          [
           0.01,
           0.01,
           0.02,
           0.04,
           0.04,
           0.04,
           0.05,
           0.05,
           0.04,
           0.04,
           0.04,
           0.04,
           0,
           0,
           0.03,
           0.03,
           0.02,
           0.11,
           0.02,
           0.09,
           0.06,
           0.06,
           0,
           -0.01,
           0,
           0,
           -0.02,
           -0.03,
           0.06,
           0.03,
           -0.02,
           -0.01,
           -0.09,
           -0.11,
           -0.02,
           0.02,
           0.02,
           -0.01,
           -0.01,
           -0.06,
           0.01,
           0.81,
           0.94,
           0.97,
           1,
           0.95,
           0.96,
           0.96,
           0.1,
           0.26,
           0.24,
           0.2,
           0.15,
           0.26,
           0.25,
           0.22
          ],
          [
           0.06,
           0.06,
           0.06,
           0.12,
           0.11,
           0.12,
           0.13,
           0.13,
           0.13,
           0.13,
           0.13,
           0.13,
           0.02,
           0.01,
           0.05,
           0.05,
           0.04,
           0.11,
           0.02,
           0.1,
           0.07,
           0.07,
           -0.01,
           -0.05,
           -0.02,
           0,
           -0.05,
           -0.06,
           0.08,
           0.08,
           0.02,
           0.02,
           -0.11,
           -0.1,
           0,
           0.02,
           0.01,
           0.03,
           0.01,
           -0.06,
           0,
           0.85,
           0.93,
           0.97,
           0.95,
           1,
           0.97,
           0.99,
           0.14,
           0.27,
           0.31,
           0.25,
           0.13,
           0.26,
           0.24,
           0.21
          ],
          [
           0.03,
           0.04,
           0.04,
           0.08,
           0.07,
           0.08,
           0.08,
           0.09,
           0.08,
           0.08,
           0.08,
           0.08,
           0.01,
           0,
           0.04,
           0.04,
           0.03,
           0.12,
           0.02,
           0.1,
           0.06,
           0.06,
           -0.01,
           -0.03,
           0,
           0,
           -0.03,
           -0.04,
           0.12,
           0.06,
           0.01,
           0,
           -0.12,
           -0.12,
           -0.01,
           0.02,
           0.01,
           0,
           0.01,
           -0.06,
           0,
           0.86,
           0.93,
           0.98,
           0.96,
           0.97,
           1,
           0.98,
           0.1,
           0.26,
           0.27,
           0.22,
           0.09,
           0.24,
           0.2,
           0.19
          ],
          [
           0.03,
           0.04,
           0.04,
           0.08,
           0.07,
           0.08,
           0.09,
           0.09,
           0.09,
           0.09,
           0.09,
           0.09,
           0.01,
           0.01,
           0.05,
           0.05,
           0.04,
           0.12,
           0.02,
           0.1,
           0.07,
           0.06,
           -0.01,
           -0.04,
           -0.01,
           0.01,
           -0.04,
           -0.04,
           0.08,
           0.06,
           0,
           0.01,
           -0.11,
           -0.11,
           -0.01,
           0.02,
           0.01,
           0.01,
           0.01,
           -0.06,
           -0.01,
           0.84,
           0.94,
           0.98,
           0.96,
           0.99,
           0.98,
           1,
           0.11,
           0.25,
           0.28,
           0.23,
           0.13,
           0.25,
           0.24,
           0.21
          ],
          [
           0.14,
           0.14,
           0.16,
           0.21,
           0.23,
           0.22,
           0.23,
           0.23,
           0.22,
           0.24,
           0.23,
           0.23,
           0.02,
           -0.01,
           0.02,
           0.02,
           0.02,
           0,
           -0.02,
           0.01,
           0,
           -0.01,
           -0.01,
           -0.07,
           -0.01,
           -0.1,
           -0.07,
           -0.14,
           0.19,
           0.44,
           0.11,
           -0.02,
           -0.08,
           -0.07,
           0,
           0.04,
           -0.01,
           0.03,
           0.05,
           0.12,
           0.01,
           0.11,
           0.15,
           0.09,
           0.1,
           0.14,
           0.1,
           0.11,
           1,
           0.67,
           0.73,
           0.64,
           0.54,
           0.09,
           0.16,
           0.13
          ],
          [
           0.09,
           0.07,
           0.1,
           0.11,
           0.13,
           0.13,
           0.13,
           0.13,
           0.12,
           0.13,
           0.13,
           0.13,
           0.02,
           -0.01,
           0.01,
           0,
           0,
           0,
           -0.01,
           0,
           -0.01,
           -0.01,
           -0.02,
           -0.04,
           0,
           -0.06,
           -0.05,
           -0.09,
           0.3,
           0.47,
           0.22,
           -0.1,
           -0.29,
           -0.33,
           0,
           0.03,
           0,
           0,
           0.01,
           0.14,
           0.02,
           0.3,
           0.39,
           0.25,
           0.26,
           0.27,
           0.26,
           0.25,
           0.67,
           1,
           0.87,
           0.73,
           0.59,
           0.48,
           0.52,
           0.46
          ],
          [
           0.16,
           0.15,
           0.16,
           0.25,
           0.26,
           0.25,
           0.27,
           0.27,
           0.26,
           0.28,
           0.28,
           0.28,
           0.04,
           0,
           0.03,
           0.02,
           0.03,
           0,
           -0.02,
           0.02,
           0.01,
           0,
           -0.04,
           -0.1,
           -0.02,
           -0.06,
           -0.11,
           -0.15,
           0.25,
           0.5,
           0.16,
           0.02,
           -0.08,
           -0.13,
           0.01,
           0.04,
           -0.01,
           0.06,
           0.03,
           0.16,
           0.01,
           0.27,
           0.3,
           0.25,
           0.24,
           0.31,
           0.27,
           0.28,
           0.73,
           0.87,
           1,
           0.83,
           0.7,
           0.45,
           0.5,
           0.43
          ],
          [
           0.15,
           0.13,
           0.14,
           0.21,
           0.22,
           0.21,
           0.23,
           0.23,
           0.22,
           0.24,
           0.23,
           0.23,
           0.04,
           0,
           0.01,
           0.01,
           0.01,
           0,
           -0.03,
           0.03,
           0.01,
           0,
           -0.03,
           -0.08,
           -0.02,
           -0.06,
           -0.09,
           -0.13,
           0.2,
           0.44,
           0.12,
           0.01,
           -0.07,
           -0.11,
           0,
           0.04,
           0,
           0.04,
           0.01,
           0.12,
           0.03,
           0.19,
           0.24,
           0.19,
           0.2,
           0.25,
           0.22,
           0.23,
           0.64,
           0.73,
           0.83,
           1,
           0.58,
           0.35,
           0.37,
           0.32
          ],
          [
           0.05,
           0.04,
           0.06,
           0.08,
           0.09,
           0.08,
           0.09,
           0.08,
           0.08,
           0.09,
           0.09,
           0.09,
           0.01,
           -0.01,
           -0.01,
           -0.01,
           0,
           -0.03,
           -0.02,
           -0.01,
           -0.02,
           -0.02,
           -0.01,
           -0.02,
           0.01,
           -0.03,
           -0.04,
           -0.06,
           0.06,
           0.23,
           -0.06,
           0.04,
           0.17,
           0.11,
           -0.03,
           0.02,
           0,
           0.01,
           0.01,
           -0.03,
           0.01,
           0.01,
           0.14,
           0.11,
           0.15,
           0.13,
           0.09,
           0.13,
           0.54,
           0.59,
           0.7,
           0.58,
           1,
           0.57,
           0.75,
           0.66
          ],
          [
           0.04,
           0.05,
           0.03,
           0.08,
           0.07,
           0.05,
           0.06,
           0.07,
           0.06,
           0.07,
           0.07,
           0.07,
           0.01,
           -0.01,
           0,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           0,
           -0.01,
           -0.01,
           -0.01,
           -0.01,
           0.01,
           -0.03,
           0,
           -0.02,
           0,
           0.03,
           -0.03,
           -0.02,
           -0.1,
           -0.11,
           -0.02,
           0.02,
           0,
           0.03,
           -0.02,
           0.01,
           0.01,
           0.2,
           0.3,
           0.25,
           0.26,
           0.26,
           0.24,
           0.25,
           0.09,
           0.48,
           0.45,
           0.35,
           0.57,
           1,
           0.83,
           0.68
          ],
          [
           -0.02,
           -0.03,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           -0.01,
           0,
           -0.01,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.02,
           0.08,
           -0.02,
           0.02,
           0.02,
           0.01,
           -0.09,
           0.01,
           0,
           0,
           0,
           -0.03,
           0,
           0.15,
           0.28,
           0.22,
           0.25,
           0.24,
           0.2,
           0.24,
           0.16,
           0.52,
           0.5,
           0.37,
           0.75,
           0.83,
           1,
           0.89
          ],
          [
           -0.01,
           -0.02,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0.01,
           0,
           0,
           0,
           0,
           0,
           -0.02,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0,
           0.06,
           0.09,
           0.03,
           -0.01,
           -0.02,
           -0.03,
           -0.05,
           0.01,
           0,
           0,
           0,
           -0.06,
           0,
           0.17,
           0.25,
           0.21,
           0.22,
           0.21,
           0.19,
           0.21,
           0.13,
           0.46,
           0.43,
           0.32,
           0.66,
           0.68,
           0.89,
           1
          ]
         ],
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "EBG",
          "FFO",
          "commodity_trade_Close",
          "Three_Month_Yield",
          "utilities_Close",
          "C_Staples_Close",
          "Health_care_Close",
          "information_Close",
          "materials_Close",
          "C_Discretionary_Close",
          "Financials_Close",
          "industrials_Close",
          "Brent_Close_quarterly_return",
          "Energy_Close_quarterly_return",
          "Financials_Close_quarterly_return",
          "industrials_Close_quarterly_return",
          "materials_Close_quarterly_return",
          "Three_Month_Yield_quarterly_return",
          "C_Staples_Close_quarterly_return",
          "information_Close_quarterly_return",
          "C_Discretionary_Close_quarterly_return",
          "Health_care_Close_quarterly_return",
          "commodity_trade_Close_quarterly_return",
          "VIX_Close",
          "VIX_Close_quarterly_return",
          "Energy_Close",
          "Gold_Close",
          "Brent_Close",
          "CPS",
          "MEAN",
          "STDEV",
          "ROE",
          "GRM",
          "ROA",
          "Real_Estate_Index_Price",
          "Real_Estate_Index_Price_quarterly_return",
          "utilities_Close_quarterly_return",
          "Hrc_close",
          "Hrc_close_quarterly_return",
          "CSH",
          "Gold_Close_quarterly_return",
          "SAL",
          "NAV",
          "EBT",
          "OPR",
          "NET",
          "EBI",
          "PRE",
          "DPS",
          "BPS",
          "EPS",
          "GPS",
          "EBS",
          "CPX",
          "ENT",
          "NDT"
         ],
         "y": [
          "EBG",
          "FFO",
          "commodity_trade_Close",
          "Three_Month_Yield",
          "utilities_Close",
          "C_Staples_Close",
          "Health_care_Close",
          "information_Close",
          "materials_Close",
          "C_Discretionary_Close",
          "Financials_Close",
          "industrials_Close",
          "Brent_Close_quarterly_return",
          "Energy_Close_quarterly_return",
          "Financials_Close_quarterly_return",
          "industrials_Close_quarterly_return",
          "materials_Close_quarterly_return",
          "Three_Month_Yield_quarterly_return",
          "C_Staples_Close_quarterly_return",
          "information_Close_quarterly_return",
          "C_Discretionary_Close_quarterly_return",
          "Health_care_Close_quarterly_return",
          "commodity_trade_Close_quarterly_return",
          "VIX_Close",
          "VIX_Close_quarterly_return",
          "Energy_Close",
          "Gold_Close",
          "Brent_Close",
          "CPS",
          "MEAN",
          "STDEV",
          "ROE",
          "GRM",
          "ROA",
          "Real_Estate_Index_Price",
          "Real_Estate_Index_Price_quarterly_return",
          "utilities_Close_quarterly_return",
          "Hrc_close",
          "Hrc_close_quarterly_return",
          "CSH",
          "Gold_Close_quarterly_return",
          "SAL",
          "NAV",
          "EBT",
          "OPR",
          "NET",
          "EBI",
          "PRE",
          "DPS",
          "BPS",
          "EPS",
          "GPS",
          "EBS",
          "CPX",
          "ENT",
          "NDT"
         ],
         "z": [
          [
           1,
           0.9490836964864436,
           0.2381776548621473,
           0.5647889458261961,
           0.55719355765835,
           0.45849172375946384,
           0.5287768831855216,
           0.5832737839185593,
           0.48271389234138606,
           0.5755096708250129,
           0.5255899500133793,
           0.5294174638417082,
           -0.10567035975530474,
           -0.16644997782208437,
           -0.09844877303348375,
           -0.11000779143346759,
           -0.11172518878033419,
           -0.07008197052718575,
           -0.14000501346261762,
           -0.045371030879774926,
           -0.16007931820663254,
           -0.09659285170717641,
           0.038190008348639816,
           -0.015563149458167027,
           0.1254922738683701,
           -0.2430273931814662,
           -0.08559460808343647,
           -0.22088150363698486,
           0.10627810243619698,
           0.12494006708004081,
           0.019409384771374414,
           0.07307394192474484,
           -0.005803044108109921,
           -0.03604633100956447,
           0.12902470114714273,
           0.1550949026116112,
           -0.017480581007640376,
           0.11515993799539095,
           -0.1472536794139184,
           0.18002172892178026,
           0.055024734526707295,
           0.04437357386771557,
           0.03260732722944577,
           0.021278909619267608,
           0.006657421913089332,
           0.05654138985744664,
           0.03276214876367547,
           0.028326957433014638,
           0.1375489457808459,
           0.08856281544401919,
           0.16490037512106895,
           0.14770702034297928,
           0.05050721060778833,
           0.041001517103188355,
           -0.022307187273628196,
           -0.007873327936502165
          ],
          [
           0.9490836964864436,
           1,
           0.21465874676277452,
           0.5634695062999386,
           0.5443285476388379,
           0.44123885583732114,
           0.5184930639424177,
           0.576146528819173,
           0.4749579969619134,
           0.5671518149371741,
           0.5161820304714851,
           0.5190109849484688,
           -0.1086725676152516,
           -0.16790805280879398,
           -0.11103706456007086,
           -0.12170813084872821,
           -0.11967333330314274,
           -0.07186754026764917,
           -0.14203821275094067,
           -0.05153910627177115,
           -0.1643461954064535,
           -0.0960554978539139,
           0.03558415191554809,
           -0.0056214327353572655,
           0.13186444415900966,
           -0.2317488918352337,
           -0.07721652416720919,
           -0.1979398981199328,
           0.09755570607971698,
           0.1083638188250145,
           0.012444030597724668,
           0.08504355851016003,
           -0.004621827753585339,
           -0.04121078079903593,
           0.1347334580714346,
           0.14975956215526648,
           -0.021548458879295285,
           0.12753158525842148,
           -0.16093370015678019,
           0.17130758694998435,
           0.05296544069586066,
           0.05534190379007975,
           0.04040210692956116,
           0.02444298746061328,
           0.014126687385064068,
           0.06433077122235008,
           0.0391749524638194,
           0.037316277755701156,
           0.13950981982169713,
           0.07306922846436016,
           0.149833412010168,
           0.13315588023216857,
           0.03814046606459434,
           0.048991149788885205,
           -0.025682833769441533,
           -0.019500636121366297
          ],
          [
           0.2381776548621473,
           0.21465874676277452,
           1,
           0.4389575839037729,
           0.7046190038230695,
           0.7863313455574057,
           0.7230106583345582,
           0.5701552336745049,
           0.534967609570076,
           0.658924490533793,
           0.6324525151293476,
           0.6350309682853089,
           -0.05884291883995694,
           -0.1939150298833387,
           -0.09922995074131712,
           -0.0868655825321496,
           -0.08624589083173821,
           -0.031047753267128473,
           -0.12562322644054938,
           -0.030898809095735284,
           -0.12842864952020513,
           -0.2051021367149951,
           0.16640276679307414,
           -0.13929386967371288,
           -0.010593008527706936,
           -0.6116760624105366,
           -0.5288879796856472,
           -0.9785449790060089,
           -0.012320542563059804,
           0.17405776567189132,
           0.04850212010719168,
           -0.009728950409117478,
           0.007507673946448987,
           -0.07117177197988575,
           -0.4191689104973375,
           0.09304372772086318,
           0.03514340046693614,
           -0.34918431759741875,
           0.09697164776644776,
           0.035655845674097936,
           0.13936932562215765,
           0.034199247219551025,
           0.05546771383301571,
           0.025911927430619642,
           0.023661507275328685,
           0.05969632909564368,
           0.042237936821644166,
           0.040275235754677376,
           0.15835725164241804,
           0.09501761241835162,
           0.16280590827735067,
           0.1417341536750063,
           0.059457044887201584,
           0.027075653646306472,
           0.00011640107190988665,
           0.00565480732758031
          ],
          [
           0.5647889458261961,
           0.5634695062999386,
           0.4389575839037729,
           1,
           0.7855919915695732,
           0.6666492751714512,
           0.7811003550643558,
           0.9020075577701704,
           0.7623196327504825,
           0.8567869662428633,
           0.8096228023360402,
           0.8196421043952123,
           0.1432640273945476,
           -0.02276045418929416,
           0.06798918022221863,
           0.09362744610542728,
           0.08002640399013289,
           0.16283578889433092,
           0.04165956093788422,
           0.24166699578051443,
           0.14716408921963745,
           0.08921468086936798,
           -0.12635118287438812,
           -0.14030595234304444,
           0.06406184345520721,
           -0.3829862499698254,
           -0.08698179613976788,
           -0.34300854886386367,
           0.054625316681871434,
           0.17491731404055139,
           0.01146866028221214,
           0.09246713368325743,
           0.005271324947218961,
           -0.05259269356857626,
           0.03373366241247356,
           0.2923759400904117,
           0.11797794244387573,
           0.3966770947758288,
           0.01826849662464291,
           0.1018203111130505,
           0.17775027415223135,
           0.07705613038141887,
           0.06984811346138481,
           0.0523481253223899,
           0.04204831049925436,
           0.11855510396183054,
           0.0754585456808525,
           0.07517279887504706,
           0.20855976416468805,
           0.11016246275550752,
           0.24887133356270022,
           0.2125349176167841,
           0.07733631770971555,
           0.07750108765779828,
           -0.0014791837473654196,
           0.008638491397458245
          ],
          [
           0.55719355765835,
           0.5443285476388379,
           0.7046190038230695,
           0.7855919915695732,
           1,
           0.9482687626585667,
           0.9300789889802077,
           0.9276072405873633,
           0.8774879007407469,
           0.9455026252346257,
           0.9164260829551436,
           0.9327492735192651,
           0.13981892242373334,
           0.012852646417526975,
           0.10587455679027498,
           0.09553850566787853,
           0.12303808873346322,
           0.03958696695098543,
           0.048432459025094285,
           0.15234424221655568,
           0.03431748200350414,
           0.008983165470838468,
           -0.07997872615217197,
           -0.3440423185860083,
           -0.09954486243191601,
           -0.38004705981911185,
           -0.30656650011309805,
           -0.6738369277850481,
           0.011147361231729613,
           0.20973454805427058,
           0.04518977397054094,
           0.07412288924067655,
           0.009532619113872907,
           -0.055117842793691675,
           0.01380058324167279,
           0.2644883730693056,
           0.1841078307897975,
           -0.021145158648052387,
           0.13732977384459755,
           0.10025061821630528,
           0.21255697723439598,
           0.06543012594895929,
           0.07612540200277036,
           0.04569584782750142,
           0.03617664217545269,
           0.10962815153883401,
           0.06894893909530812,
           0.07161579885762054,
           0.22548120570341706,
           0.13216959296555744,
           0.2594756573035441,
           0.22191320520309119,
           0.08624745502174554,
           0.06687538477099265,
           0.000053577315480782955,
           0.00794040755881214
          ],
          [
           0.45849172375946384,
           0.44123885583732114,
           0.7863313455574057,
           0.6666492751714512,
           0.9482687626585667,
           1,
           0.95745938242882,
           0.8692656386500135,
           0.8943656152698456,
           0.9327473020320688,
           0.9247952567550097,
           0.9381333919180525,
           0.13857677825767065,
           0.03026350207553038,
           0.1655650897916457,
           0.15347618269278634,
           0.18128725547324487,
           0.06801931647312001,
           0.04445921691022119,
           0.19137988867429892,
           0.081426514217856,
           0.0504891696775469,
           -0.07594709815591573,
           -0.43853464131516495,
           -0.1496686987131188,
           -0.28048460377235307,
           -0.5119900708116529,
           -0.7779799134139744,
           -0.020688290817673243,
           0.214222067936488,
           0.058186839758136176,
           0.05908489327677447,
           0.009166276681429768,
           -0.04905885052854838,
           -0.13644080100893294,
           0.19329106721682388,
           0.09594323301282734,
           -0.15326763799795884,
           0.18612839674584317,
           0.0807504605610682,
           0.12982683674202447,
           0.06407046027858136,
           0.0813403943766023,
           0.053221810172827115,
           0.04205064618393252,
           0.11525665073321731,
           0.07602277165083447,
           0.0824000570437074,
           0.21536068913273101,
           0.12548172698643945,
           0.25045946807852526,
           0.21131721324060315,
           0.08225272268319163,
           0.05179178183223123,
           0.00001042347936008765,
           0.007812650201691963
          ],
          [
           0.5287768831855216,
           0.5184930639424177,
           0.7230106583345582,
           0.7811003550643558,
           0.9300789889802077,
           0.95745938242882,
           1,
           0.9259429362558955,
           0.9313026441864108,
           0.9766065721870845,
           0.9602658144815295,
           0.968329750848174,
           0.10476908704129276,
           -0.030749283701675727,
           0.10835447020633458,
           0.10432429639584163,
           0.12035020155496952,
           0.07795128490158608,
           -0.0026020600052920214,
           0.1965389302827853,
           0.07793864025907349,
           0.07825644107452831,
           -0.04882063248177497,
           -0.35727367958822637,
           -0.08157022099896775,
           -0.2598965022987127,
           -0.5153869027207505,
           -0.7052809181059466,
           0.002692746800971107,
           0.2196591416020006,
           0.05436353984860677,
           0.07519713367570548,
           0.008604423322011707,
           -0.0446592274927523,
           -0.14921858254907056,
           0.1461736639563987,
           0.036551793985369445,
           -0.022287472797393078,
           0.07297910576269871,
           0.0951731359223699,
           0.10635203904977152,
           0.07043886806775787,
           0.08321116566784321,
           0.05809480379216063,
           0.04515319956015765,
           0.1273142317619763,
           0.08200639275498162,
           0.08862576270801625,
           0.2272710384809254,
           0.1269821462714237,
           0.2703713723523952,
           0.2307728620891947,
           0.08616579041321265,
           0.06134631946425617,
           -0.0005182861653578059,
           0.00801911179205245
          ],
          [
           0.5832737839185593,
           0.576146528819173,
           0.5701552336745049,
           0.9020075577701704,
           0.9276072405873633,
           0.8692656386500135,
           0.9259429362558955,
           1,
           0.9045418936617555,
           0.9707704871356305,
           0.9384557030867265,
           0.9467223196579155,
           0.1829296308774615,
           0.029045625753693256,
           0.18393151537069105,
           0.18036476670511725,
           0.17843816934707132,
           0.16591605406047735,
           0.0844490756415614,
           0.32579688518636335,
           0.1991891193561048,
           0.16459364593473394,
           -0.13536723229135722,
           -0.30047524920292484,
           -0.07290680702827755,
           -0.3341605285720811,
           -0.21738346794254929,
           -0.5077508938133616,
           0.03385414870308001,
           0.1980457097321248,
           0.028371741350146366,
           0.08345458495128134,
           0.006225811532693846,
           -0.05040110102668377,
           0.036939648028959104,
           0.2489951484479052,
           0.10004375503877,
           0.14813689889431317,
           0.0950069167758476,
           0.10427125891233889,
           0.1663117076976887,
           0.08269037504049404,
           0.08413257463018523,
           0.06458545663499728,
           0.051323060737403996,
           0.13125571871225175,
           0.08670793877758158,
           0.08955517294859824,
           0.23039443257050182,
           0.1251960322363448,
           0.2677905477779262,
           0.23132810902269507,
           0.08471028211030428,
           0.07245172342529724,
           -0.001729962564359082,
           0.00828431634907061
          ],
          [
           0.48271389234138606,
           0.4749579969619134,
           0.534967609570076,
           0.7623196327504825,
           0.8774879007407469,
           0.8943656152698456,
           0.9313026441864108,
           0.9045418936617555,
           1,
           0.9299139760386107,
           0.9701429759600866,
           0.9770045618438085,
           0.21366277663705593,
           0.0925919255703313,
           0.21332605848568834,
           0.19657498664262427,
           0.25014360615845527,
           0.11346767569398382,
           0.028850768972950903,
           0.25632254944239985,
           0.17256288778067658,
           0.1670963375082425,
           -0.14969214375901777,
           -0.5120872343995113,
           -0.15387002833721652,
           -0.03347663083301727,
           -0.42904228412093154,
           -0.5114789250472294,
           -0.006033322599852827,
           0.21408915895942204,
           0.05440170963806342,
           0.08536268718883964,
           0.00791273650293216,
           -0.037094133469945244,
           -0.0458474224453918,
           0.12008611435327939,
           0.035634321169622285,
           0.15768247646704187,
           0.13300791096823716,
           0.0872586950007477,
           0.09758665409405735,
           0.0726109623840008,
           0.08041984378525291,
           0.058245539709238736,
           0.04371831326874511,
           0.13070632673193122,
           0.08236333264217684,
           0.09224135363935175,
           0.21552777828161315,
           0.119176374825793,
           0.2646717188282555,
           0.22002185832600432,
           0.08192548750007889,
           0.06243543328621054,
           -0.00006203325860943357,
           0.008422004118264317
          ],
          [
           0.5755096708250129,
           0.5671518149371741,
           0.658924490533793,
           0.8567869662428633,
           0.9455026252346257,
           0.9327473020320688,
           0.9766065721870845,
           0.9707704871356305,
           0.9299139760386107,
           1,
           0.9697167695215843,
           0.9776461154718555,
           0.14839572521025657,
           0.009329840972500033,
           0.13274851656358705,
           0.13413270642761155,
           0.14182129754172268,
           0.09067085426145895,
           0.027143903252742848,
           0.21926642930846574,
           0.11993661678551727,
           0.07715727144248424,
           -0.1083497498221034,
           -0.346723651090091,
           -0.09578405025434278,
           -0.29564352223831575,
           -0.4043825468749695,
           -0.6147891863446056,
           0.01743717465551732,
           0.21523916331755188,
           0.04277215661095484,
           0.08681994456212859,
           0.008776232091046717,
           -0.04714824520432729,
           -0.06176930230689434,
           0.22153051141977098,
           0.07510579761879738,
           0.07391440506475298,
           0.10231730438317317,
           0.10454687554818487,
           0.11440833749899046,
           0.07447119513804648,
           0.08163263009899414,
           0.056753516480384784,
           0.04400992491058606,
           0.12956748963077683,
           0.08127348522522176,
           0.08735676251789809,
           0.2364291541645233,
           0.13002644182809137,
           0.28005718745126057,
           0.2378146691261771,
           0.08856988751618453,
           0.06920073595342086,
           -0.00045291324350510626,
           0.008608961651620369
          ],
          [
           0.5255899500133793,
           0.5161820304714851,
           0.6324525151293476,
           0.8096228023360402,
           0.9164260829551436,
           0.9247952567550097,
           0.9602658144815295,
           0.9384557030867265,
           0.9701429759600866,
           0.9697167695215843,
           1,
           0.9916265358109618,
           0.1544964347978122,
           -0.0028912479356132666,
           0.17327183166734056,
           0.12882320441598727,
           0.1456755130874423,
           0.08058234107871272,
           -0.033502025619068856,
           0.1910390186718229,
           0.10416103124733014,
           0.07191744020820229,
           -0.1064177579464136,
           -0.4124793485936792,
           -0.07871275546487669,
           -0.20829447873328988,
           -0.4467636941148178,
           -0.5979705261966006,
           0.00947713084548406,
           0.22123424063864597,
           0.049043670044639574,
           0.07943881771528555,
           0.00941407816742992,
           -0.04633768314173026,
           -0.09553402787418426,
           0.13436014465772167,
           0.013117378934700706,
           0.1207775734887835,
           0.15692372020847173,
           0.09513221353187987,
           0.0540003949631003,
           0.07167078666731302,
           0.07863770064450064,
           0.05429827293925442,
           0.038979974101920147,
           0.12663679062588024,
           0.07904944115091408,
           0.08612713597474983,
           0.23174425710284394,
           0.12727480262554025,
           0.27699153729037285,
           0.22895984177838283,
           0.08730288551978482,
           0.06800645465435586,
           0.0003346262597932895,
           0.008589368035587202
          ],
          [
           0.5294174638417082,
           0.5190109849484688,
           0.6350309682853089,
           0.8196421043952123,
           0.9327492735192651,
           0.9381333919180525,
           0.968329750848174,
           0.9467223196579155,
           0.9770045618438085,
           0.9776461154718555,
           0.9916265358109618,
           1,
           0.1865986591505027,
           0.043814353947952474,
           0.1806744324679263,
           0.1765917445037175,
           0.18848999573159175,
           0.09949031099525438,
           0.018500424431131382,
           0.22989778227031396,
           0.13630913472855768,
           0.1025421804019965,
           -0.12483711421547444,
           -0.43609251604862936,
           -0.11991726394893915,
           -0.192575319956597,
           -0.4420361010048875,
           -0.5996170743718556,
           0.003736251165632428,
           0.2203919861969848,
           0.050046211836822084,
           0.08242504805394245,
           0.00866695270213318,
           -0.046908451178234334,
           -0.07338777444352214,
           0.17824698593598656,
           0.06476186621719132,
           0.12060194487836183,
           0.14672033723688524,
           0.09518953331566979,
           0.10005648151931025,
           0.0727387860902134,
           0.08154379883557229,
           0.05612265236126545,
           0.04278507726292092,
           0.12947618789237011,
           0.08132081759495405,
           0.08914537616999445,
           0.2307899320455023,
           0.12756328711370915,
           0.27579346563097196,
           0.23174879440138035,
           0.08709844291141079,
           0.06636271888239749,
           -0.00003174304143950264,
           0.00878059867286731
          ],
          [
           -0.10567035975530474,
           -0.1086725676152516,
           -0.05884291883995694,
           0.1432640273945476,
           0.13981892242373334,
           0.13857677825767065,
           0.10476908704129276,
           0.1829296308774615,
           0.21366277663705593,
           0.14839572521025657,
           0.1544964347978122,
           0.1865986591505027,
           1,
           0.7439296468618886,
           0.3561863807566348,
           0.35827004968625603,
           0.4580974418463966,
           0.03605745210012348,
           0.07569306639107341,
           0.24511287665394924,
           0.2868745594379507,
           0.1639731513045619,
           -0.9007675528337117,
           -0.37805429974768195,
           -0.3843695046493461,
           0.12931935520001642,
           0.1768116026289732,
           0.14035839171204076,
           -0.03258596185156092,
           0.021308921855466872,
           -0.0007663471459661916,
           0.016666047976430794,
           -0.001852596533776769,
           -0.022448837512780224,
           0.02039576223600634,
           0.12280472641187666,
           -0.09158323299011031,
           0.2358129194127048,
           0.45548432672366496,
           -0.023845927494004673,
           0.1753742545818035,
           0.010851003493069474,
           0.007846096465308343,
           -0.002179134317563191,
           -0.00006908122321930606,
           0.017353174706591836,
           0.007295810040806446,
           0.012948840998605516,
           0.023824428210294495,
           0.01958555987158994,
           0.043027451408785854,
           0.03926510864712119,
           0.0057877620742679305,
           0.00650646685814862,
           -0.00013121205812792095,
           0.0006752437886069413
          ],
          [
           -0.16644997782208437,
           -0.16790805280879398,
           -0.1939150298833387,
           -0.02276045418929416,
           0.012852646417526975,
           0.03026350207553038,
           -0.030749283701675727,
           0.029045625753693256,
           0.0925919255703313,
           0.009329840972500033,
           -0.0028912479356132666,
           0.043814353947952474,
           0.7439296468618886,
           1,
           0.5562440801438336,
           0.6466511473156933,
           0.7579326416141808,
           0.07590969857695559,
           0.43480667868291967,
           0.3673687451776767,
           0.5160578719507507,
           0.4316862211559792,
           -0.6671081079856744,
           -0.5554242288810061,
           -0.7271663176433638,
           0.37999344731705786,
           0.13967167914886391,
           0.25136019847061913,
           -0.03735058011746879,
           -0.014402874416782258,
           0.006220338543099035,
           0.023319695016431777,
           -0.0016655711098748165,
           0.009630322717504003,
           0.15358149398143697,
           0.256767226569736,
           0.16654592097480486,
           0.16805004345735922,
           0.32223174480205063,
           -0.03154534106614697,
           -0.042173888240339484,
           0.0019123676064138177,
           -0.0007685020919298283,
           -0.0033206594442549086,
           -0.004471407658047546,
           0.009820495505211616,
           -0.001027720451470733,
           0.0078222800028322,
           -0.014122846043596099,
           -0.005999125838318191,
           0.004583575472181388,
           -0.002236307312637326,
           -0.010455338075208648,
           -0.01177551222115849,
           -0.00010497566864086708,
           -0.0015818622783690206
          ],
          [
           -0.09844877303348375,
           -0.11103706456007086,
           -0.09922995074131712,
           0.06798918022221863,
           0.10587455679027498,
           0.1655650897916457,
           0.10835447020633458,
           0.18393151537069105,
           0.21332605848568834,
           0.13274851656358705,
           0.17327183166734056,
           0.1806744324679263,
           0.3561863807566348,
           0.5562440801438336,
           1,
           0.8721226029723798,
           0.817858854794182,
           0.5169035202793575,
           0.581799974451054,
           0.6888317253096882,
           0.7899861009083055,
           0.6836404207848484,
           -0.3158066892182909,
           -0.5279869423443261,
           -0.5543829844980652,
           0.26079232201468006,
           0.07062250966159672,
           0.11661852483105559,
           -0.030467070750304935,
           -0.00019903505225027146,
           -0.005904586199871624,
           0.017850470586752976,
           -0.004359177836890809,
           0.012579009244469419,
           0.14320164196600474,
           0.12496248301585162,
           0.13725320317705814,
           0.020154848253706715,
           0.2630956164523429,
           -0.01976875418184168,
           -0.18606734535325178,
           0.04111984270461193,
           0.03037290077640823,
           0.0422942574940295,
           0.025654905222241967,
           0.049906829668761546,
           0.04219704425659732,
           0.047175258522891846,
           0.024860983705798166,
           0.006122021087149214,
           0.032489627351544974,
           0.005813373310463084,
           -0.005872240686077308,
           -0.004561007634398637,
           -0.0021190393965980466,
           -0.003235550135673405
          ],
          [
           -0.11000779143346759,
           -0.12170813084872821,
           -0.0868655825321496,
           0.09362744610542728,
           0.09553850566787853,
           0.15347618269278634,
           0.10432429639584163,
           0.18036476670511725,
           0.19657498664262427,
           0.13413270642761155,
           0.12882320441598727,
           0.1765917445037175,
           0.35827004968625603,
           0.6466511473156933,
           0.8721226029723798,
           1,
           0.8992175185542951,
           0.5439664638631324,
           0.7774822275863166,
           0.7857694719236171,
           0.8984097252721996,
           0.796004598853022,
           -0.2910351422026774,
           -0.5374462556698194,
           -0.6780580101908698,
           0.26125369288433736,
           0.0334311927427403,
           0.10890654943090537,
           -0.03609347212204058,
           -0.004761710210729915,
           -0.004700426808261579,
           0.018546044891099206,
           -0.0054662469475318665,
           0.00876364967080746,
           0.16816977088910287,
           0.3224441885325063,
           0.37329754678105415,
           0.05226928788057051,
           0.2131147901800924,
           -0.024063325532010654,
           -0.06010670896155994,
           0.04095806845674561,
           0.034901781346080456,
           0.04429569193119272,
           0.034946234245623535,
           0.05321498700348118,
           0.04469546331052669,
           0.050486093552641005,
           0.019527067418685682,
           -0.0004087927341493351,
           0.019641962409467334,
           0.010682993725853566,
           -0.008930632046425422,
           -0.011128519823351368,
           -0.002636238153640449,
           -0.002645752295054137
          ],
          [
           -0.11172518878033419,
           -0.11967333330314274,
           -0.08624589083173821,
           0.08002640399013289,
           0.12303808873346322,
           0.18128725547324487,
           0.12035020155496952,
           0.17843816934707132,
           0.25014360615845527,
           0.14182129754172268,
           0.1456755130874423,
           0.18848999573159175,
           0.4580974418463966,
           0.7579326416141808,
           0.817858854794182,
           0.8992175185542951,
           1,
           0.3989431303747742,
           0.662498186473516,
           0.6691291274904431,
           0.776613281076267,
           0.7282966710796253,
           -0.4220754230906354,
           -0.6512453554738035,
           -0.7505912438924853,
           0.30168966698955146,
           0.05314093057312371,
           0.10804663902991209,
           -0.04634672169467193,
           0.004693298428085511,
           0.0029729629221145097,
           0.021522960884135755,
           -0.003185979960957675,
           0.008388494672463259,
           0.17346481425184884,
           0.26995305781790774,
           0.2384130553711177,
           0.023780397709317707,
           0.20417458248899795,
           -0.022473640761804893,
           0.05688890486330214,
           0.03061626014660704,
           0.026413948092432117,
           0.02995498746034487,
           0.021613642460130745,
           0.0422663183145503,
           0.030478311831685426,
           0.03711077599085009,
           0.022082512027255183,
           0.004880707062188499,
           0.025816393067269874,
           0.012682677799700282,
           -0.00389384954542584,
           -0.008773424020452062,
           -0.001493942114388403,
           -0.001851309455635211
          ],
          [
           -0.07008197052718575,
           -0.07186754026764917,
           -0.031047753267128473,
           0.16283578889433092,
           0.03958696695098543,
           0.06801931647312001,
           0.07795128490158608,
           0.16591605406047735,
           0.11346767569398382,
           0.09067085426145895,
           0.08058234107871272,
           0.09949031099525438,
           0.03605745210012348,
           0.07590969857695559,
           0.5169035202793575,
           0.5439664638631324,
           0.3989431303747742,
           1,
           0.4659625984288942,
           0.7842132988160566,
           0.6874993488379493,
           0.6218238997826243,
           -0.019105526030903968,
           -0.026503972550133176,
           -0.03704467223652235,
           0.05598654216439941,
           0.07809586100901798,
           0.03240202009384742,
           0.022816914316251723,
           -0.013240205609424042,
           -0.014989918647526256,
           -0.002570270096137871,
           -0.018609834709993795,
           -0.017396773589844775,
           0.027188777512967565,
           0.09598182082813901,
           0.30351732978772744,
           0.10176596070140545,
           0.1447768893607905,
           -0.026571880450256637,
           0.06324276280031897,
           0.08903491764448616,
           0.08789466854672587,
           0.11737808756666541,
           0.11219519054222009,
           0.11044433203755546,
           0.11776853467850759,
           0.11572468633835511,
           -0.0043837885634771855,
           -0.00453075176311033,
           -0.0014105665768239524,
           -0.0017750630181504437,
           -0.025016385836222177,
           -0.008245823825335818,
           -0.01229541138433293,
           -0.023451763461548487
          ],
          [
           -0.14000501346261762,
           -0.14203821275094067,
           -0.12562322644054938,
           0.04165956093788422,
           0.048432459025094285,
           0.04445921691022119,
           -0.0026020600052920214,
           0.0844490756415614,
           0.028850768972950903,
           0.027143903252742848,
           -0.033502025619068856,
           0.018500424431131382,
           0.07569306639107341,
           0.43480667868291967,
           0.581799974451054,
           0.7774822275863166,
           0.662498186473516,
           0.4659625984288942,
           1,
           0.6776526644596313,
           0.7755773915534113,
           0.7574602196942488,
           -0.03572762364922707,
           -0.28893376896377315,
           -0.5252350893608644,
           0.1406059186731973,
           0.1466031602183782,
           0.1299500844933461,
           -0.014422480541239187,
           -0.029884641455015522,
           -0.010640074957539544,
           0.02389200764444259,
           -0.005500255998286456,
           0.013620637164737765,
           0.27227219159349436,
           0.4765639616421137,
           0.6764700524574971,
           -0.07347606530304242,
           -0.020677871939682138,
           -0.026797747106930225,
           0.09199323169652684,
           0.014523476366802547,
           0.015685027572562727,
           0.022817868467389257,
           0.022341445716498907,
           0.01793221200925467,
           0.019137238478933727,
           0.019340665511081413,
           -0.023927148793197493,
           -0.01271394248038454,
           -0.02270592142829439,
           -0.025239873670873048,
           -0.01949230065441609,
           -0.00977542325137606,
           -0.0018867072957929481,
           -0.0019524088104962405
          ],
          [
           -0.045371030879774926,
           -0.05153910627177115,
           -0.030898809095735284,
           0.24166699578051443,
           0.15234424221655568,
           0.19137988867429892,
           0.1965389302827853,
           0.32579688518636335,
           0.25632254944239985,
           0.21926642930846574,
           0.1910390186718229,
           0.22989778227031396,
           0.24511287665394924,
           0.3673687451776767,
           0.6888317253096882,
           0.7857694719236171,
           0.6691291274904431,
           0.7842132988160566,
           0.6776526644596313,
           1,
           0.9137800094838956,
           0.8341965682685858,
           -0.19740386198342216,
           -0.23603354350553982,
           -0.3018563544556886,
           0.13412077520885166,
           0.09080136712749994,
           0.048245247883584544,
           -0.012419366819635489,
           0.005898376841279733,
           -0.006354683638137804,
           0.012924756741626239,
           -0.010892949258435642,
           -0.006050192766825649,
           0.12243571774092732,
           0.2949595287946225,
           0.3452042224192315,
           0.053276844225658086,
           0.08468441267419952,
           -0.017683149596033287,
           0.11917739142087494,
           0.0768010551418292,
           0.07706806364099333,
           0.09978940536400055,
           0.09147678171246036,
           0.10145393149389736,
           0.10066251474790867,
           0.10106064567962264,
           0.012229947856316448,
           0.004952807977427503,
           0.024252441643988397,
           0.03254229736169695,
           -0.006339524558146231,
           -0.0031009246880221663,
           -0.006758970914903812,
           -0.002377831149621098
          ],
          [
           -0.16007931820663254,
           -0.1643461954064535,
           -0.12842864952020513,
           0.14716408921963745,
           0.03431748200350414,
           0.081426514217856,
           0.07793864025907349,
           0.1991891193561048,
           0.17256288778067658,
           0.11993661678551727,
           0.10416103124733014,
           0.13630913472855768,
           0.2868745594379507,
           0.5160578719507507,
           0.7899861009083055,
           0.8984097252721996,
           0.776613281076267,
           0.6874993488379493,
           0.7755773915534113,
           0.9137800094838956,
           1,
           0.8581115630956572,
           -0.22790271977990664,
           -0.34788527774308153,
           -0.48910877109823575,
           0.23688500610096658,
           0.1085173737359561,
           0.15931885460496495,
           -0.013824051455925215,
           -0.009661321905655648,
           -0.011452017281917476,
           0.017338710841185937,
           -0.008523292950110532,
           0.008150303359763349,
           0.1360357294514458,
           0.28362458513468486,
           0.34308019087422387,
           0.11927673506740079,
           0.10922446202639005,
           -0.034849815844088486,
           -0.02056035323993812,
           0.0528977317692586,
           0.046038219302715344,
           0.06398234615456934,
           0.05504483663161365,
           0.07059114207866082,
           0.06442111520183892,
           0.06842903820139813,
           -0.0010660599589879004,
           -0.008524934226266975,
           0.011909854018751602,
           0.007780567960430482,
           -0.01572513335352094,
           -0.007293994738665724,
           -0.004185356744100419,
           -0.0030013523800118407
          ],
          [
           -0.09659285170717641,
           -0.0960554978539139,
           -0.2051021367149951,
           0.08921468086936798,
           0.008983165470838468,
           0.0504891696775469,
           0.07825644107452831,
           0.16459364593473394,
           0.1670963375082425,
           0.07715727144248424,
           0.07191744020820229,
           0.1025421804019965,
           0.1639731513045619,
           0.4316862211559792,
           0.6836404207848484,
           0.796004598853022,
           0.7282966710796253,
           0.6218238997826243,
           0.7574602196942488,
           0.8341965682685858,
           0.8581115630956572,
           1,
           -0.15285719696366107,
           -0.3683336530860664,
           -0.45249269546742144,
           0.3143806200306976,
           0.12294561302067354,
           0.21064885207517692,
           -0.025392011725204,
           -0.026304472555636452,
           -0.013077435262994938,
           0.018679349220232285,
           -0.006717753296249841,
           0.018418160519771905,
           0.19162675036919333,
           0.1715109479213662,
           0.3186972758508123,
           0.13227350095452622,
           -0.008787240508971889,
           -0.020848003688982653,
           -0.009762017417211742,
           0.048505432718014,
           0.041999873910593004,
           0.06330001273480196,
           0.05590274897333009,
           0.06543579657396785,
           0.06085389261655755,
           0.06332142980411716,
           -0.013491945816981443,
           -0.013910586855077645,
           -0.00023266663544093614,
           0.003349831564559948,
           -0.017592580608633987,
           -0.006425096854187829,
           -0.003611728959811668,
           -0.0025809939460106355
          ],
          [
           0.038190008348639816,
           0.03558415191554809,
           0.16640276679307414,
           -0.12635118287438812,
           -0.07997872615217197,
           -0.07594709815591573,
           -0.04882063248177497,
           -0.13536723229135722,
           -0.14969214375901777,
           -0.1083497498221034,
           -0.1064177579464136,
           -0.12483711421547444,
           -0.9007675528337117,
           -0.6671081079856744,
           -0.3158066892182909,
           -0.2910351422026774,
           -0.4220754230906354,
           -0.019105526030903968,
           -0.03572762364922707,
           -0.19740386198342216,
           -0.22790271977990664,
           -0.15285719696366107,
           1,
           0.31752074194945207,
           0.30975193281713403,
           -0.14360347271974075,
           -0.21151163623726518,
           -0.22992600822312032,
           0.02253409788596724,
           -0.0061619190199018545,
           0.006750749472342707,
           -0.021421078419625473,
           0.0007555059718743442,
           0.015962405896697753,
           -0.03426086606716991,
           -0.07053468599792351,
           0.16987704760202263,
           -0.22682955849661204,
           -0.40319137219934226,
           0.00956064433140609,
           -0.17228852031591851,
           -0.008266675875604708,
           -0.004545546580670676,
           0.0019212315072620275,
           -0.0003332164749933805,
           -0.01479197971411378,
           -0.005864744360185743,
           -0.010072609159906744,
           -0.006696124780421833,
           -0.016481048168796517,
           -0.036645354301327,
           -0.02915151935333705,
           -0.005159270789444962,
           -0.009917538808664745,
           -0.00014824737081160975,
           -0.0003175278337418933
          ],
          [
           -0.015563149458167027,
           -0.0056214327353572655,
           -0.13929386967371288,
           -0.14030595234304444,
           -0.3440423185860083,
           -0.43853464131516495,
           -0.35727367958822637,
           -0.30047524920292484,
           -0.5120872343995113,
           -0.346723651090091,
           -0.4124793485936792,
           -0.43609251604862936,
           -0.37805429974768195,
           -0.5554242288810061,
           -0.5279869423443261,
           -0.5374462556698194,
           -0.6512453554738035,
           -0.026503972550133176,
           -0.28893376896377315,
           -0.23603354350553982,
           -0.34788527774308153,
           -0.3683336530860664,
           0.31752074194945207,
           1,
           0.7588845132333762,
           -0.3698530982921998,
           0.21854296308971952,
           0.11183991369090285,
           0.08519105638831735,
           -0.07589700610776363,
           -0.03612070898470807,
           -0.05444310381723607,
           -0.0051355796489571965,
           -0.007415615126974019,
           -0.030953230442729716,
           -0.10793726145692246,
           -0.07780672475004331,
           0.037933890480212444,
           -0.09889587498205865,
           -0.002461821586005459,
           0.07204915416438476,
           -0.02640361038666518,
           -0.026534941403118333,
           -0.01308484503361983,
           -0.00940623025155737,
           -0.05386145220818655,
           -0.027240571089424997,
           -0.04035446927297504,
           -0.06760096004810069,
           -0.040737641243157326,
           -0.10221930753566702,
           -0.0768110780618754,
           -0.022933433542121563,
           -0.012359109519320868,
           -0.0017063167081923653,
           -0.0027101546983417614
          ],
          [
           0.1254922738683701,
           0.13186444415900966,
           -0.010593008527706936,
           0.06406184345520721,
           -0.09954486243191601,
           -0.1496686987131188,
           -0.08157022099896775,
           -0.07290680702827755,
           -0.15387002833721652,
           -0.09578405025434278,
           -0.07871275546487669,
           -0.11991726394893915,
           -0.3843695046493461,
           -0.7271663176433638,
           -0.5543829844980652,
           -0.6780580101908698,
           -0.7505912438924853,
           -0.03704467223652235,
           -0.5252350893608644,
           -0.3018563544556886,
           -0.48910877109823575,
           -0.45249269546742144,
           0.30975193281713403,
           0.7588845132333762,
           1,
           -0.2520771818875507,
           0.04871862242353827,
           -0.026527350869294468,
           0.04899244299203558,
           -0.003342866092776462,
           -0.010208921986455215,
           -0.02636106994424967,
           0.00036588600825235117,
           -0.011836686965584979,
           -0.0897661640745926,
           -0.24747424632820733,
           -0.23763226433700274,
           0.11096459029225046,
           0.007895500151282145,
           0.02372049982950889,
           0.07609923547888063,
           -0.006419945901678516,
           -0.0067653139761942715,
           -0.001036260614158971,
           -0.0046786178907434976,
           -0.016971435780155475,
           -0.00289260308546518,
           -0.012941417125253705,
           -0.008827561785280828,
           -0.004238290560153136,
           -0.01870822104812203,
           -0.015394952757898699,
           0.005132697836497742,
           0.009087444520673211,
           -0.00027712570250665677,
           0.0003085398510986077
          ],
          [
           -0.2430273931814662,
           -0.2317488918352337,
           -0.6116760624105366,
           -0.3829862499698254,
           -0.38004705981911185,
           -0.28048460377235307,
           -0.2598965022987127,
           -0.3341605285720811,
           -0.03347663083301727,
           -0.29564352223831575,
           -0.20829447873328988,
           -0.192575319956597,
           0.12931935520001642,
           0.37999344731705786,
           0.26079232201468006,
           0.26125369288433736,
           0.30168966698955146,
           0.05598654216439941,
           0.1406059186731973,
           0.13412077520885166,
           0.23688500610096658,
           0.3143806200306976,
           -0.14360347271974075,
           -0.3698530982921998,
           -0.2520771818875507,
           1,
           -0.14364274605307,
           0.5386287886647586,
           -0.06584778377910637,
           -0.05605604976988191,
           0.030714082830391315,
           0.03930266937080006,
           -0.0038795913109410454,
           0.0760129144648182,
           0.13636066524703577,
           -0.18053801414444687,
           -0.02689410216032532,
           0.21492781148418927,
           0.01633503029089159,
           -0.03904006501743025,
           -0.274461806182978,
           -0.01764744150216741,
           -0.01599250394631731,
           0.0013509025355115463,
           -0.0024151681336341377,
           0.0025094393477845312,
           -0.0026763248254370307,
           0.014522521320475852,
           -0.09995865649911868,
           -0.05579994107391595,
           -0.06424792663631743,
           -0.062310041136743,
           -0.03290849242843754,
           -0.031678879527146216,
           -0.000007266868267057065,
           -0.004070362179693465
          ],
          [
           -0.08559460808343647,
           -0.07721652416720919,
           -0.5288879796856472,
           -0.08698179613976788,
           -0.30656650011309805,
           -0.5119900708116529,
           -0.5153869027207505,
           -0.21738346794254929,
           -0.42904228412093154,
           -0.4043825468749695,
           -0.4467636941148178,
           -0.4420361010048875,
           0.1768116026289732,
           0.13967167914886391,
           0.07062250966159672,
           0.0334311927427403,
           0.05314093057312371,
           0.07809586100901798,
           0.1466031602183782,
           0.09080136712749994,
           0.1085173737359561,
           0.12294561302067354,
           -0.21151163623726518,
           0.21854296308971952,
           0.04871862242353827,
           -0.14364274605307,
           1,
           0.6172313619433962,
           0.043026534103082846,
           -0.12996693035759324,
           -0.07105545139170708,
           -0.02143804295442268,
           -0.005350839009756469,
           0.006418669816604019,
           0.44258742058076983,
           0.04450572972266792,
           0.06558529358289407,
           0.20293560262361449,
           -0.05215217894334675,
           -0.016410939859861164,
           0.31324767026359734,
           -0.0028540364603386143,
           -0.035768501100243245,
           -0.020295823199406877,
           -0.016378626410703992,
           -0.053441322656180686,
           -0.03187826080102558,
           -0.04481560756277988,
           -0.0742554178645739,
           -0.045303032338727796,
           -0.10939819932860087,
           -0.08732394535450695,
           -0.036043776579837504,
           0.0015122759162525248,
           -0.0013985440923853178,
           -0.0026854919581002666
          ],
          [
           -0.22088150363698486,
           -0.1979398981199328,
           -0.9785449790060089,
           -0.34300854886386367,
           -0.6738369277850481,
           -0.7779799134139744,
           -0.7052809181059466,
           -0.5077508938133616,
           -0.5114789250472294,
           -0.6147891863446056,
           -0.5979705261966006,
           -0.5996170743718556,
           0.14035839171204076,
           0.25136019847061913,
           0.11661852483105559,
           0.10890654943090537,
           0.10804663902991209,
           0.03240202009384742,
           0.1299500844933461,
           0.048245247883584544,
           0.15931885460496495,
           0.21064885207517692,
           -0.22992600822312032,
           0.11183991369090285,
           -0.026527350869294468,
           0.5386287886647586,
           0.6172313619433962,
           1,
           0.019848876146946002,
           -0.1694025690663006,
           -0.054764869491111104,
           0.013579677465961924,
           -0.007211115867403615,
           0.06243550489670563,
           0.42202750022099067,
           -0.05136325645177159,
           -0.034712882585064754,
           0.4317008770775391,
           -0.07960971971587794,
           -0.0335514237215928,
           -0.11242939563161047,
           -0.029881146787594737,
           -0.0570677251689698,
           -0.02940006979393982,
           -0.02653880565798484,
           -0.05805058909968625,
           -0.04373452305616672,
           -0.04278048675172523,
           -0.14352671935124364,
           -0.08944111056298266,
           -0.14826832679495175,
           -0.12929025834891328,
           -0.056132286702011196,
           -0.01992526121922691,
           -0.00010207631435353367,
           -0.004811607696752734
          ],
          [
           0.10627810243619698,
           0.09755570607971698,
           -0.012320542563059804,
           0.054625316681871434,
           0.011147361231729613,
           -0.020688290817673243,
           0.002692746800971107,
           0.03385414870308001,
           -0.006033322599852827,
           0.01743717465551732,
           0.00947713084548406,
           0.003736251165632428,
           -0.03258596185156092,
           -0.03735058011746879,
           -0.030467070750304935,
           -0.03609347212204058,
           -0.04634672169467193,
           0.022816914316251723,
           -0.014422480541239187,
           -0.012419366819635489,
           -0.013824051455925215,
           -0.025392011725204,
           0.02253409788596724,
           0.08519105638831735,
           0.04899244299203558,
           -0.06584778377910637,
           0.043026534103082846,
           0.019848876146946002,
           1,
           0.32543339697168006,
           0.24311218696735543,
           -0.02032469421004238,
           -0.08129832996385303,
           -0.09070523352696139,
           0.09238104510822086,
           -0.02024628436734244,
           -0.023165887229700595,
           0.04132754413664121,
           -0.04071131634359541,
           0.06493143963313641,
           -0.016366759229025012,
           0.24910811284479054,
           0.11043986915695539,
           0.09839223569035165,
           0.05783278399633828,
           0.07932513475954263,
           0.11830772822841824,
           0.08048062404764976,
           0.19301176477602547,
           0.2983248636902107,
           0.2547742683417634,
           0.1990830235246377,
           0.062474729013915306,
           -0.004306846861350403,
           0.015601332815957467,
           0.05781303938497864
          ],
          [
           0.12494006708004081,
           0.1083638188250145,
           0.17405776567189132,
           0.17491731404055139,
           0.20973454805427058,
           0.214222067936488,
           0.2196591416020006,
           0.1980457097321248,
           0.21408915895942204,
           0.21523916331755188,
           0.22123424063864597,
           0.2203919861969848,
           0.021308921855466872,
           -0.014402874416782258,
           -0.00019903505225027146,
           -0.004761710210729915,
           0.004693298428085511,
           -0.013240205609424042,
           -0.029884641455015522,
           0.005898376841279733,
           -0.009661321905655648,
           -0.026304472555636452,
           -0.0061619190199018545,
           -0.07589700610776363,
           -0.003342866092776462,
           -0.05605604976988191,
           -0.12996693035759324,
           -0.1694025690663006,
           0.32543339697168006,
           1,
           0.8050784828381307,
           0.02089265512769004,
           -0.03444606750229962,
           -0.06276754065978636,
           -0.008115822687070985,
           0.01484224259417869,
           0.003992881619631378,
           0.020728248287895275,
           0.039698078465774445,
           0.013171222129199462,
           0.02192118439307835,
           0.23527588517752202,
           0.0958342123122297,
           0.030522121684093705,
           0.026063344376151206,
           0.08392567789673709,
           0.0646578026000109,
           0.05715960102988382,
           0.44213817663043536,
           0.47090024504757505,
           0.4989119648162343,
           0.43887296208350635,
           0.22822758047954683,
           0.03016076051431954,
           0.07799366666960708,
           0.08583419893131386
          ],
          [
           0.019409384771374414,
           0.012444030597724668,
           0.04850212010719168,
           0.01146866028221214,
           0.04518977397054094,
           0.058186839758136176,
           0.05436353984860677,
           0.028371741350146366,
           0.05440170963806342,
           0.04277215661095484,
           0.049043670044639574,
           0.050046211836822084,
           -0.0007663471459661916,
           0.006220338543099035,
           -0.005904586199871624,
           -0.004700426808261579,
           0.0029729629221145097,
           -0.014989918647526256,
           -0.010640074957539544,
           -0.006354683638137804,
           -0.011452017281917476,
           -0.013077435262994938,
           0.006750749472342707,
           -0.03612070898470807,
           -0.010208921986455215,
           0.030714082830391315,
           -0.07105545139170708,
           -0.054764869491111104,
           0.24311218696735543,
           0.8050784828381307,
           1,
           -0.031597279025333734,
           -0.10562390190393958,
           -0.13024783727799316,
           0.022446080737654917,
           -0.017671948903775683,
           0.008514074924936183,
           -0.015233229041393424,
           0.019420517760908886,
           -0.0765629442230573,
           0.004574288425679268,
           0.20142163169396596,
           0.05937972349113007,
           -0.008937434122555448,
           -0.018924226287299516,
           0.01806544432947724,
           0.014303247231069965,
           0.0033074123302717123,
           0.10639208874350391,
           0.21786950929796076,
           0.1632736538522807,
           0.12452647315710294,
           -0.064068214311079,
           -0.0343638856113983,
           -0.02058507867263883,
           0.029609829293273247
          ],
          [
           0.07307394192474484,
           0.08504355851016003,
           -0.009728950409117478,
           0.09246713368325743,
           0.07412288924067655,
           0.05908489327677447,
           0.07519713367570548,
           0.08345458495128134,
           0.08536268718883964,
           0.08681994456212859,
           0.07943881771528555,
           0.08242504805394245,
           0.016666047976430794,
           0.023319695016431777,
           0.017850470586752976,
           0.018546044891099206,
           0.021522960884135755,
           -0.002570270096137871,
           0.02389200764444259,
           0.012924756741626239,
           0.017338710841185937,
           0.018679349220232285,
           -0.021421078419625473,
           -0.05444310381723607,
           -0.02636106994424967,
           0.03930266937080006,
           -0.02143804295442268,
           0.013579677465961924,
           -0.02032469421004238,
           0.02089265512769004,
           -0.031597279025333734,
           1,
           0.1828715127959909,
           0.2832107861386245,
           0.047764978451129106,
           0.04315231286257555,
           0.028373072487753396,
           0.04911146705368587,
           -0.033421759454455566,
           0.0034444738983196582,
           -0.012406887324206747,
           -0.029799545200306615,
           -0.06738618050917204,
           -0.002307991156748099,
           -0.007328080583192041,
           0.016865117292890707,
           0.0013205599453587442,
           0.007958887040815156,
           -0.017135921932660163,
           -0.10139221803779914,
           0.016266709454054257,
           0.009560404841950861,
           0.04215247008634757,
           -0.019664915323582974,
           0.016299664099668345,
           -0.006084931379063212
          ],
          [
           -0.005803044108109921,
           -0.004621827753585339,
           0.007507673946448987,
           0.005271324947218961,
           0.009532619113872907,
           0.009166276681429768,
           0.008604423322011707,
           0.006225811532693846,
           0.00791273650293216,
           0.008776232091046717,
           0.00941407816742992,
           0.00866695270213318,
           -0.001852596533776769,
           -0.0016655711098748165,
           -0.004359177836890809,
           -0.0054662469475318665,
           -0.003185979960957675,
           -0.018609834709993795,
           -0.005500255998286456,
           -0.010892949258435642,
           -0.008523292950110532,
           -0.006717753296249841,
           0.0007555059718743442,
           -0.0051355796489571965,
           0.00036588600825235117,
           -0.0038795913109410454,
           -0.005350839009756469,
           -0.007211115867403615,
           -0.08129832996385303,
           -0.03444606750229962,
           -0.10562390190393958,
           0.1828715127959909,
           1,
           0.5019209473211795,
           -0.1190509863979263,
           0.014787524756816866,
           -0.0018755873998766994,
           -0.001398724847399598,
           -0.0008617088317452627,
           -0.10969355663046503,
           -0.0006601250164623818,
           -0.21535695972677793,
           -0.22074413250768551,
           -0.11930347851469747,
           -0.09367648135926786,
           -0.10659409140954529,
           -0.12099053094953799,
           -0.10755713328940839,
           -0.07839434445220722,
           -0.29232774948917156,
           -0.08313008871587525,
           -0.07050895633659462,
           0.16855190000048104,
           -0.09541325184715027,
           0.022537127389571216,
           -0.023520061486634672
          ],
          [
           -0.03604633100956447,
           -0.04121078079903593,
           -0.07117177197988575,
           -0.05259269356857626,
           -0.055117842793691675,
           -0.04905885052854838,
           -0.0446592274927523,
           -0.05040110102668377,
           -0.037094133469945244,
           -0.04714824520432729,
           -0.04633768314173026,
           -0.046908451178234334,
           -0.022448837512780224,
           0.009630322717504003,
           0.012579009244469419,
           0.00876364967080746,
           0.008388494672463259,
           -0.017396773589844775,
           0.013620637164737765,
           -0.006050192766825649,
           0.008150303359763349,
           0.018418160519771905,
           0.015962405896697753,
           -0.007415615126974019,
           -0.011836686965584979,
           0.0760129144648182,
           0.006418669816604019,
           0.06243550489670563,
           -0.09070523352696139,
           -0.06276754065978636,
           -0.13024783727799316,
           0.2832107861386245,
           0.5019209473211795,
           1,
           -0.038522793765031434,
           -0.011500240650629047,
           -0.005843848579420259,
           -0.010296588738321477,
           -0.025302210533206377,
           -0.11040140987157668,
           -0.03345893641919585,
           -0.23304908341110425,
           -0.24010990403257393,
           -0.12564632157217392,
           -0.10937620508580093,
           -0.09930655840242307,
           -0.12287460077349945,
           -0.10611298287252106,
           -0.0749586759691843,
           -0.32952650860719573,
           -0.1346987557022522,
           -0.1080362198102224,
           0.11069889042836356,
           -0.11034501800364377,
           0.009566997262378937,
           -0.03247650601032644
          ],
          [
           0.12902470114714273,
           0.1347334580714346,
           -0.4191689104973375,
           0.03373366241247356,
           0.01380058324167279,
           -0.13644080100893294,
           -0.14921858254907056,
           0.036939648028959104,
           -0.0458474224453918,
           -0.06176930230689434,
           -0.09553402787418426,
           -0.07338777444352214,
           0.02039576223600634,
           0.15358149398143697,
           0.14320164196600474,
           0.16816977088910287,
           0.17346481425184884,
           0.027188777512967565,
           0.27227219159349436,
           0.12243571774092732,
           0.1360357294514458,
           0.19162675036919333,
           -0.03426086606716991,
           -0.030953230442729716,
           -0.0897661640745926,
           0.13636066524703577,
           0.44258742058076983,
           0.42202750022099067,
           0.09238104510822086,
           -0.008115822687070985,
           0.022446080737654917,
           0.047764978451129106,
           -0.1190509863979263,
           -0.038522793765031434,
           1,
           0.2641154038445176,
           0.207565914719494,
           0.13627040121112163,
           -0.06302157361333345,
           0.003245478381230237,
           0.06054454541368402,
           0.06357228055281873,
           -0.005683147111129104,
           -0.0007468216974063289,
           -0.024567990022463553,
           -0.002151415032495119,
           -0.00784237578101487,
           -0.014326722130355374,
           0.004752043655646494,
           0.0005186314978819647,
           0.014386171210772064,
           -0.0037707455572643708,
           -0.025317907431899087,
           -0.023609281732385648,
           -0.08834343781663818,
           -0.049767084570440503
          ],
          [
           0.1550949026116112,
           0.14975956215526648,
           0.09304372772086318,
           0.2923759400904117,
           0.2644883730693056,
           0.19329106721682388,
           0.1461736639563987,
           0.2489951484479052,
           0.12008611435327939,
           0.22153051141977098,
           0.13436014465772167,
           0.17824698593598656,
           0.12280472641187666,
           0.256767226569736,
           0.12496248301585162,
           0.3224441885325063,
           0.26995305781790774,
           0.09598182082813901,
           0.4765639616421137,
           0.2949595287946225,
           0.28362458513468486,
           0.1715109479213662,
           -0.07053468599792351,
           -0.10793726145692246,
           -0.24747424632820733,
           -0.18053801414444687,
           0.04450572972266792,
           -0.05136325645177159,
           -0.02024628436734244,
           0.01484224259417869,
           -0.017671948903775683,
           0.04315231286257555,
           0.014787524756816866,
           -0.011500240650629047,
           0.2641154038445176,
           1,
           0.5136368903530047,
           0.06651182651389395,
           0.0807720962377329,
           0.027074313503602364,
           0.11499048044680506,
           0.012979436240186638,
           0.019053689763067982,
           0.012252381567450251,
           0.01800024468282453,
           0.0210604834316476,
           0.01828946561507515,
           0.017173305081369928,
           0.036528272428582836,
           0.02819347362035758,
           0.04100822605811792,
           0.04149190941934147,
           0.020781117219353707,
           0.018965149251168132,
           0.014752401399160541,
           0.010782671532921293
          ],
          [
           -0.017480581007640376,
           -0.021548458879295285,
           0.03514340046693614,
           0.11797794244387573,
           0.1841078307897975,
           0.09594323301282734,
           0.036551793985369445,
           0.10004375503877,
           0.035634321169622285,
           0.07510579761879738,
           0.013117378934700706,
           0.06476186621719132,
           -0.09158323299011031,
           0.16654592097480486,
           0.13725320317705814,
           0.37329754678105415,
           0.2384130553711177,
           0.30351732978772744,
           0.6764700524574971,
           0.3452042224192315,
           0.34308019087422387,
           0.3186972758508123,
           0.16987704760202263,
           -0.07780672475004331,
           -0.23763226433700274,
           -0.02689410216032532,
           0.06558529358289407,
           -0.034712882585064754,
           -0.023165887229700595,
           0.003992881619631378,
           0.008514074924936183,
           0.028373072487753396,
           -0.0018755873998766994,
           -0.005843848579420259,
           0.207565914719494,
           0.5136368903530047,
           1,
           0.020342938344437723,
           0.08661608259172592,
           -0.005104505009606013,
           0.312447239229218,
           0.0038681540476782957,
           0.014260080284706335,
           0.00716592228209776,
           0.015526279669654123,
           0.006410765624076502,
           0.010220835121424498,
           0.005606756809990386,
           -0.012151855586079358,
           0.0024820047808740107,
           -0.010644984881431056,
           -0.0017910170172579995,
           -0.004839973992381817,
           -0.0031774791447154054,
           -0.0011170199638208826,
           0.0005783593320193341
          ],
          [
           0.11515993799539095,
           0.12753158525842148,
           -0.34918431759741875,
           0.3966770947758288,
           -0.021145158648052387,
           -0.15326763799795884,
           -0.022287472797393078,
           0.14813689889431317,
           0.15768247646704187,
           0.07391440506475298,
           0.1207775734887835,
           0.12060194487836183,
           0.2358129194127048,
           0.16805004345735922,
           0.020154848253706715,
           0.05226928788057051,
           0.023780397709317707,
           0.10176596070140545,
           -0.07347606530304242,
           0.053276844225658086,
           0.11927673506740079,
           0.13227350095452622,
           -0.22682955849661204,
           0.037933890480212444,
           0.11096459029225046,
           0.21492781148418927,
           0.20293560262361449,
           0.4317008770775391,
           0.04132754413664121,
           0.020728248287895275,
           -0.015233229041393424,
           0.04911146705368587,
           -0.001398724847399598,
           -0.010296588738321477,
           0.13627040121112163,
           0.06651182651389395,
           0.020342938344437723,
           1,
           0.2738634024086589,
           0.02195052883142534,
           -0.08912130159789537,
           0.01640388998743268,
           -0.0041297649577806085,
           -0.002836825237181664,
           -0.006017294324750368,
           0.02668633722711934,
           0.003949177057719133,
           0.007166811858396961,
           0.03013932400124241,
           0.002759721482153803,
           0.057253380814684396,
           0.042221382880769744,
           0.009039300682583748,
           0.029638394945205013,
           0.00022595530924144063,
           0.0020543542638882745
          ],
          [
           -0.1472536794139184,
           -0.16093370015678019,
           0.09697164776644776,
           0.01826849662464291,
           0.13732977384459755,
           0.18612839674584317,
           0.07297910576269871,
           0.0950069167758476,
           0.13300791096823716,
           0.10231730438317317,
           0.15692372020847173,
           0.14672033723688524,
           0.45548432672366496,
           0.32223174480205063,
           0.2630956164523429,
           0.2131147901800924,
           0.20417458248899795,
           0.1447768893607905,
           -0.020677871939682138,
           0.08468441267419952,
           0.10922446202639005,
           -0.008787240508971889,
           -0.40319137219934226,
           -0.09889587498205865,
           0.007895500151282145,
           0.01633503029089159,
           -0.05215217894334675,
           -0.07960971971587794,
           -0.04071131634359541,
           0.039698078465774445,
           0.019420517760908886,
           -0.033421759454455566,
           -0.0008617088317452627,
           -0.025302210533206377,
           -0.06302157361333345,
           0.0807720962377329,
           0.08661608259172592,
           0.2738634024086589,
           1,
           -0.03258994489627726,
           -0.04299485008605388,
           0.01068543348487271,
           0.009330724577449502,
           0.0032130157972044596,
           -0.00715805932009112,
           0.008260083614467808,
           0.005450680723231817,
           0.00657359297647599,
           0.05025635243095818,
           0.014192513170354996,
           0.02874108750380335,
           0.007545279777187934,
           0.005634339081640727,
           -0.015199477915651363,
           -0.0004404069641473495,
           -0.0010142391845376275
          ],
          [
           0.18002172892178026,
           0.17130758694998435,
           0.035655845674097936,
           0.1018203111130505,
           0.10025061821630528,
           0.0807504605610682,
           0.0951731359223699,
           0.10427125891233889,
           0.0872586950007477,
           0.10454687554818487,
           0.09513221353187987,
           0.09518953331566979,
           -0.023845927494004673,
           -0.03154534106614697,
           -0.01976875418184168,
           -0.024063325532010654,
           -0.022473640761804893,
           -0.026571880450256637,
           -0.026797747106930225,
           -0.017683149596033287,
           -0.034849815844088486,
           -0.020848003688982653,
           0.00956064433140609,
           -0.002461821586005459,
           0.02372049982950889,
           -0.03904006501743025,
           -0.016410939859861164,
           -0.0335514237215928,
           0.06493143963313641,
           0.013171222129199462,
           -0.0765629442230573,
           0.0034444738983196582,
           -0.10969355663046503,
           -0.11040140987157668,
           0.003245478381230237,
           0.027074313503602364,
           -0.005104505009606013,
           0.02195052883142534,
           -0.03258994489627726,
           1,
           0.00390744488541775,
           -0.07425444483663003,
           -0.05540989555221216,
           -0.0683506731117304,
           -0.06139909848476189,
           -0.0573542930551584,
           -0.06380024479713148,
           -0.060090967075131424,
           0.11760182545454438,
           0.13684607115616293,
           0.1587010390967541,
           0.12262711409571428,
           -0.03494937179801078,
           0.00981614900937605,
           -0.028903677580426737,
           -0.0634632935066897
          ],
          [
           0.055024734526707295,
           0.05296544069586066,
           0.13936932562215765,
           0.17775027415223135,
           0.21255697723439598,
           0.12982683674202447,
           0.10635203904977152,
           0.1663117076976887,
           0.09758665409405735,
           0.11440833749899046,
           0.0540003949631003,
           0.10005648151931025,
           0.1753742545818035,
           -0.042173888240339484,
           -0.18606734535325178,
           -0.06010670896155994,
           0.05688890486330214,
           0.06324276280031897,
           0.09199323169652684,
           0.11917739142087494,
           -0.02056035323993812,
           -0.009762017417211742,
           -0.17228852031591851,
           0.07204915416438476,
           0.07609923547888063,
           -0.274461806182978,
           0.31324767026359734,
           -0.11242939563161047,
           -0.016366759229025012,
           0.02192118439307835,
           0.004574288425679268,
           -0.012406887324206747,
           -0.0006601250164623818,
           -0.03345893641919585,
           0.06054454541368402,
           0.11499048044680506,
           0.312447239229218,
           -0.08912130159789537,
           -0.04299485008605388,
           0.00390744488541775,
           1,
           -0.0004720770722885624,
           0.009899138978960114,
           -0.007103924524114475,
           0.00532716084979277,
           -0.004011531998912033,
           -0.0024411846788910925,
           -0.008062369613235365,
           0.00833460691373781,
           0.021347859872489924,
           0.006567917223040462,
           0.03101663446619098,
           0.009651524284567893,
           0.012546450487575711,
           -0.0006440174894136099,
           0.0020824074181408866
          ],
          [
           0.04437357386771557,
           0.05534190379007975,
           0.034199247219551025,
           0.07705613038141887,
           0.06543012594895929,
           0.06407046027858136,
           0.07043886806775787,
           0.08269037504049404,
           0.0726109623840008,
           0.07447119513804648,
           0.07167078666731302,
           0.0727387860902134,
           0.010851003493069474,
           0.0019123676064138177,
           0.04111984270461193,
           0.04095806845674561,
           0.03061626014660704,
           0.08903491764448616,
           0.014523476366802547,
           0.0768010551418292,
           0.0528977317692586,
           0.048505432718014,
           -0.008266675875604708,
           -0.02640361038666518,
           -0.006419945901678516,
           -0.01764744150216741,
           -0.0028540364603386143,
           -0.029881146787594737,
           0.24910811284479054,
           0.23527588517752202,
           0.20142163169396596,
           -0.029799545200306615,
           -0.21535695972677793,
           -0.23304908341110425,
           0.06357228055281873,
           0.012979436240186638,
           0.0038681540476782957,
           0.01640388998743268,
           0.01068543348487271,
           -0.07425444483663003,
           -0.0004720770722885624,
           1,
           0.8313220711089693,
           0.8478752191025352,
           0.8087656132141688,
           0.8452235404586226,
           0.8560299868939131,
           0.8406139629151668,
           0.10792440125864924,
           0.2999426311801129,
           0.27310606873193877,
           0.1948254257559439,
           0.008643757504631724,
           0.20032399019983146,
           0.15130151465453684,
           0.16578162873436258
          ],
          [
           0.03260732722944577,
           0.04040210692956116,
           0.05546771383301571,
           0.06984811346138481,
           0.07612540200277036,
           0.0813403943766023,
           0.08321116566784321,
           0.08413257463018523,
           0.08041984378525291,
           0.08163263009899414,
           0.07863770064450064,
           0.08154379883557229,
           0.007846096465308343,
           -0.0007685020919298283,
           0.03037290077640823,
           0.034901781346080456,
           0.026413948092432117,
           0.08789466854672587,
           0.015685027572562727,
           0.07706806364099333,
           0.046038219302715344,
           0.041999873910593004,
           -0.004545546580670676,
           -0.026534941403118333,
           -0.0067653139761942715,
           -0.01599250394631731,
           -0.035768501100243245,
           -0.0570677251689698,
           0.11043986915695539,
           0.0958342123122297,
           0.05937972349113007,
           -0.06738618050917204,
           -0.22074413250768551,
           -0.24010990403257393,
           -0.005683147111129104,
           0.019053689763067982,
           0.014260080284706335,
           -0.0041297649577806085,
           0.009330724577449502,
           -0.05540989555221216,
           0.009899138978960114,
           0.8313220711089693,
           1,
           0.9343741040261867,
           0.9373472727387705,
           0.9315952619920731,
           0.927022091859663,
           0.9351114494508371,
           0.14872855051924588,
           0.3908938230121554,
           0.29595592196979925,
           0.2435202224267814,
           0.1430263933689944,
           0.2971809321952692,
           0.27811457295698133,
           0.2490454882483616
          ],
          [
           0.021278909619267608,
           0.02444298746061328,
           0.025911927430619642,
           0.0523481253223899,
           0.04569584782750142,
           0.053221810172827115,
           0.05809480379216063,
           0.06458545663499728,
           0.058245539709238736,
           0.056753516480384784,
           0.05429827293925442,
           0.05612265236126545,
           -0.002179134317563191,
           -0.0033206594442549086,
           0.0422942574940295,
           0.04429569193119272,
           0.02995498746034487,
           0.11737808756666541,
           0.022817868467389257,
           0.09978940536400055,
           0.06398234615456934,
           0.06330001273480196,
           0.0019212315072620275,
           -0.01308484503361983,
           -0.001036260614158971,
           0.0013509025355115463,
           -0.020295823199406877,
           -0.02940006979393982,
           0.09839223569035165,
           0.030522121684093705,
           -0.008937434122555448,
           -0.002307991156748099,
           -0.11930347851469747,
           -0.12564632157217392,
           -0.0007468216974063289,
           0.012252381567450251,
           0.00716592228209776,
           -0.002836825237181664,
           0.0032130157972044596,
           -0.0683506731117304,
           -0.007103924524114475,
           0.8478752191025352,
           0.9343741040261867,
           1,
           0.9704944498569561,
           0.9677424680507957,
           0.9827388095763052,
           0.9778965689394695,
           0.08558804808735887,
           0.24720374126716574,
           0.2450187862036798,
           0.19128926644908922,
           0.10596886852725541,
           0.24640017453568094,
           0.2220577927378363,
           0.20694550807380246
          ],
          [
           0.006657421913089332,
           0.014126687385064068,
           0.023661507275328685,
           0.04204831049925436,
           0.03617664217545269,
           0.04205064618393252,
           0.04515319956015765,
           0.051323060737403996,
           0.04371831326874511,
           0.04400992491058606,
           0.038979974101920147,
           0.04278507726292092,
           -0.00006908122321930606,
           -0.004471407658047546,
           0.025654905222241967,
           0.034946234245623535,
           0.021613642460130745,
           0.11219519054222009,
           0.022341445716498907,
           0.09147678171246036,
           0.05504483663161365,
           0.05590274897333009,
           -0.0003332164749933805,
           -0.00940623025155737,
           -0.0046786178907434976,
           -0.0024151681336341377,
           -0.016378626410703992,
           -0.02653880565798484,
           0.05783278399633828,
           0.026063344376151206,
           -0.018924226287299516,
           -0.007328080583192041,
           -0.09367648135926786,
           -0.10937620508580093,
           -0.024567990022463553,
           0.01800024468282453,
           0.015526279669654123,
           -0.006017294324750368,
           -0.00715805932009112,
           -0.06139909848476189,
           0.00532716084979277,
           0.8087656132141688,
           0.9373472727387705,
           0.9704944498569561,
           1,
           0.9470809131698235,
           0.9562781733325978,
           0.960981507145402,
           0.09901062660302902,
           0.25903984120665446,
           0.2434672689060011,
           0.19850451212339762,
           0.14580668959160478,
           0.2599026627641746,
           0.253719444272671,
           0.2177553513974179
          ],
          [
           0.05654138985744664,
           0.06433077122235008,
           0.05969632909564368,
           0.11855510396183054,
           0.10962815153883401,
           0.11525665073321731,
           0.1273142317619763,
           0.13125571871225175,
           0.13070632673193122,
           0.12956748963077683,
           0.12663679062588024,
           0.12947618789237011,
           0.017353174706591836,
           0.009820495505211616,
           0.049906829668761546,
           0.05321498700348118,
           0.0422663183145503,
           0.11044433203755546,
           0.01793221200925467,
           0.10145393149389736,
           0.07059114207866082,
           0.06543579657396785,
           -0.01479197971411378,
           -0.05386145220818655,
           -0.016971435780155475,
           0.0025094393477845312,
           -0.053441322656180686,
           -0.05805058909968625,
           0.07932513475954263,
           0.08392567789673709,
           0.01806544432947724,
           0.016865117292890707,
           -0.10659409140954529,
           -0.09930655840242307,
           -0.002151415032495119,
           0.0210604834316476,
           0.006410765624076502,
           0.02668633722711934,
           0.008260083614467808,
           -0.0573542930551584,
           -0.004011531998912033,
           0.8452235404586226,
           0.9315952619920731,
           0.9677424680507957,
           0.9470809131698235,
           1,
           0.9722540405104204,
           0.991400946662585,
           0.1365887245189296,
           0.2662647977437249,
           0.31452465245561806,
           0.2530476600978981,
           0.1341805044853191,
           0.2554263534867414,
           0.23942083658572996,
           0.20727611322478756
          ],
          [
           0.03276214876367547,
           0.0391749524638194,
           0.042237936821644166,
           0.0754585456808525,
           0.06894893909530812,
           0.07602277165083447,
           0.08200639275498162,
           0.08670793877758158,
           0.08236333264217684,
           0.08127348522522176,
           0.07904944115091408,
           0.08132081759495405,
           0.007295810040806446,
           -0.001027720451470733,
           0.04219704425659732,
           0.04469546331052669,
           0.030478311831685426,
           0.11776853467850759,
           0.019137238478933727,
           0.10066251474790867,
           0.06442111520183892,
           0.06085389261655755,
           -0.005864744360185743,
           -0.027240571089424997,
           -0.00289260308546518,
           -0.0026763248254370307,
           -0.03187826080102558,
           -0.04373452305616672,
           0.11830772822841824,
           0.0646578026000109,
           0.014303247231069965,
           0.0013205599453587442,
           -0.12099053094953799,
           -0.12287460077349945,
           -0.00784237578101487,
           0.01828946561507515,
           0.010220835121424498,
           0.003949177057719133,
           0.005450680723231817,
           -0.06380024479713148,
           -0.0024411846788910925,
           0.8560299868939131,
           0.927022091859663,
           0.9827388095763052,
           0.9562781733325978,
           0.9722540405104204,
           1,
           0.9825328581441753,
           0.10001353764629743,
           0.25977824175340936,
           0.26906999463073067,
           0.21779746190307753,
           0.08907363955165831,
           0.2356128443035433,
           0.19985265280783737,
           0.18759007917002252
          ],
          [
           0.028326957433014638,
           0.037316277755701156,
           0.040275235754677376,
           0.07517279887504706,
           0.07161579885762054,
           0.0824000570437074,
           0.08862576270801625,
           0.08955517294859824,
           0.09224135363935175,
           0.08735676251789809,
           0.08612713597474983,
           0.08914537616999445,
           0.012948840998605516,
           0.0078222800028322,
           0.047175258522891846,
           0.050486093552641005,
           0.03711077599085009,
           0.11572468633835511,
           0.019340665511081413,
           0.10106064567962264,
           0.06842903820139813,
           0.06332142980411716,
           -0.010072609159906744,
           -0.04035446927297504,
           -0.012941417125253705,
           0.014522521320475852,
           -0.04481560756277988,
           -0.04278048675172523,
           0.08048062404764976,
           0.05715960102988382,
           0.0033074123302717123,
           0.007958887040815156,
           -0.10755713328940839,
           -0.10611298287252106,
           -0.014326722130355374,
           0.017173305081369928,
           0.005606756809990386,
           0.007166811858396961,
           0.00657359297647599,
           -0.060090967075131424,
           -0.008062369613235365,
           0.8406139629151668,
           0.9351114494508371,
           0.9778965689394695,
           0.960981507145402,
           0.991400946662585,
           0.9825328581441753,
           1,
           0.1148075684093623,
           0.2507434198969878,
           0.2786490823208669,
           0.22691977095424465,
           0.12927951744844288,
           0.24672385521604345,
           0.23759706535609323,
           0.20852776557072433
          ],
          [
           0.1375489457808459,
           0.13950981982169713,
           0.15835725164241804,
           0.20855976416468805,
           0.22548120570341706,
           0.21536068913273101,
           0.2272710384809254,
           0.23039443257050182,
           0.21552777828161315,
           0.2364291541645233,
           0.23174425710284394,
           0.2307899320455023,
           0.023824428210294495,
           -0.014122846043596099,
           0.024860983705798166,
           0.019527067418685682,
           0.022082512027255183,
           -0.0043837885634771855,
           -0.023927148793197493,
           0.012229947856316448,
           -0.0010660599589879004,
           -0.013491945816981443,
           -0.006696124780421833,
           -0.06760096004810069,
           -0.008827561785280828,
           -0.09995865649911868,
           -0.0742554178645739,
           -0.14352671935124364,
           0.19301176477602547,
           0.44213817663043536,
           0.10639208874350391,
           -0.017135921932660163,
           -0.07839434445220722,
           -0.0749586759691843,
           0.004752043655646494,
           0.036528272428582836,
           -0.012151855586079358,
           0.03013932400124241,
           0.05025635243095818,
           0.11760182545454438,
           0.00833460691373781,
           0.10792440125864924,
           0.14872855051924588,
           0.08558804808735887,
           0.09901062660302902,
           0.1365887245189296,
           0.10001353764629743,
           0.1148075684093623,
           1,
           0.6671933245876446,
           0.7252134034878278,
           0.6415760937049177,
           0.5372592343548269,
           0.08804617878831508,
           0.15515186698790845,
           0.1274133268056597
          ],
          [
           0.08856281544401919,
           0.07306922846436016,
           0.09501761241835162,
           0.11016246275550752,
           0.13216959296555744,
           0.12548172698643945,
           0.1269821462714237,
           0.1251960322363448,
           0.119176374825793,
           0.13002644182809137,
           0.12727480262554025,
           0.12756328711370915,
           0.01958555987158994,
           -0.005999125838318191,
           0.006122021087149214,
           -0.0004087927341493351,
           0.004880707062188499,
           -0.00453075176311033,
           -0.01271394248038454,
           0.004952807977427503,
           -0.008524934226266975,
           -0.013910586855077645,
           -0.016481048168796517,
           -0.040737641243157326,
           -0.004238290560153136,
           -0.05579994107391595,
           -0.045303032338727796,
           -0.08944111056298266,
           0.2983248636902107,
           0.47090024504757505,
           0.21786950929796076,
           -0.10139221803779914,
           -0.29232774948917156,
           -0.32952650860719573,
           0.0005186314978819647,
           0.02819347362035758,
           0.0024820047808740107,
           0.002759721482153803,
           0.014192513170354996,
           0.13684607115616293,
           0.021347859872489924,
           0.2999426311801129,
           0.3908938230121554,
           0.24720374126716574,
           0.25903984120665446,
           0.2662647977437249,
           0.25977824175340936,
           0.2507434198969878,
           0.6671933245876446,
           1,
           0.8650886919480705,
           0.7299118666200722,
           0.5878429502635777,
           0.4833370905556069,
           0.5215378363485044,
           0.461231492112366
          ],
          [
           0.16490037512106895,
           0.149833412010168,
           0.16280590827735067,
           0.24887133356270022,
           0.2594756573035441,
           0.25045946807852526,
           0.2703713723523952,
           0.2677905477779262,
           0.2646717188282555,
           0.28005718745126057,
           0.27699153729037285,
           0.27579346563097196,
           0.043027451408785854,
           0.004583575472181388,
           0.032489627351544974,
           0.019641962409467334,
           0.025816393067269874,
           -0.0014105665768239524,
           -0.02270592142829439,
           0.024252441643988397,
           0.011909854018751602,
           -0.00023266663544093614,
           -0.036645354301327,
           -0.10221930753566702,
           -0.01870822104812203,
           -0.06424792663631743,
           -0.10939819932860087,
           -0.14826832679495175,
           0.2547742683417634,
           0.4989119648162343,
           0.1632736538522807,
           0.016266709454054257,
           -0.08313008871587525,
           -0.1346987557022522,
           0.014386171210772064,
           0.04100822605811792,
           -0.010644984881431056,
           0.057253380814684396,
           0.02874108750380335,
           0.1587010390967541,
           0.006567917223040462,
           0.27310606873193877,
           0.29595592196979925,
           0.2450187862036798,
           0.2434672689060011,
           0.31452465245561806,
           0.26906999463073067,
           0.2786490823208669,
           0.7252134034878278,
           0.8650886919480705,
           1,
           0.832397236285469,
           0.7014168515466613,
           0.453978992929262,
           0.4952415601486471,
           0.43327738330977617
          ],
          [
           0.14770702034297928,
           0.13315588023216857,
           0.1417341536750063,
           0.2125349176167841,
           0.22191320520309119,
           0.21131721324060315,
           0.2307728620891947,
           0.23132810902269507,
           0.22002185832600432,
           0.2378146691261771,
           0.22895984177838283,
           0.23174879440138035,
           0.03926510864712119,
           -0.002236307312637326,
           0.005813373310463084,
           0.010682993725853566,
           0.012682677799700282,
           -0.0017750630181504437,
           -0.025239873670873048,
           0.03254229736169695,
           0.007780567960430482,
           0.003349831564559948,
           -0.02915151935333705,
           -0.0768110780618754,
           -0.015394952757898699,
           -0.062310041136743,
           -0.08732394535450695,
           -0.12929025834891328,
           0.1990830235246377,
           0.43887296208350635,
           0.12452647315710294,
           0.009560404841950861,
           -0.07050895633659462,
           -0.1080362198102224,
           -0.0037707455572643708,
           0.04149190941934147,
           -0.0017910170172579995,
           0.042221382880769744,
           0.007545279777187934,
           0.12262711409571428,
           0.03101663446619098,
           0.1948254257559439,
           0.2435202224267814,
           0.19128926644908922,
           0.19850451212339762,
           0.2530476600978981,
           0.21779746190307753,
           0.22691977095424465,
           0.6415760937049177,
           0.7299118666200722,
           0.832397236285469,
           1,
           0.5831485549119798,
           0.34544920953165925,
           0.37245353739589165,
           0.31956606674685845
          ],
          [
           0.05050721060778833,
           0.03814046606459434,
           0.059457044887201584,
           0.07733631770971555,
           0.08624745502174554,
           0.08225272268319163,
           0.08616579041321265,
           0.08471028211030428,
           0.08192548750007889,
           0.08856988751618453,
           0.08730288551978482,
           0.08709844291141079,
           0.0057877620742679305,
           -0.010455338075208648,
           -0.005872240686077308,
           -0.008930632046425422,
           -0.00389384954542584,
           -0.025016385836222177,
           -0.01949230065441609,
           -0.006339524558146231,
           -0.01572513335352094,
           -0.017592580608633987,
           -0.005159270789444962,
           -0.022933433542121563,
           0.005132697836497742,
           -0.03290849242843754,
           -0.036043776579837504,
           -0.056132286702011196,
           0.062474729013915306,
           0.22822758047954683,
           -0.064068214311079,
           0.04215247008634757,
           0.16855190000048104,
           0.11069889042836356,
           -0.025317907431899087,
           0.020781117219353707,
           -0.004839973992381817,
           0.009039300682583748,
           0.005634339081640727,
           -0.03494937179801078,
           0.009651524284567893,
           0.008643757504631724,
           0.1430263933689944,
           0.10596886852725541,
           0.14580668959160478,
           0.1341805044853191,
           0.08907363955165831,
           0.12927951744844288,
           0.5372592343548269,
           0.5878429502635777,
           0.7014168515466613,
           0.5831485549119798,
           1,
           0.569069794592603,
           0.7539724372217373,
           0.6595533776251902
          ],
          [
           0.041001517103188355,
           0.048991149788885205,
           0.027075653646306472,
           0.07750108765779828,
           0.06687538477099265,
           0.05179178183223123,
           0.06134631946425617,
           0.07245172342529724,
           0.06243543328621054,
           0.06920073595342086,
           0.06800645465435586,
           0.06636271888239749,
           0.00650646685814862,
           -0.01177551222115849,
           -0.004561007634398637,
           -0.011128519823351368,
           -0.008773424020452062,
           -0.008245823825335818,
           -0.00977542325137606,
           -0.0031009246880221663,
           -0.007293994738665724,
           -0.006425096854187829,
           -0.009917538808664745,
           -0.012359109519320868,
           0.009087444520673211,
           -0.031678879527146216,
           0.0015122759162525248,
           -0.01992526121922691,
           -0.004306846861350403,
           0.03016076051431954,
           -0.0343638856113983,
           -0.019664915323582974,
           -0.09541325184715027,
           -0.11034501800364377,
           -0.023609281732385648,
           0.018965149251168132,
           -0.0031774791447154054,
           0.029638394945205013,
           -0.015199477915651363,
           0.00981614900937605,
           0.012546450487575711,
           0.20032399019983146,
           0.2971809321952692,
           0.24640017453568094,
           0.2599026627641746,
           0.2554263534867414,
           0.2356128443035433,
           0.24672385521604345,
           0.08804617878831508,
           0.4833370905556069,
           0.453978992929262,
           0.34544920953165925,
           0.569069794592603,
           1,
           0.83153706897375,
           0.6787462757420698
          ],
          [
           -0.022307187273628196,
           -0.025682833769441533,
           0.00011640107190988665,
           -0.0014791837473654196,
           0.000053577315480782955,
           0.00001042347936008765,
           -0.0005182861653578059,
           -0.001729962564359082,
           -0.00006203325860943357,
           -0.00045291324350510626,
           0.0003346262597932895,
           -0.00003174304143950264,
           -0.00013121205812792095,
           -0.00010497566864086708,
           -0.0021190393965980466,
           -0.002636238153640449,
           -0.001493942114388403,
           -0.01229541138433293,
           -0.0018867072957929481,
           -0.006758970914903812,
           -0.004185356744100419,
           -0.003611728959811668,
           -0.00014824737081160975,
           -0.0017063167081923653,
           -0.00027712570250665677,
           -0.000007266868267057065,
           -0.0013985440923853178,
           -0.00010207631435353367,
           0.015601332815957467,
           0.07799366666960708,
           -0.02058507867263883,
           0.016299664099668345,
           0.022537127389571216,
           0.009566997262378937,
           -0.08834343781663818,
           0.014752401399160541,
           -0.0011170199638208826,
           0.00022595530924144063,
           -0.0004404069641473495,
           -0.028903677580426737,
           -0.0006440174894136099,
           0.15130151465453684,
           0.27811457295698133,
           0.2220577927378363,
           0.253719444272671,
           0.23942083658572996,
           0.19985265280783737,
           0.23759706535609323,
           0.15515186698790845,
           0.5215378363485044,
           0.4952415601486471,
           0.37245353739589165,
           0.7539724372217373,
           0.83153706897375,
           1,
           0.8892320194531579
          ],
          [
           -0.007873327936502165,
           -0.019500636121366297,
           0.00565480732758031,
           0.008638491397458245,
           0.00794040755881214,
           0.007812650201691963,
           0.00801911179205245,
           0.00828431634907061,
           0.008422004118264317,
           0.008608961651620369,
           0.008589368035587202,
           0.00878059867286731,
           0.0006752437886069413,
           -0.0015818622783690206,
           -0.003235550135673405,
           -0.002645752295054137,
           -0.001851309455635211,
           -0.023451763461548487,
           -0.0019524088104962405,
           -0.002377831149621098,
           -0.0030013523800118407,
           -0.0025809939460106355,
           -0.0003175278337418933,
           -0.0027101546983417614,
           0.0003085398510986077,
           -0.004070362179693465,
           -0.0026854919581002666,
           -0.004811607696752734,
           0.05781303938497864,
           0.08583419893131386,
           0.029609829293273247,
           -0.006084931379063212,
           -0.023520061486634672,
           -0.03247650601032644,
           -0.049767084570440503,
           0.010782671532921293,
           0.0005783593320193341,
           0.0020543542638882745,
           -0.0010142391845376275,
           -0.0634632935066897,
           0.0020824074181408866,
           0.16578162873436258,
           0.2490454882483616,
           0.20694550807380246,
           0.2177553513974179,
           0.20727611322478756,
           0.18759007917002252,
           0.20852776557072433,
           0.1274133268056597,
           0.461231492112366,
           0.43327738330977617,
           0.31956606674685845,
           0.6595533776251902,
           0.6787462757420698,
           0.8892320194531579,
           1
          ]
         ],
         "zmax": 1,
         "zmin": -1
        }
       ],
       "layout": {
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Correlation Matrix"
        },
        "width": 900,
        "xaxis": {
         "side": "bottom",
         "title": {
          "text": "Variables"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "title": {
          "text": "Variables"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linked = sch.linkage(sch.distance.pdist(corr), method='ward')\n",
    "cluster_order = sch.dendrogram(linked, no_plot=True)['leaves']\n",
    "\n",
    "# Reorder the correlation matrix\n",
    "correlation_matrix_ordered = corr.iloc[cluster_order, cluster_order]\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   z=correlation_matrix_ordered.values,\n",
    "                   x=correlation_matrix_ordered.columns,\n",
    "                   y=correlation_matrix_ordered.columns,\n",
    "                   text=correlation_matrix_ordered.round(2).values,\n",
    "                   texttemplate=\"%{text}\",\n",
    "                   colorscale='Viridis',\n",
    "                   zmin=-1, zmax=1))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix',\n",
    "    xaxis_title=\"Variables\",\n",
    "    yaxis_title=\"Variables\",\n",
    "    xaxis=dict(side='bottom'),\n",
    "    yaxis=dict(autorange='reversed'),\n",
    "    width=900,  # or any width you desire\n",
    "    height=800,  # or any height you desire\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 13,\n",
       " 30,\n",
       " 27,\n",
       " 39,\n",
       " 32,\n",
       " 35,\n",
       " 37,\n",
       " 38,\n",
       " 31,\n",
       " 34,\n",
       " 36,\n",
       " 44,\n",
       " 49,\n",
       " 50,\n",
       " 52,\n",
       " 54,\n",
       " 43,\n",
       " 48,\n",
       " 53,\n",
       " 47,\n",
       " 51,\n",
       " 46,\n",
       " 25,\n",
       " 41,\n",
       " 33,\n",
       " 26,\n",
       " 28,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 22,\n",
       " 15,\n",
       " 21,\n",
       " 24,\n",
       " 40,\n",
       " 55,\n",
       " 29,\n",
       " 45,\n",
       " 5,\n",
       " 42,\n",
       " 23,\n",
       " 16,\n",
       " 10,\n",
       " 19,\n",
       " 18,\n",
       " 8,\n",
       " 20,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 14,\n",
       " 9,\n",
       " 4,\n",
       " 11,\n",
       " 17,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_order += [max(cluster_order)+1, max(cluster_order)+2, max(cluster_order)+3,max(cluster_order)+4,max(cluster_order)+5]\n",
    "cluster_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EBG</th>\n",
       "      <th>FFO</th>\n",
       "      <th>commodity_trade_Close</th>\n",
       "      <th>Three_Month_Yield</th>\n",
       "      <th>utilities_Close</th>\n",
       "      <th>C_Staples_Close</th>\n",
       "      <th>Health_care_Close</th>\n",
       "      <th>information_Close</th>\n",
       "      <th>materials_Close</th>\n",
       "      <th>C_Discretionary_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>GPS</th>\n",
       "      <th>EBS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>ENT</th>\n",
       "      <th>NDT</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sector</th>\n",
       "      <th>numeric_sector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.227910</td>\n",
       "      <td>0.225153</td>\n",
       "      <td>99.675354</td>\n",
       "      <td>0.03</td>\n",
       "      <td>33.480000</td>\n",
       "      <td>31.230000</td>\n",
       "      <td>35.529999</td>\n",
       "      <td>25.700001</td>\n",
       "      <td>39.369999</td>\n",
       "      <td>40.209999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.251326</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.219455</td>\n",
       "      <td>0.234633</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.319596</td>\n",
       "      <td>0.317883</td>\n",
       "      <td>99.795883</td>\n",
       "      <td>0.02</td>\n",
       "      <td>33.619999</td>\n",
       "      <td>29.660000</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>29.360001</td>\n",
       "      <td>34.860001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.315255</td>\n",
       "      <td>0.324273</td>\n",
       "      <td>0.303188</td>\n",
       "      <td>0.304899</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.269770</td>\n",
       "      <td>0.327487</td>\n",
       "      <td>99.709015</td>\n",
       "      <td>0.02</td>\n",
       "      <td>35.980000</td>\n",
       "      <td>32.490002</td>\n",
       "      <td>34.689999</td>\n",
       "      <td>25.450001</td>\n",
       "      <td>33.500000</td>\n",
       "      <td>39.020000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585</td>\n",
       "      <td>0.361496</td>\n",
       "      <td>0.320066</td>\n",
       "      <td>0.336828</td>\n",
       "      <td>0.295398</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.183399</td>\n",
       "      <td>0.199184</td>\n",
       "      <td>99.517723</td>\n",
       "      <td>0.07</td>\n",
       "      <td>35.040001</td>\n",
       "      <td>34.080002</td>\n",
       "      <td>37.610001</td>\n",
       "      <td>30.160000</td>\n",
       "      <td>36.970001</td>\n",
       "      <td>45.090000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.199331</td>\n",
       "      <td>0.201505</td>\n",
       "      <td>0.175010</td>\n",
       "      <td>0.190412</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.248831</td>\n",
       "      <td>0.264272</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>36.990002</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>38.009998</td>\n",
       "      <td>28.730000</td>\n",
       "      <td>35.290001</td>\n",
       "      <td>43.779999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.282720</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>0.249700</td>\n",
       "      <td>0.242842</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>0.519314</td>\n",
       "      <td>0.566079</td>\n",
       "      <td>100.758179</td>\n",
       "      <td>2.45</td>\n",
       "      <td>52.919998</td>\n",
       "      <td>50.779999</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>61.980000</td>\n",
       "      <td>50.520000</td>\n",
       "      <td>99.010002</td>\n",
       "      <td>...</td>\n",
       "      <td>1.080</td>\n",
       "      <td>0.587800</td>\n",
       "      <td>45.779944</td>\n",
       "      <td>0.609228</td>\n",
       "      <td>0.525924</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>0.245076</td>\n",
       "      <td>0.294405</td>\n",
       "      <td>100.577995</td>\n",
       "      <td>2.40</td>\n",
       "      <td>58.169998</td>\n",
       "      <td>56.110001</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>55.500000</td>\n",
       "      <td>113.849998</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.327376</td>\n",
       "      <td>41.655270</td>\n",
       "      <td>0.333021</td>\n",
       "      <td>0.296725</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>0.489922</td>\n",
       "      <td>0.538019</td>\n",
       "      <td>100.660065</td>\n",
       "      <td>2.12</td>\n",
       "      <td>59.630001</td>\n",
       "      <td>58.070000</td>\n",
       "      <td>92.639999</td>\n",
       "      <td>78.040001</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>119.199997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>44.643278</td>\n",
       "      <td>0.548109</td>\n",
       "      <td>0.526172</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>0.638738</td>\n",
       "      <td>0.665659</td>\n",
       "      <td>100.623726</td>\n",
       "      <td>1.88</td>\n",
       "      <td>64.739998</td>\n",
       "      <td>61.419998</td>\n",
       "      <td>90.129997</td>\n",
       "      <td>80.529999</td>\n",
       "      <td>58.200001</td>\n",
       "      <td>120.699997</td>\n",
       "      <td>...</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.654236</td>\n",
       "      <td>44.768947</td>\n",
       "      <td>0.726207</td>\n",
       "      <td>0.641649</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>0.488651</td>\n",
       "      <td>0.522301</td>\n",
       "      <td>100.559471</td>\n",
       "      <td>1.55</td>\n",
       "      <td>64.620003</td>\n",
       "      <td>62.980000</td>\n",
       "      <td>101.860001</td>\n",
       "      <td>91.669998</td>\n",
       "      <td>61.419998</td>\n",
       "      <td>125.419998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.522177</td>\n",
       "      <td>46.407565</td>\n",
       "      <td>0.556598</td>\n",
       "      <td>0.501654</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>Financials</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EBG       FFO  commodity_trade_Close  Three_Month_Yield  \\\n",
       "51     0.227910  0.225153              99.675354               0.03   \n",
       "52     0.319596  0.317883              99.795883               0.02   \n",
       "53     0.269770  0.327487              99.709015               0.02   \n",
       "54     0.183399  0.199184              99.517723               0.07   \n",
       "55     0.248831  0.264272             100.000000               0.09   \n",
       "...         ...       ...                    ...                ...   \n",
       "14055  0.519314  0.566079             100.758179               2.45   \n",
       "14056  0.245076  0.294405             100.577995               2.40   \n",
       "14057  0.489922  0.538019             100.660065               2.12   \n",
       "14058  0.638738  0.665659             100.623726               1.88   \n",
       "14059  0.488651  0.522301             100.559471               1.55   \n",
       "\n",
       "       utilities_Close  C_Staples_Close  Health_care_Close  information_Close  \\\n",
       "51           33.480000        31.230000          35.529999          25.700001   \n",
       "52           33.619999        29.660000          31.730000          23.600000   \n",
       "53           35.980000        32.490002          34.689999          25.450001   \n",
       "54           35.040001        34.080002          37.610001          30.160000   \n",
       "55           36.990002        34.770000          38.009998          28.730000   \n",
       "...                ...              ...                ...                ...   \n",
       "14055        52.919998        50.779999          86.510002          61.980000   \n",
       "14056        58.169998        56.110001          91.750000          74.000000   \n",
       "14057        59.630001        58.070000          92.639999          78.040001   \n",
       "14058        64.739998        61.419998          90.129997          80.529999   \n",
       "14059        64.620003        62.980000         101.860001          91.669998   \n",
       "\n",
       "       materials_Close  C_Discretionary_Close  ...    GPS       EBS  \\\n",
       "51           39.369999              40.209999  ...  0.300  0.251326   \n",
       "52           29.360001              34.860001  ...  0.795  0.315255   \n",
       "53           33.500000              39.020000  ...  0.585  0.361496   \n",
       "54           36.970001              45.090000  ...  0.840  0.199331   \n",
       "55           35.290001              43.779999  ...  0.515  0.282720   \n",
       "...                ...                    ...  ...    ...       ...   \n",
       "14055        50.520000              99.010002  ...  1.080  0.587800   \n",
       "14056        55.500000             113.849998  ...  1.040  0.327376   \n",
       "14057        58.500000             119.199997  ...  0.990  0.540476   \n",
       "14058        58.200001             120.699997  ...  1.170  0.654236   \n",
       "14059        61.419998             125.419998  ...  0.970  0.522177   \n",
       "\n",
       "             CPX       ENT       NDT  OFTIC       PENDS        Date  \\\n",
       "51      0.259769  0.219455  0.234633    AFL  2011-06-30  2011-06-30   \n",
       "52      0.324273  0.303188  0.304899    AFL  2011-09-30  2011-09-30   \n",
       "53      0.320066  0.336828  0.295398    AFL  2011-12-31  2011-12-31   \n",
       "54      0.201505  0.175010  0.190412    AFL  2012-03-31  2012-03-31   \n",
       "55      0.289115  0.249700  0.242842    AFL  2012-06-30  2012-06-30   \n",
       "...          ...       ...       ...    ...         ...         ...   \n",
       "14055  45.779944  0.609228  0.525924   ZION  2018-12-31  2018-12-31   \n",
       "14056  41.655270  0.333021  0.296725   ZION  2019-03-31  2019-03-31   \n",
       "14057  44.643278  0.548109  0.526172   ZION  2019-06-30  2019-06-30   \n",
       "14058  44.768947  0.726207  0.641649   ZION  2019-09-30  2019-09-30   \n",
       "14059  46.407565  0.556598  0.501654   ZION  2019-12-31  2019-12-31   \n",
       "\n",
       "           Sector  numeric_sector  \n",
       "51     Financials               5  \n",
       "52     Financials               5  \n",
       "53     Financials               5  \n",
       "54     Financials               5  \n",
       "55     Financials               5  \n",
       "...           ...             ...  \n",
       "14055  Financials               5  \n",
       "14056  Financials               5  \n",
       "14057  Financials               5  \n",
       "14058  Financials               5  \n",
       "14059  Financials               5  \n",
       "\n",
       "[1260 rows x 61 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data = pivoted_data.iloc[:, cluster_order]\n",
    "pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d8575fd27741be8ff308bf92607484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "company_dict = {}\n",
    "\n",
    "for company in tqdm(pivoted_data[\"OFTIC\"].unique()):\n",
    "    comp_data = pivoted_data[pivoted_data[\"OFTIC\"] == company].sort_values(by='PENDS')\n",
    "    X = comp_data.drop(columns=[\"EPS\", \"Sector\",\"PENDS\",\"OFTIC\",\"Date\"])\n",
    "    y = comp_data['EPS']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    X_train_df = X_train\n",
    "    X_test_df = X_test\n",
    "    y_train_df = y_train\n",
    "    y_test_df = y_test\n",
    "\n",
    "    for column in X_train.columns:\n",
    "        if \"embedding\" in column:\n",
    "            continue\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train[[column]].values)\n",
    "        X_train_df[column] = X_train_scaled\n",
    "            \n",
    "        X_test_scaled = scaler.transform(X_test[[column]].values)\n",
    "        X_test_df[column] = X_test_scaled\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    y_train_scaled = pd.DataFrame(scaler_y.fit_transform(y_train.values.reshape(-1, 1)), columns=['EPS'], index=y_train.index)\n",
    "    y_train_df = y_train_scaled\n",
    "            \n",
    "    y_test_scaled = pd.DataFrame(scaler_y.transform(y_test.values.reshape(-1, 1)), columns=['EPS'], index=y_test.index)\n",
    "    y_test_df = y_test_scaled\n",
    "\n",
    "    X_train_df[\"lagged_EPS\"] = y_train_df\n",
    "    X_test_df[\"lagged_EPS\"] = y_test_df\n",
    "\n",
    "    company_dict[company] = {\"X_train\": X_train_df, \"X_test\": X_test_df, \"y_train\": y_train_df, \"y_test\": y_test_df, \"scaler_y\": scaler_y}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EBG</th>\n",
       "      <th>FFO</th>\n",
       "      <th>commodity_trade_Close</th>\n",
       "      <th>Three_Month_Yield</th>\n",
       "      <th>utilities_Close</th>\n",
       "      <th>C_Staples_Close</th>\n",
       "      <th>Health_care_Close</th>\n",
       "      <th>information_Close</th>\n",
       "      <th>materials_Close</th>\n",
       "      <th>C_Discretionary_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>PRE</th>\n",
       "      <th>DPS</th>\n",
       "      <th>BPS</th>\n",
       "      <th>GPS</th>\n",
       "      <th>EBS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>ENT</th>\n",
       "      <th>NDT</th>\n",
       "      <th>numeric_sector</th>\n",
       "      <th>lagged_EPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.826128</td>\n",
       "      <td>0.864740</td>\n",
       "      <td>0.596333</td>\n",
       "      <td>1.115607</td>\n",
       "      <td>0.944302</td>\n",
       "      <td>0.803158</td>\n",
       "      <td>1.015309</td>\n",
       "      <td>1.096844</td>\n",
       "      <td>0.921078</td>\n",
       "      <td>1.120578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639713</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980024</td>\n",
       "      <td>0.287850</td>\n",
       "      <td>0.717296</td>\n",
       "      <td>0.820201</td>\n",
       "      <td>0.833997</td>\n",
       "      <td>0.835628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.106667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.207215</td>\n",
       "      <td>1.264188</td>\n",
       "      <td>0.552220</td>\n",
       "      <td>1.265896</td>\n",
       "      <td>0.979561</td>\n",
       "      <td>0.891296</td>\n",
       "      <td>1.244750</td>\n",
       "      <td>1.236968</td>\n",
       "      <td>0.916587</td>\n",
       "      <td>1.239801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953405</td>\n",
       "      <td>0.295327</td>\n",
       "      <td>1.184915</td>\n",
       "      <td>1.151386</td>\n",
       "      <td>1.198384</td>\n",
       "      <td>1.184623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.002111</td>\n",
       "      <td>1.101017</td>\n",
       "      <td>0.775552</td>\n",
       "      <td>1.416185</td>\n",
       "      <td>0.993357</td>\n",
       "      <td>0.775615</td>\n",
       "      <td>1.075172</td>\n",
       "      <td>0.917743</td>\n",
       "      <td>0.678858</td>\n",
       "      <td>0.965678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986609</td>\n",
       "      <td>0.145794</td>\n",
       "      <td>1.076053</td>\n",
       "      <td>1.010111</td>\n",
       "      <td>1.112325</td>\n",
       "      <td>1.060511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.444183</td>\n",
       "      <td>0.248053</td>\n",
       "      <td>0.662898</td>\n",
       "      <td>1.387283</td>\n",
       "      <td>1.261625</td>\n",
       "      <td>0.971355</td>\n",
       "      <td>1.178018</td>\n",
       "      <td>1.205165</td>\n",
       "      <td>0.838627</td>\n",
       "      <td>1.189071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644810</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.196537</td>\n",
       "      <td>0.347664</td>\n",
       "      <td>0.253698</td>\n",
       "      <td>0.452812</td>\n",
       "      <td>0.240911</td>\n",
       "      <td>0.476706</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.887430</td>\n",
       "      <td>0.975364</td>\n",
       "      <td>0.714209</td>\n",
       "      <td>1.225434</td>\n",
       "      <td>1.336229</td>\n",
       "      <td>1.043335</td>\n",
       "      <td>1.195486</td>\n",
       "      <td>1.301770</td>\n",
       "      <td>0.934873</td>\n",
       "      <td>1.269607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652456</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.369652</td>\n",
       "      <td>0.295327</td>\n",
       "      <td>0.930325</td>\n",
       "      <td>0.965583</td>\n",
       "      <td>0.913918</td>\n",
       "      <td>0.976911</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.218511</td>\n",
       "      <td>1.296393</td>\n",
       "      <td>0.691490</td>\n",
       "      <td>1.086705</td>\n",
       "      <td>1.597343</td>\n",
       "      <td>1.166361</td>\n",
       "      <td>1.146222</td>\n",
       "      <td>1.361310</td>\n",
       "      <td>0.925249</td>\n",
       "      <td>1.292187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685588</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.472723</td>\n",
       "      <td>0.276636</td>\n",
       "      <td>1.356457</td>\n",
       "      <td>1.291918</td>\n",
       "      <td>1.303241</td>\n",
       "      <td>1.365598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.373333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.850800</td>\n",
       "      <td>1.028198</td>\n",
       "      <td>0.651317</td>\n",
       "      <td>0.895954</td>\n",
       "      <td>1.591211</td>\n",
       "      <td>1.223650</td>\n",
       "      <td>1.376447</td>\n",
       "      <td>1.627690</td>\n",
       "      <td>1.028553</td>\n",
       "      <td>1.363239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351714</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.440552</td>\n",
       "      <td>0.284112</td>\n",
       "      <td>0.984323</td>\n",
       "      <td>0.929323</td>\n",
       "      <td>0.978454</td>\n",
       "      <td>0.957014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.026667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         EBG       FFO  commodity_trade_Close  Three_Month_Yield  \\\n",
       "79  0.826128  0.864740               0.596333           1.115607   \n",
       "80  1.207215  1.264188               0.552220           1.265896   \n",
       "81  1.002111  1.101017               0.775552           1.416185   \n",
       "82  0.444183  0.248053               0.662898           1.387283   \n",
       "83  0.887430  0.975364               0.714209           1.225434   \n",
       "84  1.218511  1.296393               0.691490           1.086705   \n",
       "85  0.850800  1.028198               0.651317           0.895954   \n",
       "\n",
       "    utilities_Close  C_Staples_Close  Health_care_Close  information_Close  \\\n",
       "79         0.944302         0.803158           1.015309           1.096844   \n",
       "80         0.979561         0.891296           1.244750           1.236968   \n",
       "81         0.993357         0.775615           1.075172           0.917743   \n",
       "82         1.261625         0.971355           1.178018           1.205165   \n",
       "83         1.336229         1.043335           1.195486           1.301770   \n",
       "84         1.597343         1.166361           1.146222           1.361310   \n",
       "85         1.591211         1.223650           1.376447           1.627690   \n",
       "\n",
       "    materials_Close  C_Discretionary_Close  ...       PRE       DPS       BPS  \\\n",
       "79         0.921078               1.120578  ...  0.639713  1.000000  0.980024   \n",
       "80         0.916587               1.239801  ...  0.446015  1.000000  0.953405   \n",
       "81         0.678858               0.965678  ...  0.407785  1.000000  0.986609   \n",
       "82         0.838627               1.189071  ...  0.644810  1.090909  1.196537   \n",
       "83         0.934873               1.269607  ...  0.652456  1.090909  1.369652   \n",
       "84         0.925249               1.292187  ...  0.685588  1.090909  1.472723   \n",
       "85         1.028553               1.363239  ...  0.351714  1.090909  1.440552   \n",
       "\n",
       "         GPS       EBS       CPX       ENT       NDT  numeric_sector  \\\n",
       "79  0.287850  0.717296  0.820201  0.833997  0.835628             0.0   \n",
       "80  0.295327  1.184915  1.151386  1.198384  1.184623             0.0   \n",
       "81  0.145794  1.076053  1.010111  1.112325  1.060511             0.0   \n",
       "82  0.347664  0.253698  0.452812  0.240911  0.476706             0.0   \n",
       "83  0.295327  0.930325  0.965583  0.913918  0.976911             0.0   \n",
       "84  0.276636  1.356457  1.291918  1.303241  1.365598             0.0   \n",
       "85  0.284112  0.984323  0.929323  0.978454  0.957014             0.0   \n",
       "\n",
       "    lagged_EPS  \n",
       "79    1.106667  \n",
       "80    1.026667  \n",
       "81    1.000000  \n",
       "82    1.266667  \n",
       "83    1.293333  \n",
       "84    1.373333  \n",
       "85    1.026667  \n",
       "\n",
       "[7 rows x 57 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"AFL\"][\"X_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 57)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"AFL\"][\"X_train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.106667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.373333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.026667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         EPS\n",
       "79  1.106667\n",
       "80  1.026667\n",
       "81  1.000000\n",
       "82  1.266667\n",
       "83  1.293333\n",
       "84  1.373333\n",
       "85  1.026667"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"AFL\"][\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class RollingWindowDataset(Dataset):\n",
    "    def __init__(self, X, y, device=device):\n",
    "        self.X = X.clone().detach().to(torch.float)\n",
    "        self.y = y.clone().detach().to(torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure idx is within the valid range\n",
    "        if idx > len(self.X):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        X_window = self.X[idx]\n",
    "        y_target = self.y[idx]  \n",
    "\n",
    "        return X_window.clone().detach().to(torch.float).to(device), y_target.clone().detach().to(torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "time_steps = 4\n",
    "print(len(company_dict[company][\"X_train\"]))\n",
    "print((len(company_dict[company][\"X_test\"])))\n",
    "\n",
    "for company in company_list:\n",
    "    company_dict[company][\"y_train\"][\"Type\"] = \"Train\"\n",
    "    company_dict[company][\"y_test\"][\"Type\"] = \"Test\"\n",
    "\n",
    "    comp_df_X = pd.concat([company_dict[company]['X_train'], company_dict[company]['X_test']], axis=0)\n",
    "    comp_df_y = pd.concat([company_dict[company]['y_train'], company_dict[company]['y_test']], axis=0)\n",
    "\n",
    "    for i in range((len(comp_df_X)) - time_steps):\n",
    "\n",
    "        if comp_df_y.iloc[i + time_steps][\"Type\"] == \"Train\":\n",
    "            X_train.append(comp_df_X.iloc[i : (i + time_steps)])\n",
    "            y_train.append(comp_df_y.iloc[i + time_steps][\"EPS\"])\n",
    "\n",
    "        elif comp_df_y.iloc[i + time_steps][\"Type\"] == \"Test\":\n",
    "            X_test.append(comp_df_X.iloc[i : (i + time_steps)])\n",
    "            y_test.append(comp_df_y.iloc[i + time_steps][\"EPS\"])\n",
    "\n",
    "    # for i in range((len(company_dict[company][\"X_train\"])) - time_steps):\n",
    "\n",
    "    #     X_train.append(company_dict[company][\"X_train\"].iloc[i : (i + time_steps)])\n",
    "    #     y_train.append(company_dict[company][\"y_train\"][i+time_steps])\n",
    "\n",
    "    # for i in range((len(company_dict[company][\"X_test\"])) - time_steps):\n",
    "\n",
    "    #     X_test.append(company_dict[company][\"X_test\"].iloc[i : (i+ time_steps)])\n",
    "    #     y_test.append(company_dict[company][\"y_test\"][i+time_steps])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.10666667,  1.02666667,  1.        ,  1.26666667,  1.29333333,\n",
       "        1.37333333,  1.02666667,  0.44736842,  0.26754386,  0.64473684,\n",
       "        1.14912281,  0.51754386,  0.33333333,  0.80701754,  0.78688525,\n",
       "        0.81967213,  0.70491803,  0.92622951,  0.97540984,  0.98360656,\n",
       "        0.94262295,  0.37209302,  1.25581395, -0.11627907,  0.30232558,\n",
       "        0.13953488,  0.04651163,  0.25581395,  0.89705882,  0.94117647,\n",
       "        0.83823529,  0.76470588,  0.86764706,  0.95588235,  0.86764706,\n",
       "        0.98966408,  1.2118863 ,  0.83979328,  0.97674419,  0.9250646 ,\n",
       "        1.11627907,  1.42377261,  0.72674419,  0.64534884,  0.23255814,\n",
       "        0.45930233,  0.53488372,  0.54069767,  0.62209302,  0.67647059,\n",
       "        0.68627451,  1.15686275,  0.73529412,  0.75490196,  0.91176471,\n",
       "        0.83333333,  1.23387097,  1.2983871 ,  1.24193548,  1.42741935,\n",
       "        1.29032258,  1.30645161,  1.21774194,  0.90243902,  0.66666667,\n",
       "        0.92682927,  0.80487805,  0.91869919,  1.03252033,  0.72357724,\n",
       "        1.3       ,  1.20555556,  0.58333333,  1.15555556,  1.25      ,\n",
       "        1.30555556,  0.79444444,  0.87548139,  0.9139923 ,  0.88318357,\n",
       "        0.84082157,  0.85365854,  0.72272144,  0.70988447,  0.88976378,\n",
       "        0.90551181,  0.61417323,  1.09448819,  1.04724409,  1.18110236,\n",
       "        1.12598425,  0.78125   ,  0.78125   ,  0.09375   ,  0.46875   ,\n",
       "        0.75      ,  0.90625   ,  0.71875   ,  1.31578947,  1.36842105,\n",
       "        1.52631579,  1.10526316,  1.10526316,  1.52631579,  1.52631579,\n",
       "        1.14585232,  1.25524157,  1.04557885,  1.25524157,  1.35551504,\n",
       "        1.59252507,  1.41932543,  1.01257862,  0.79245283,  0.75471698,\n",
       "        1.03144654,  1.03144654,  1.08176101,  0.98742138,  0.79381443,\n",
       "        0.74226804,  0.93814433,  1.12371134,  1.        ,  1.15463918,\n",
       "        1.05154639,  0.75438596,  0.47368421,  0.74561404,  1.12280702,\n",
       "        0.8245614 ,  0.46491228,  0.83333333,  0.98850575,  1.04597701,\n",
       "        1.        ,  1.27586207,  1.26436782,  1.42528736,  1.4137931 ,\n",
       "        1.15625   ,  1.296875  ,  1.41666667,  1.203125  ,  1.33854167,\n",
       "        1.265625  ,  1.33333333,  0.9047619 ,  0.85714286,  1.03190476,\n",
       "        0.96857143,  0.96857143,  1.04761905,  1.07952381,  1.14772727,\n",
       "        1.03409091,  1.13636364,  0.92045455,  1.22727273,  1.15909091,\n",
       "        1.28409091,  0.94736842,  1.28421053,  0.69473684,  1.03157895,\n",
       "        1.12631579,  0.82105263,  1.01052632,  0.87155963,  1.03669725,\n",
       "        0.95412844,  1.03669725,  1.11926606,  1.03669725,  0.89908257,\n",
       "        1.2265625 ,  1.3046875 ,  1.25      ,  1.140625  ,  1.3515625 ,\n",
       "        1.3984375 ,  1.421875  ,  0.96517413,  1.03482587,  0.68159204,\n",
       "        0.960199  ,  1.02985075,  1.06965174,  0.62686567,  0.87096774,\n",
       "        1.03225806,  1.03225806,  1.03225806,  1.03225806,  1.09677419,\n",
       "        1.09677419,  0.93642857,  1.03964286,  1.12690476,  1.14285714,\n",
       "        1.13488095,  1.29357143,  1.20630952,  1.11904762,  1.23809524,\n",
       "        1.23809524,  1.33333333,  1.26190476,  1.45238095,  1.19047619,\n",
       "        1.22222222,  1.04040404,  0.84848485,  0.4040404 ,  0.61616162,\n",
       "        0.67676768,  1.15151515,  1.        ,  1.5       ,  0.66037736,\n",
       "        1.30188679,  1.24528302,  1.33962264,  1.24528302,  0.66180049,\n",
       "        0.83941606,  0.73965937,  0.90997567,  0.71289538,  0.56934307,\n",
       "        1.02919708,  0.9047619 ,  1.0952381 ,  0.92857143,  0.78571429,\n",
       "        1.42857143,  0.88095238,  0.54761905,  0.87669544,  1.0823058 ,\n",
       "        0.83569667,  0.9041307 ,  1.23304562,  1.2946979 ,  1.00678175,\n",
       "        0.78494624,  0.94623656,  0.98924731,  0.94623656,  0.89247312,\n",
       "        1.08602151,  0.87096774])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 4, 57)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(864, 4, 57)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.21216573, 0.17762852, 0.09855325, ..., 0.26082965,\n",
       "         0.        , 0.36      ],\n",
       "        [0.43480938, 0.43147686, 0.17390993, ..., 0.44958088,\n",
       "         0.        , 0.49333333],\n",
       "        [0.31381473, 0.45776731, 0.11959856, ..., 0.4240608 ,\n",
       "         0.        , 0.25333333],\n",
       "        [0.10407803, 0.10654103, 0.        , ..., 0.14203851,\n",
       "         0.        , 0.48      ]],\n",
       "\n",
       "       [[0.43480938, 0.43147686, 0.17390993, ..., 0.44958088,\n",
       "         0.        , 0.49333333],\n",
       "        [0.31381473, 0.45776731, 0.11959856, ..., 0.4240608 ,\n",
       "         0.        , 0.25333333],\n",
       "        [0.10407803, 0.10654103, 0.        , ..., 0.14203851,\n",
       "         0.        , 0.48      ],\n",
       "        [0.26296843, 0.28471819, 0.30152688, ..., 0.28287934,\n",
       "         0.        , 0.42666667]],\n",
       "\n",
       "       [[0.31381473, 0.45776731, 0.11959856, ..., 0.4240608 ,\n",
       "         0.        , 0.25333333],\n",
       "        [0.10407803, 0.10654103, 0.        , ..., 0.14203851,\n",
       "         0.        , 0.48      ],\n",
       "        [0.26296843, 0.28471819, 0.30152688, ..., 0.28287934,\n",
       "         0.        , 0.42666667],\n",
       "        [0.44527837, 0.51245672, 0.14113517, ..., 0.49465654,\n",
       "         0.        , 0.64      ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.90064676, 0.78269389, 0.85149516, ..., 0.74790192,\n",
       "         0.        , 0.44086022],\n",
       "        [0.2238162 , 0.36787332, 0.75173986, ..., 0.23363987,\n",
       "         0.        , 0.47311828],\n",
       "        [0.25932832, 0.32566126, 0.79316743, ..., 0.30372401,\n",
       "         0.        , 0.48387097],\n",
       "        [0.55462092, 0.50870826, 0.84996876, ..., 0.51127284,\n",
       "         0.        , 0.61290323]],\n",
       "\n",
       "       [[0.2238162 , 0.36787332, 0.75173986, ..., 0.23363987,\n",
       "         0.        , 0.47311828],\n",
       "        [0.25932832, 0.32566126, 0.79316743, ..., 0.30372401,\n",
       "         0.        , 0.48387097],\n",
       "        [0.55462092, 0.50870826, 0.84996876, ..., 0.51127284,\n",
       "         0.        , 0.61290323],\n",
       "        [1.        , 1.        , 0.7656206 , ..., 1.        ,\n",
       "         0.        , 0.60215054]],\n",
       "\n",
       "       [[0.25932832, 0.32566126, 0.79316743, ..., 0.30372401,\n",
       "         0.        , 0.48387097],\n",
       "        [0.55462092, 0.50870826, 0.84996876, ..., 0.51127284,\n",
       "         0.        , 0.61290323],\n",
       "        [1.        , 1.        , 0.7656206 , ..., 1.        ,\n",
       "         0.        , 0.60215054],\n",
       "        [0.2966778 , 0.51325785, 0.68573718, ..., 0.51243639,\n",
       "         0.        , 0.68817204]]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1067, device='cuda:0')\n",
      "tensor([[1.0000, 1.0000, 0.7656, 0.6127, 1.0000, 0.8931, 0.9814, 0.8489, 0.8803,\n",
      "         0.8313, 0.8881, 0.8995, 0.9171, 0.6872, 0.6252, 0.6960, 0.7838, 0.2940,\n",
      "         0.2677, 0.6063, 0.4784, 0.5388, 0.1143, 0.0000, 0.1488, 0.2397, 0.3073,\n",
      "         0.2367, 0.6459, 0.7432, 0.4825, 0.1056, 1.0000, 0.4363, 0.4033, 0.7087,\n",
      "         0.4117, 0.4444, 0.2943, 1.0000, 0.6628, 0.2579, 0.7923, 0.8129, 1.0000,\n",
      "         0.3800, 0.5816, 0.3262, 0.5909, 0.8156, 0.2243, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.5467],\n",
      "        [0.6338, 0.5133, 0.6857, 0.8035, 0.9811, 1.0000, 1.0000, 0.9648, 1.0000,\n",
      "         0.9609, 1.0000, 1.0000, 0.8576, 0.6881, 0.6948, 0.7590, 0.8094, 0.3747,\n",
      "         0.6006, 0.6139, 0.7922, 0.4632, 0.1207, 0.0457, 0.2997, 0.3306, 0.3398,\n",
      "         0.3457, 0.3846, 0.9142, 0.6524, 0.0000, 0.4722, 0.2462, 0.4785, 0.7957,\n",
      "         0.2806, 0.5791, 0.6245, 0.5145, 0.6308, 0.2125, 1.0000, 0.5283, 0.4351,\n",
      "         0.2480, 0.0000, 0.2447, 0.6818, 0.2951, 1.0000, 0.3960, 0.4970, 0.5697,\n",
      "         0.6182, 0.0000, 0.4533],\n",
      "        [0.5013, 0.2455, 0.6680, 1.0000, 0.8712, 0.8436, 0.9749, 1.0000, 0.8848,\n",
      "         1.0000, 0.9814, 0.9703, 0.6861, 0.3861, 0.4898, 0.5322, 0.4932, 0.3556,\n",
      "         0.0000, 0.3925, 0.5523, 0.3597, 0.2125, 0.3127, 0.6149, 0.2140, 0.3715,\n",
      "         0.3854, 0.0744, 1.0000, 0.4356, 0.1501, 0.3961, 0.1180, 0.2879, 0.4803,\n",
      "         0.1220, 1.0000, 1.0000, 0.3333, 0.6287, 0.2346, 0.9754, 0.9252, 0.1623,\n",
      "         0.8760, 0.3675, 0.5684, 1.0000, 1.0000, 0.2280, 0.2377, 0.3186, 0.2574,\n",
      "         0.4454, 0.0000, 1.0000],\n",
      "        [0.8261, 0.8647, 0.5963, 1.1156, 0.9443, 0.8032, 1.0153, 1.0968, 0.9211,\n",
      "         1.1206, 0.9279, 0.9130, 0.8089, 0.8643, 0.4374, 0.4848, 0.6935, 0.3187,\n",
      "         0.2517, 0.5383, 0.7350, 0.5155, 0.1326, 0.1967, 0.1270, 0.4191, 0.2700,\n",
      "         0.4925, 0.6032, 0.9789, 0.7974, 0.1960, 0.8957, 0.3778, 0.4468, 0.9038,\n",
      "         0.4460, 1.1090, 0.5352, 0.8056, 0.4445, 0.3038, 0.9368, 0.6984, 0.8809,\n",
      "         0.9840, 0.2670, 0.6397, 1.0000, 0.9800, 0.2879, 0.7173, 0.8202, 0.8340,\n",
      "         0.8356, 0.0000, 1.1067]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float, device=device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float, device=device)\n",
    "\n",
    "train_data = RollingWindowDataset(X_train_tensor, y_train_tensor, device=device)\n",
    "test_data = RollingWindowDataset(X_test_tensor, y_test_tensor, device=device)\n",
    "\n",
    "print(test_data.__getitem__(0)[1])\n",
    "print(test_data.__getitem__(1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OneDimCNNLSTMModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_size, layer_size, output_dim, dropout_prob, conv_channels, kernel_size, pool_size, stride):\n",
    "#         super(OneDimCNNLSTMModel, self).__init__()\n",
    "\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.layer_size = layer_size\n",
    "#         self.hn, self.cn = None, None\n",
    "\n",
    "#         conv_output_width = (input_dim - kernel_size)  + 1\n",
    "#         # Pooling output width (no padding, considering the stride for pooling)\n",
    "#         pool_output_width = (conv_output_width - pool_size) // stride + 1\n",
    "\n",
    "#         self.lstm_input_size = conv_channels * pool_output_width  # LSTM input dimensions\n",
    "\n",
    "#         self.conv = nn.Conv2d(in_channels=1, out_channels=conv_channels, kernel_size=(1, kernel_size))\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=(1,pool_size), stride=(1, stride))\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size = self.lstm_input_size, hidden_size = self.hidden_size, num_layers=self.layer_size,\n",
    "#                             dropout=(dropout_prob if self.layer_size > 1 else 0), batch_first=True)\n",
    "                            \n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "#         self.fc = nn.Linear(self.hidden_size, output_dim)\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         # Initialize hidden and cell states with zeros\n",
    "#         h0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(device)\n",
    "#         c0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(device)\n",
    "#         return (h0, c0)\n",
    "    \n",
    "#     def reset_hidden(self):\n",
    "#         self.hn = None\n",
    "#         self.cn = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         batch_size, seq_len, num_of_feature = x.shape\n",
    "#         x = x.view(batch_size, 1, seq_len, num_of_feature)\n",
    "\n",
    "#         x = self.conv(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.maxpool(x)\n",
    "\n",
    "#         batch_size, channels, height, width = x.shape\n",
    "#         x = x.permute(0, 2, 3, 1) # [batch, height, width, channels]\n",
    "#         x = x.reshape(batch_size, height*width, channels)\n",
    "#         # print(x.shape)\n",
    "\n",
    "#         assert x.size(-1) == self.lstm.input_size, f\"Mismatch in LSTM input size. Expected: {self.lstm.input_size}, Got: {x.size(-1)}\"\n",
    "\n",
    "#         if self.hn == None or self.cn == None:\n",
    "#             self.hn, self.cn = self.init_hidden(x.size(0))\n",
    "        \n",
    "#         else:\n",
    "#             self.hn, self.cn = self.hn.detach(), self.cn.detach()\n",
    "            \n",
    "#         if self.hn.size(1) > x.size(0):\n",
    "#             self.hn = self.hn[:, -x.size(0):, :].contiguous()\n",
    "#             self.cn = self.cn[:, -x.size(0):, :].contiguous()\n",
    "\n",
    "#         # Forward propagate LSTM\n",
    "#         out, (self.hn, self.cn) = self.lstm(x, (self.hn, self.cn))\n",
    "\n",
    "#         out = self.dropout(out[:, -1, :])  # Add dropout\n",
    "\n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDimCNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, layer_size, output_dim, dropout_prob, conv_channels, kernel_size):\n",
    "        super(OneDimCNNLSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_size = layer_size\n",
    "        self.hn, self.cn = None, None\n",
    "        self.padding = 2\n",
    "\n",
    "        # 1D Convolution\n",
    "        self.conv = nn.Conv1d(in_channels=input_dim, out_channels=conv_channels, kernel_size=kernel_size, padding=self.padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # self.maxpool = nn.MaxPool1d(kernel_size=pool_size, stride=stride)\n",
    "        self.dropout_1 = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Calculate output width after convolution and pooling\n",
    "        conv_output_length = (input_dim - kernel_size + 2 * self.padding) + 1\n",
    "        # pool_output_length = (conv_output_length - pool_size) // stride + 1\n",
    "\n",
    "        self.lstm_input_size = conv_channels   # LSTM input dimensions\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=hidden_size, num_layers=layer_size,\n",
    "                            dropout=(dropout_prob if layer_size > 1 else 0), batch_first=True)\n",
    "        self.dropout_2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        device = next(self.parameters()).device\n",
    "        h0 = torch.zeros(self.layer_size, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(self.layer_size, batch_size, self.hidden_size, device=device)\n",
    "        return (h0, c0)\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.hn, self.cn = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, num_features = x.shape\n",
    "\n",
    "        # Permute x to match [batch_size, num_features, seq_len]\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply 1D convolution\n",
    "        x = self.conv(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout_1(x)\n",
    "\n",
    "        # Prepare for LSTM\n",
    "        # Assuming output of conv is [batch, conv_channels, new_seq_len]\n",
    "        # Flatten the output for the LSTM input if necessary\n",
    "        x = x.permute(0, 2, 1)  # Change back to [batch, length, channels]\n",
    "        x = x.reshape(batch_size, -1, self.lstm_input_size)\n",
    "\n",
    "\n",
    "        # LSTM processing\n",
    "        if self.hn is None or self.cn is None:\n",
    "            self.hn, self.cn = self.init_hidden(batch_size)\n",
    "        else:\n",
    "            self.hn, self.cn = self.hn.detach(), self.cn.detach()\n",
    "            last_hn = self.hn[:,-1:,:]\n",
    "            last_cn = self.cn[:,-1:,:]\n",
    "\n",
    "            self.hn = last_hn.repeat(1, x.size(0), 1)\n",
    "            self.cn = last_cn.repeat(1, x.size(0), 1)\n",
    "        \n",
    "        x, (self.hn, self.cn) = self.lstm(x, (self.hn, self.cn))\n",
    "        x = self.dropout_2(x[:, -1, :])  # Only take the last time step\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_supported_splits(comp_size, num_companies):\n",
    "    supported_splits = []\n",
    "    for n_splits in range(1, comp_size + 1):\n",
    "        if comp_size % n_splits != 0:\n",
    "            continue\n",
    "        \n",
    "        supported_splits.append(n_splits)\n",
    "    return supported_splits\n",
    "\n",
    "\n",
    "def custom_time_series_folds(data, n_splits):\n",
    "\n",
    "    total_size = len(data)\n",
    "    comp_size = total_size // len(company_list)\n",
    "    comp_fold_size = comp_size//n_splits\n",
    "\n",
    "    if comp_size % n_splits != 0:\n",
    "        supported_splits = find_supported_splits(comp_size, n_splits)\n",
    "        print(supported_splits)\n",
    "        print(f\"fold_size: {comp_fold_size} comp_size: {comp_size}\")\n",
    "        raise ValueError(\"Fold size must be divisible by the number of companies.\")\n",
    "\n",
    "    accumulated_train_idx = []     \n",
    "\n",
    "    for i in range(n_splits-1):\n",
    "        current_fold_val_idx = []\n",
    "        current_fold_train_idx = []\n",
    "\n",
    "        for j in range(len(company_list)):\n",
    "\n",
    "            start_idx = j * comp_size\n",
    "            val_start_idx = start_idx + (i+1) * comp_fold_size \n",
    "        \n",
    "            end_idx = val_start_idx + comp_fold_size\n",
    "        \n",
    "            current_comp_train_idx = list(range(start_idx, val_start_idx))\n",
    "            current_fold_train_idx.extend(current_comp_train_idx)  \n",
    "        \n",
    "            val_idx = list(range(val_start_idx, end_idx))\n",
    "            current_fold_val_idx.extend(val_idx)  \n",
    "\n",
    "        \n",
    "        yield current_fold_train_idx, current_fold_val_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelActioner:\n",
    "\n",
    "    def __init__(self, train_data, test_data, device):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def custom_time_series_cv(self, config, trial):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        epochs = config[\"epochs\"]\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        num_layers = config[\"num_layers\"]\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        dropout_prob = config[\"dropout_prob\"]\n",
    "        weight_decay = config[\"weight_decay\"]\n",
    "        lr_step_size = epochs//5\n",
    "        gamma = config[\"gamma\"]\n",
    "        kernel_size = config[\"kernel_size\"]\n",
    "        conv_channels = config[\"conv_channels\"]\n",
    "        # pool_size = config[\"pool_size\"]\n",
    "        # stride = config[\"stride\"]\n",
    " \n",
    "\n",
    "        suffle = False\n",
    "\n",
    "        fold_results = []\n",
    "        num_of_fold = 4\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(custom_time_series_folds(self.train_data, num_of_fold)):\n",
    "            print(f\"Fold: {fold+1}/{num_of_fold}\")\n",
    "\n",
    "            train_subset = Subset(self.train_data, train_idx)\n",
    "            val_subset = Subset(self.train_data, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=suffle)\n",
    "            val_loader = DataLoader(val_subset, batch_size=3, shuffle=suffle)\n",
    "\n",
    "            self.model = OneDimCNNLSTMModel(input_dim=self.train_data.__getitem__(0)[0].shape[1], hidden_size=hidden_size, layer_size=num_layers, dropout_prob=dropout_prob, output_dim=1, conv_channels=conv_channels, kernel_size=kernel_size).to(self.device)\n",
    "\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            scheduler = ReduceLROnPlateau(self.optimizer, patience=lr_step_size, factor=gamma, mode=\"min\") \n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "\n",
    "                self.model.reset_hidden()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                total_sample_train = 0\n",
    "\n",
    "                self.model.train()\n",
    "\n",
    "                for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    target = target.view(-1,1) \n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    preds = self.model(data)\n",
    "\n",
    "                    loss = self.criterion(preds, target)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step() # Update model params\n",
    "\n",
    "                    running_loss += loss.item() * data.size(0)\n",
    "                    total_sample_train += data.size(0)\n",
    "\n",
    "                train_loss = running_loss/total_sample_train\n",
    "\n",
    "                self.model.reset_hidden()\n",
    "                self.model.eval()\n",
    "\n",
    "                val_running_loss = 0.0\n",
    "                total_sample_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                        data, target = data.to(self.device), target.to(self.device)\n",
    "                        target = target.view(-1,1)\n",
    "\n",
    "                        preds = self.model(data)\n",
    "                        loss = self.criterion(preds, target)\n",
    "\n",
    "                        val_running_loss += loss.item() * data.size(0)\n",
    "                        total_sample_val += data.size(0)\n",
    "                \n",
    "                val_loss = val_running_loss/total_sample_val\n",
    "                fold_results.append(val_loss)\n",
    "                scheduler.step(train_loss)\n",
    "                \n",
    "                unique_step = fold * epochs + epoch\n",
    "                trial.report(val_loss, unique_step)\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "                print(f'Current Learning Rate: {current_lr}')\n",
    "                print(f\"train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "                \n",
    "        mean_val_loss = np.mean(fold_results)\n",
    "        print(f\"Mean validation loss: {mean_val_loss}\")\n",
    "        return mean_val_loss\n",
    "\n",
    "\n",
    "                    \n",
    "    def train(self, config):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        epochs = config[\"epochs\"]\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        num_layers = config[\"num_layers\"]\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        dropout_prob = config[\"dropout_prob\"]\n",
    "        weight_decay = config[\"weight_decay\"]\n",
    "        lr_step_size = epochs//5\n",
    "        gamma = config[\"gamma\"]\n",
    "        kernel_size = config[\"kernel_size\"]\n",
    "        conv_channels = config[\"conv_channels\"]\n",
    "        # pool_size = config[\"pool_size\"]\n",
    "        # stride = config[\"stride\"]\n",
    " \n",
    "        self.model = OneDimCNNLSTMModel(input_dim=self.train_data.__getitem__(0)[0].shape[1], hidden_size=hidden_size, layer_size=num_layers, dropout_prob=dropout_prob, output_dim=1, conv_channels=conv_channels, kernel_size=kernel_size).to(self.device)\n",
    "\n",
    "        # Update optimizer with updated lr\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Creating data loader\n",
    "        train_loader = DataLoader(dataset=self.train_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, patience=lr_step_size, factor=gamma, mode=\"min\")  \n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "\n",
    "            self.model.reset_hidden()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            total_sample_train = 0\n",
    "\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                target = target.view(-1,1)  \n",
    "                # print(data)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(data)\n",
    "\n",
    "                loss = self.criterion(preds, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step() # Update model params\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                total_sample_train += data.size(0)\n",
    "\n",
    "            train_loss = running_loss/total_sample_train\n",
    "            scheduler.step(train_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print(f'Current Learning Rate: {current_lr}')\n",
    "            print(f\"train_loss: {train_loss}\")\n",
    "        \n",
    "        return self.model\n",
    "            \n",
    "    \n",
    "    def test(self, config):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        all_preds = []\n",
    "\n",
    "        test_loader = DataLoader(dataset=self.test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        running_loss = .0\n",
    "        total_sample = 0\n",
    "\n",
    "        self.model.reset_hidden()\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                target = target.view(-1,1)\n",
    "                \n",
    "                preds = self.model(data)\n",
    "                loss = self.criterion(preds, target)\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                total_sample += data.size(0)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            test_loss = running_loss/total_sample\n",
    "            print(f\"test_loss: {test_loss}\")\n",
    "\n",
    "        return all_preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    config = {\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 22, 100),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 100, 400),\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1),\n",
    "        \"dropout_prob\": trial.suggest_float(\"dropout_prob\", 0.1, 0.2),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-1, log=True),\n",
    "        # \"lr_step_size\": trial.suggest_int(\"lr_step_size\", 3, 10), \n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-4, 1e-1),\n",
    "        \"conv_channels\": trial.suggest_int(\"conv_channels\", 16, 128, step=16),\n",
    "        \"kernel_size\": trial.suggest_int(\"kernel_size\", 1, 3),\n",
    "        \"num_layers\": trial.suggest_int(\"num_layers\", 1, 5),\n",
    "        # \"pool_size\": trial.suggest_int(\"pool_size\", 2, 6),\n",
    "        # \"stride\": trial.suggest_int(\"stride\", 1, 3)\n",
    "    }\n",
    "\n",
    "    trainer = ModelActioner(train_data, test_data, device)\n",
    "\n",
    "    val_loss = trainer.custom_time_series_cv(config, trial)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting study 'CNN-LSTM-Tunner'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:24:24,556] A new study created in RDB with name: CNN-LSTM-Tunner\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569ecdc1b3a74d308e9f6d732edd2a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimizing:   0%|          | 0/20 [00:00<?, ?trial/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 94.34097599127779, val_loss: 44.479259411493935\n",
      "epochs 2/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 5.5508590166767435, val_loss: 0.05576068742892756\n",
      "epochs 3/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.5640971969675135, val_loss: 110.03163115183513\n",
      "epochs 4/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 21.374660855090177, val_loss: 0.15888623146990236\n",
      "epochs 5/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.1646797905365627, val_loss: 5.125060634480582\n",
      "epochs 6/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.5848159315409485, val_loss: 2.719027812282244\n",
      "epochs 7/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6479871598658739, val_loss: 0.6776986455337869\n",
      "epochs 8/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.8347158418209465, val_loss: 1.07033560383651\n",
      "epochs 9/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.8242010548710823, val_loss: 0.7337316616127888\n",
      "epochs 10/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7451301335736558, val_loss: 0.2861925997616102\n",
      "epochs 11/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.34763822621769375, val_loss: 1.2488235086202621\n",
      "epochs 12/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.44499704117576283, val_loss: 4.623830404546526\n",
      "epochs 13/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.1903053947069027, val_loss: 0.0823019502228514\n",
      "epochs 14/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7469020658345135, val_loss: 0.5722381411534216\n",
      "epochs 15/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5057215061452653, val_loss: 0.0715653028681926\n",
      "epochs 16/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.32130170572135186, val_loss: 0.19430488720596864\n",
      "epochs 17/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3507836248587679, val_loss: 0.06746025065513095\n",
      "epochs 18/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.3383925137696444, val_loss: 1.2997338763541646\n",
      "epochs 19/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5556420950150048, val_loss: 0.16188490085349055\n",
      "epochs 20/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3828472424712446, val_loss: 2.2542357759343252\n",
      "epochs 21/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.113933537431337, val_loss: 0.38439660493491423\n",
      "epochs 22/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.2524893288673074, val_loss: 0.36426336106119883\n",
      "epochs 23/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3863297311796082, val_loss: 0.4285024780676597\n",
      "epochs 24/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4380693350815111, val_loss: 0.1081036560822718\n",
      "epochs 25/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3586572898137901, val_loss: 0.05940847042074893\n",
      "epochs 26/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.12448313873675135, val_loss: 0.19239932972899018\n",
      "epochs 27/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.19019955824370738, val_loss: 0.1705708638142419\n",
      "epochs 28/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.26722934152241107, val_loss: 0.057869871914994694\n",
      "epochs 29/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3479584985999046, val_loss: 0.5788986034070452\n",
      "epochs 30/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.40265067252847886, val_loss: 0.14349051125302342\n",
      "epochs 31/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.10960819013416767, val_loss: 0.056123196845065754\n",
      "epochs 32/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 5.943275499054128, val_loss: 0.747322230703301\n",
      "epochs 33/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4639631518059307, val_loss: 0.2433222784214498\n",
      "epochs 34/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.5802758029075685, val_loss: 0.33294063467635876\n",
      "epochs 35/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7934114394916428, val_loss: 1.812910872201125\n",
      "epochs 36/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 7.968079650015743, val_loss: 0.4369603193675478\n",
      "epochs 37/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.9001015780700578, val_loss: 1.3472910709679127\n",
      "epochs 38/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6945907263844101, val_loss: 0.05943342956339216\n",
      "epochs 39/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.9810444502918808, val_loss: 0.2576084223336592\n",
      "epochs 40/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.8870746698920373, val_loss: 0.5716710400043262\n",
      "epochs 41/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.748791657112263, val_loss: 0.7122451520214478\n",
      "epochs 42/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6500085848901007, val_loss: 2.615329702695211\n",
      "epochs 43/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.8784508641671251, val_loss: 0.8089944517446889\n",
      "epochs 44/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.184880306047422, val_loss: 0.058762010149722405\n",
      "epochs 45/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.8757507966624365, val_loss: 4.282648046811421\n",
      "epochs 46/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.9923099342871595, val_loss: 1.8205568351679378\n",
      "epochs 47/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7197808060381148, val_loss: 2.1722381810347238\n",
      "epochs 48/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6663449966797123, val_loss: 0.13070945930182157\n",
      "epochs 49/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.3715903917672458, val_loss: 0.31136695916454\n",
      "epochs 50/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4180867456727558, val_loss: 0.0965257601783378\n",
      "epochs 51/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.14933988465755074, val_loss: 0.14914853813954526\n",
      "epochs 52/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.2276014270329917, val_loss: 1.0052548394434981\n",
      "epochs 53/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4721143314683879, val_loss: 0.05576073203701526\n",
      "epochs 54/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.48881677676130225, val_loss: 0.05785636863179712\n",
      "epochs 55/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.29654841463047044, val_loss: 0.06413115135021347\n",
      "epochs 56/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.15855010143584675, val_loss: 0.108974112137528\n",
      "epochs 57/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.17170469559453153, val_loss: 0.17663193806543454\n",
      "epochs 58/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.21237008746161504, val_loss: 0.08256384125302753\n",
      "epochs 59/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.160101436044055, val_loss: 0.060352219030998334\n",
      "epochs 60/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.21341069925714423, val_loss: 0.06514064406494274\n",
      "epochs 61/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.12907861773338583, val_loss: 0.06314587576323699\n",
      "epochs 62/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.13946647659220077, val_loss: 0.2933639961281895\n",
      "epochs 63/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 6.426770882336078, val_loss: 0.17094375408487394\n",
      "epochs 64/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.23709304361707634, val_loss: 0.10447408916499829\n",
      "epochs 65/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4062538577450646, val_loss: 9.52442120652025\n",
      "epochs 66/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.4877789233017853, val_loss: 0.34036302217282355\n",
      "epochs 67/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.0857381431592836, val_loss: 3.2500849531756506\n",
      "epochs 68/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 24.405110678187125, val_loss: 0.5825820598337386\n",
      "epochs 69/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.487608672292144, val_loss: 12.029772599538168\n",
      "epochs 70/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 13.404319766494963, val_loss: 22.783716758092243\n",
      "epochs 71/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 14.83156914015611, val_loss: 0.8567959889769554\n",
      "epochs 72/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 10.587372918923696, val_loss: 3.6585798043654196\n",
      "epochs 73/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 5.37082560967516, val_loss: 0.19579328634472404\n",
      "epochs 74/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 11.72288806350143, val_loss: 0.1324444106640941\n",
      "epochs 75/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.5660075628095202, val_loss: 0.2822517161257565\n",
      "epochs 76/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.600988561908404, val_loss: 0.05700237024575472\n",
      "epochs 77/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.4854926996760898, val_loss: 0.07325533211567542\n",
      "epochs 78/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.5701035647480576, val_loss: 0.19523891962469658\n",
      "epochs 79/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.38233758398780116, val_loss: 0.07479118191662969\n",
      "epochs 80/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.34985918910415087, val_loss: 0.110421066876572\n",
      "epochs 81/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3429132106679457, val_loss: 0.08225440485532293\n",
      "epochs 82/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.4637291456262271, val_loss: 0.1274166685907403\n",
      "epochs 83/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.49462227633705846, val_loss: 0.32855538121010697\n",
      "epochs 84/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3695146182069072, val_loss: 0.08528671266200642\n",
      "epochs 85/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3587400692480582, val_loss: 0.06435789310813157\n",
      "epochs 86/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.41426591851093153, val_loss: 0.3334000698006194\n",
      "epochs 87/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.372252452980589, val_loss: 0.19387972946889123\n",
      "epochs 88/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.39374975446197724, val_loss: 0.11339740578437664\n",
      "epochs 89/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.45453230098441794, val_loss: 0.06135380886795853\n",
      "epochs 90/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3684507898158497, val_loss: 0.0667161216277034\n",
      "epochs 91/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.4888763775428136, val_loss: 0.3345837923698127\n",
      "epochs 92/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.5466360107616142, val_loss: 0.07049032351642381\n",
      "epochs 93/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3511621863753707, val_loss: 0.07237344832295396\n",
      "epochs 94/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3123403290907542, val_loss: 0.057260878005763516\n",
      "epochs 95/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3780022249729545, val_loss: 0.07018732911253917\n",
      "epochs 96/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3786149049798648, val_loss: 0.07910475431708619\n",
      "epochs 97/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.29543524980545044, val_loss: 0.14651685608744933\n",
      "epochs 98/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.34451596190532047, val_loss: 0.6012408271845844\n",
      "epochs 99/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.41010514619173827, val_loss: 0.07743038053740747\n",
      "epochs 100/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.31605349867432203, val_loss: 0.13271798787172884\n",
      "epochs 101/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.267727796126295, val_loss: 0.10404817795723728\n",
      "epochs 102/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3342136473015503, val_loss: 0.2868178830507936\n",
      "epochs 103/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3557195037051483, val_loss: 0.18077419027637612\n",
      "epochs 104/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3944398269609169, val_loss: 0.07543813540056969\n",
      "epochs 105/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.275040369618822, val_loss: 0.0593773481070659\n",
      "epochs 106/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.2452011113917386, val_loss: 0.07671283887854467\n",
      "epochs 107/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.31484416375557583, val_loss: 0.057265868504247114\n",
      "epochs 108/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.35527342392338646, val_loss: 0.16888768387434538\n",
      "epochs 109/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.2987758715947469, val_loss: 0.20645348965626908\n",
      "epochs 110/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3404920872438837, val_loss: 0.0659532652769283\n",
      "epochs 111/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.25323759157348563, val_loss: 0.12425038846251685\n",
      "epochs 112/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.3527620487191059, val_loss: 0.07465321880752324\n",
      "epochs 113/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.26765599488108244, val_loss: 0.06621593438709776\n",
      "epochs 114/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.29838255889437815, val_loss: 0.14317524443241483\n",
      "epochs 115/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.264964632138058, val_loss: 0.08003594949453448\n",
      "epochs 116/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.2735013070481795, val_loss: 0.06650166732207355\n",
      "epochs 117/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.3361864583911719, val_loss: 0.07558373324961092\n",
      "epochs 118/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.22676188305572229, val_loss: 0.0643980530989615\n",
      "epochs 119/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.22147199870259673, val_loss: 0.05661199501547445\n",
      "epochs 120/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.22560094172755876, val_loss: 0.06812897723607926\n",
      "epochs 121/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.2550135875741641, val_loss: 0.07298750354190513\n",
      "epochs 122/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.27431174643613676, val_loss: 0.05702016133323519\n",
      "epochs 123/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.21994963260712447, val_loss: 0.07320618003723212\n",
      "epochs 124/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.24793697948809024, val_loss: 0.09456752037981965\n",
      "epochs 125/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.22700329456064436, val_loss: 0.059424931524618946\n",
      "epochs 126/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.20372580295359646, val_loss: 0.0611845291148509\n",
      "epochs 127/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.24055862923463187, val_loss: 0.091227296091448\n",
      "epochs 128/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.2287888855293945, val_loss: 0.07632608282599701\n",
      "epochs 129/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.23210966228334992, val_loss: 0.08549351563285906\n",
      "epochs 130/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.19330691711770165, val_loss: 0.05637358710696895\n",
      "epochs 131/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.2462215997554638, val_loss: 0.10497386229771008\n",
      "epochs 132/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.24443259366132594, val_loss: 0.07531694855424576\n",
      "epochs 133/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.18002316962789605, val_loss: 0.1152673522038741\n",
      "epochs 134/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.23514365763575942, val_loss: 0.08282034138877255\n",
      "epochs 135/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.2210275787446234, val_loss: 0.08374423207715154\n",
      "epochs 136/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.1944582142211773, val_loss: 0.16402714850846678\n",
      "epochs 137/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.2655991059210565, val_loss: 0.07504954342018916\n",
      "epochs 138/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.1890217532162313, val_loss: 0.061494593342002996\n",
      "epochs 139/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.202480332305034, val_loss: 0.115159343588554\n",
      "epochs 140/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.22425360950054946, val_loss: 0.18633585807401687\n",
      "epochs 141/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.28515638162692386, val_loss: 0.16193672788277683\n",
      "epochs 142/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.41141302373122285, val_loss: 0.12619443716903334\n",
      "epochs 143/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.4159461028046078, val_loss: 0.2993863375288331\n",
      "epochs 144/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.2544924115969075, val_loss: 0.0710916290376594\n",
      "epochs 145/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.22391623341374928, val_loss: 0.11318955386458482\n",
      "epochs 146/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.18236737174016457, val_loss: 0.05562242818672934\n",
      "epochs 147/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.14154929046829542, val_loss: 0.07404756308339226\n",
      "epochs 148/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.18045801117464347, val_loss: 0.05797209608135745\n",
      "epochs 149/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.20438597875612755, val_loss: 0.0880188108195499\n",
      "epochs 150/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.18262159438044936, val_loss: 0.06724033205440112\n",
      "epochs 151/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.21046132732320716, val_loss: 0.06276174676895607\n",
      "epochs 152/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.23132411487124585, val_loss: 0.15240585766393147\n",
      "epochs 153/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.2039354976128649, val_loss: 0.06680307711293507\n",
      "epochs 154/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.1567918549257296, val_loss: 0.05897745897689472\n",
      "epochs 155/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.16392675566452522, val_loss: 0.06422324149752967\n",
      "epochs 156/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.17308126969469917, val_loss: 0.08520987331414492\n",
      "epochs 157/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.17071783915162086, val_loss: 0.0903049753461447\n",
      "epochs 158/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.1620057964766467, val_loss: 0.05829360362582116\n",
      "epochs 159/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.1556182022171992, val_loss: 0.08530863171391603\n",
      "epochs 160/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.1581466606369725, val_loss: 0.073154640778537\n",
      "epochs 161/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.15439907240646858, val_loss: 0.05857391628281524\n",
      "epochs 162/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14456276636984614, val_loss: 0.06713771748602287\n",
      "epochs 163/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13077251157826847, val_loss: 0.05938803982199817\n",
      "epochs 164/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.11189404468017596, val_loss: 0.0584450375608867\n",
      "epochs 165/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12889042131050868, val_loss: 0.05837414304551203\n",
      "epochs 166/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.1540221091773775, val_loss: 0.06515799456004363\n",
      "epochs 167/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12308818025997391, val_loss: 0.06029529601801187\n",
      "epochs 168/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14985941640204853, val_loss: 0.058134698771431834\n",
      "epochs 169/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.15871663019061089, val_loss: 0.0594684993896711\n",
      "epochs 170/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13361959255955838, val_loss: 0.05895910054742773\n",
      "epochs 171/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.15377101164173196, val_loss: 0.05869851662848507\n",
      "epochs 172/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14067673848734963, val_loss: 0.05922167772627694\n",
      "epochs 173/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.1387861257901898, val_loss: 0.058177691861702546\n",
      "epochs 174/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13620191177836172, val_loss: 0.0586538173132188\n",
      "epochs 175/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14464758170975578, val_loss: 0.06642229565315777\n",
      "epochs 176/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12080201396235714, val_loss: 0.05744436568218387\n",
      "epochs 177/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12795891943905088, val_loss: 0.057496424499226526\n",
      "epochs 178/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14094508384112958, val_loss: 0.06444686403928143\n",
      "epochs 179/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.1499054147689431, val_loss: 0.06271016687161238\n",
      "epochs 180/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.10896514480312665, val_loss: 0.06261631331128431\n",
      "epochs 181/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14094228297472, val_loss: 0.05991345213664721\n",
      "epochs 182/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13876551596654785, val_loss: 0.06271040635167285\n",
      "epochs 183/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12200380723785471, val_loss: 0.05695246933222128\n",
      "epochs 184/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.11023599888991427, val_loss: 0.05858453815421348\n",
      "epochs 185/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14323027818291276, val_loss: 0.06357715804996486\n",
      "epochs 186/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14997508928731637, val_loss: 0.06406838899824328\n",
      "epochs 187/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13614334704147446, val_loss: 0.06380566739891139\n",
      "epochs 188/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.11836937539003513, val_loss: 0.0564978317205613\n",
      "epochs 189/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13745783434973824, val_loss: 0.06472760116917521\n",
      "epochs 190/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12923379325204426, val_loss: 0.05977145383723029\n",
      "epochs 191/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.11951793868232656, val_loss: 0.060678452639775865\n",
      "epochs 192/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12886976174734258, val_loss: 0.0645495751854873\n",
      "epochs 193/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.11618113290104601, val_loss: 0.05711825064726225\n",
      "epochs 194/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12100232585712715, val_loss: 0.06385324108123314\n",
      "epochs 195/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14048449432960264, val_loss: 0.06226528904517181\n",
      "epochs 196/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12555917186869514, val_loss: 0.06495152876919343\n",
      "epochs 197/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.11479662634708264, val_loss: 0.06919800177274738\n",
      "epochs 198/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13593094809739678, val_loss: 0.07313072893884964\n",
      "epochs 199/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.10896554078768801, val_loss: 0.05657190790872038\n",
      "epochs 200/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12463801257588246, val_loss: 0.07799134420282725\n",
      "epochs 201/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13722037661958625, val_loss: 0.061338545092439745\n",
      "epochs 202/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12377236107433284, val_loss: 0.05744905756728258\n",
      "epochs 203/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.15290620164186866, val_loss: 0.08983588657671741\n",
      "epochs 204/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12233912917198958, val_loss: 0.064125944250716\n",
      "epochs 205/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.11147074553149718, val_loss: 0.059903926020423465\n",
      "epochs 206/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13605788998581744, val_loss: 0.08673684083300436\n",
      "epochs 207/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13512383829112407, val_loss: 0.0652825965096579\n",
      "epochs 208/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.13103796320932884, val_loss: 0.06093947395533582\n",
      "epochs 209/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.10942598170152416, val_loss: 0.07745948072018412\n",
      "epochs 210/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12973695731273405, val_loss: 0.06771265592882021\n",
      "epochs 211/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12517698854207993, val_loss: 0.0721523154352326\n",
      "epochs 212/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.12761544091282068, val_loss: 0.07437709797522984\n",
      "epochs 213/213\n",
      "Current Learning Rate: 3.5352942470606816e-05\n",
      "train_loss: 0.14301533652124582, val_loss: 0.06533115547952345\n",
      "Fold: 2/4\n",
      "epochs 1/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 55.12534050995277, val_loss: 13.858163414729965\n",
      "epochs 2/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 4.811583879369277, val_loss: 1.1625788439479139\n",
      "epochs 3/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 6.2710816691871045, val_loss: 1.4249698602490954\n",
      "epochs 4/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.253653096931952, val_loss: 0.8218759194844298\n",
      "epochs 5/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.4094107277967312, val_loss: 2.9733265572124057\n",
      "epochs 6/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.888839581636367, val_loss: 14.07735628551907\n",
      "epochs 7/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.6988643688598164, val_loss: 0.6905144424074225\n",
      "epochs 8/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7039770995163255, val_loss: 1.2116421196195815\n",
      "epochs 9/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.46627979656612434, val_loss: 0.17081958485909532\n",
      "epochs 10/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5744411587991096, val_loss: 0.16702834421691173\n",
      "epochs 11/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4582511188669337, val_loss: 0.07850007152713563\n",
      "epochs 12/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4349852927650015, val_loss: 0.1615219890405165\n",
      "epochs 13/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5577945503647681, val_loss: 0.06359531307336713\n",
      "epochs 14/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.394311964649845, val_loss: 0.03581771269414781\n",
      "epochs 15/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.24358367092079586, val_loss: 0.06006542295734915\n",
      "epochs 16/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.24328451156961145, val_loss: 0.0354680813114909\n",
      "epochs 17/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.21320967621135492, val_loss: 0.10722037391193832\n",
      "epochs 18/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7335646857542021, val_loss: 2.0605971250269146\n",
      "epochs 19/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.49729853423519266, val_loss: 0.1852900096241178\n",
      "epochs 20/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.33358240965753794, val_loss: 0.15273796800627476\n",
      "epochs 21/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.35840686679714256, val_loss: 0.6123139696816603\n",
      "epochs 22/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.2203850976053487, val_loss: 0.11750691495641756\n",
      "epochs 23/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.8633956148599584, val_loss: 0.2561850179530059\n",
      "epochs 24/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6627818878602099, val_loss: 1.199664490090476\n",
      "epochs 25/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.0489767380748634, val_loss: 0.03603101905577609\n",
      "epochs 26/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.650014721064104, val_loss: 2.214856407708592\n",
      "epochs 27/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.1040092925368636, val_loss: 0.09286387630062462\n",
      "epochs 28/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7497311607003212, val_loss: 0.038976958183209516\n",
      "epochs 29/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6238339643373534, val_loss: 0.11486885766983808\n",
      "epochs 30/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.37662154898323397, val_loss: 1.9390694399674733\n",
      "epochs 31/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.31744132359960564, val_loss: 0.03968339349537726\n",
      "epochs 32/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.27711063778648776, val_loss: 10.463116157373102\n",
      "epochs 33/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4481188763726365, val_loss: 0.03466839506129165\n",
      "epochs 34/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.21215798702366925, val_loss: 0.045765127048879445\n",
      "epochs 35/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6678612625778273, val_loss: 0.12818410652592624\n",
      "epochs 36/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.2553723901855173, val_loss: 0.03700945731705158\n",
      "epochs 37/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.19925745509357917, val_loss: 0.05315653123802298\n",
      "epochs 38/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3483170096383051, val_loss: 0.4930256029797925\n",
      "epochs 39/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1594979681626514, val_loss: 0.03589971970197641\n",
      "epochs 40/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.12593985805977825, val_loss: 0.12046821598455103\n",
      "epochs 41/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.16234189361609794, val_loss: 2.701509471568796\n",
      "epochs 42/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.2950414064581748, val_loss: 1.8142189664973154\n",
      "epochs 43/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.0075839000443616, val_loss: 1.024650450795889\n",
      "epochs 44/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.582064357586205, val_loss: 3.2663349476125507\n",
      "epochs 45/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7572370754485881, val_loss: 3.133497658703062\n",
      "epochs 46/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 60.63513469696045, val_loss: 32.225314219792686\n",
      "epochs 47/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 16.685234169165295, val_loss: 17.150005055798424\n",
      "epochs 48/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 21.39269465704759, val_loss: 10.121799919340345\n",
      "epochs 49/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 8.527634705658313, val_loss: 0.034894475578362796\n",
      "epochs 50/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 7.362122056660829, val_loss: 4.560826626088884\n",
      "epochs 51/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.143358615813432, val_loss: 0.08411843227764217\n",
      "epochs 52/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 4.948123845789167, val_loss: 0.043868390589826355\n",
      "epochs 53/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.4387186249649084, val_loss: 5.5984622372521295\n",
      "epochs 54/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.9819662961418982, val_loss: 0.03902598661483757\n",
      "epochs 55/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.8653817350665727, val_loss: 6.6711817317538795\n",
      "epochs 56/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.9326585765790056, val_loss: 0.17416819295613095\n",
      "epochs 57/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.9770591581861177, val_loss: 0.03863810226014013\n",
      "epochs 58/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6818377107933715, val_loss: 1.7336058765649796\n",
      "epochs 59/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.596464462854244, val_loss: 0.04336182631312921\n",
      "epochs 60/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.2521369181297444, val_loss: 1.1781718913051817\n",
      "epochs 61/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.8842101044676922, val_loss: 0.16843735003688684\n",
      "epochs 62/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.34358169738617206, val_loss: 0.1683220078314965\n",
      "epochs 63/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7130093899452024, val_loss: 0.11434958017424732\n",
      "epochs 64/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.42831009322846375, val_loss: 0.1483003029651526\n",
      "epochs 65/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.2823140616356223, val_loss: 0.04329408007712724\n",
      "epochs 66/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.2501271648539438, val_loss: 0.04409799621822054\n",
      "epochs 67/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5690645307164501, val_loss: 0.03596442697259287\n",
      "epochs 68/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.53752797809257, val_loss: 0.05137672899662801\n",
      "epochs 69/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.37024385413085975, val_loss: 0.06541411829877891\n",
      "epochs 70/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.20870010506499698, val_loss: 0.13866651881835423\n",
      "epochs 71/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7869534702557657, val_loss: 0.19322772555622375\n",
      "epochs 72/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.23914610718687376, val_loss: 0.034355954777715345\n",
      "epochs 73/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.8730710622406116, val_loss: 0.08454334228978648\n",
      "epochs 74/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.33092909233851564, val_loss: 0.03481573077472907\n",
      "epochs 75/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.19997812086647307, val_loss: 0.035167081256481145\n",
      "epochs 76/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.18856810757683384, val_loss: 0.11630975639693336\n",
      "epochs 77/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.15976417761434009, val_loss: 0.060670630446919754\n",
      "epochs 78/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.8035952862766054, val_loss: 0.7595829760862721\n",
      "epochs 79/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.9476676050711561, val_loss: 0.14510006915548324\n",
      "epochs 80/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.25260257106964235, val_loss: 0.07263851352521063\n",
      "epochs 81/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3633411241074403, val_loss: 0.6469921600073576\n",
      "epochs 82/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.3097835688472346, val_loss: 0.43724971926874584\n",
      "epochs 83/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.2613635011776178, val_loss: 0.47394139433486593\n",
      "epochs 84/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 3.399788224034839, val_loss: 0.034901685015130274\n",
      "epochs 85/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.19949133845942993, val_loss: 0.04233538545627703\n",
      "epochs 86/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.2492615937910698, val_loss: 0.04861139924469171\n",
      "epochs 87/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.18226932775643137, val_loss: 0.08548840083676623\n",
      "epochs 88/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.17404948637165404, val_loss: 0.03572903667372884\n",
      "epochs 89/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08758835435879452, val_loss: 0.06556089855762871\n",
      "epochs 90/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08852584601414425, val_loss: 0.03624429455440906\n",
      "epochs 91/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07584357741116374, val_loss: 0.04916497555920311\n",
      "epochs 92/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10374521990134208, val_loss: 0.08265619526112762\n",
      "epochs 93/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08938669047697827, val_loss: 0.034493220876407174\n",
      "epochs 94/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0728718732242231, val_loss: 0.03437824100991646\n",
      "epochs 95/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08727188760207759, val_loss: 0.050388548995316446\n",
      "epochs 96/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10471198102459311, val_loss: 0.11399537952578005\n",
      "epochs 97/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10914328723662982, val_loss: 0.034538354966773314\n",
      "epochs 98/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06686789214749027, val_loss: 0.038035873085997686\n",
      "epochs 99/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10175544071804594, val_loss: 0.043655495851352394\n",
      "epochs 100/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.11653167016252323, val_loss: 0.054546824960400246\n",
      "epochs 101/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0730620685585395, val_loss: 0.03984413113600264\n",
      "epochs 102/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09104930349039259, val_loss: 0.05414430816745153\n",
      "epochs 103/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.1102775289349396, val_loss: 0.114310749547763\n",
      "epochs 104/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.1110237073843126, val_loss: 0.034298519393284245\n",
      "epochs 105/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06633204580457122, val_loss: 0.03498108917362212\n",
      "epochs 106/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09436702367815154, val_loss: 0.07329321307467278\n",
      "epochs 107/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09943452539543311, val_loss: 0.07462456662485945\n",
      "epochs 108/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.13471087913408322, val_loss: 0.06559388279209896\n",
      "epochs 109/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0733237380368842, val_loss: 0.040588771756221026\n",
      "epochs 110/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0855072974572303, val_loss: 0.03487323575649902\n",
      "epochs 111/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10481547750532627, val_loss: 0.04932731988891545\n",
      "epochs 112/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.11920902127813962, val_loss: 0.10807455207880896\n",
      "epochs 113/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08476656816761803, val_loss: 0.040098216408902466\n",
      "epochs 114/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10644256773715217, val_loss: 0.03807956121131105\n",
      "epochs 115/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09042472130170574, val_loss: 0.05579423305688477\n",
      "epochs 116/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.14001690285901228, val_loss: 0.11534648109065732\n",
      "epochs 117/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08392687041864351, val_loss: 0.03749048140404435\n",
      "epochs 118/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08075463373420967, val_loss: 0.042377452747637614\n",
      "epochs 119/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08531625568004395, val_loss: 0.06999540126384494\n",
      "epochs 120/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08735125711946576, val_loss: 0.07109871959366704\n",
      "epochs 121/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08585021151574673, val_loss: 0.061593985464343054\n",
      "epochs 122/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08917852360927672, val_loss: 0.05485774126403461\n",
      "epochs 123/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0913512719001759, val_loss: 0.04588247429880236\n",
      "epochs 124/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10466929301791997, val_loss: 0.08614233492011812\n",
      "epochs 125/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08810850360465271, val_loss: 0.054885846005668606\n",
      "epochs 126/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09027990062410633, val_loss: 0.05019339207339928\n",
      "epochs 127/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09671307155103595, val_loss: 0.0451983201184905\n",
      "epochs 128/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.10908843025013253, val_loss: 0.08417489693359433\n",
      "epochs 129/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0836888267831118, val_loss: 0.049171570603599925\n",
      "epochs 130/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09316295161153432, val_loss: 0.06122442958743098\n",
      "epochs 131/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0851134195992792, val_loss: 0.04243664079533321\n",
      "epochs 132/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.09971788013353944, val_loss: 0.04864194127811869\n",
      "epochs 133/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.11773613792050767, val_loss: 0.07754602649947628\n",
      "epochs 134/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07591158767334288, val_loss: 0.044674489472830224\n",
      "epochs 135/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07653357092015169, val_loss: 0.06206104383778034\n",
      "epochs 136/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.08047611475059832, val_loss: 0.05391155600793556\n",
      "epochs 137/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07275304201714418, val_loss: 0.045031568074288465\n",
      "epochs 138/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0730954183920942, val_loss: 0.05991657243349538\n",
      "epochs 139/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07444360247744178, val_loss: 0.041672835818139395\n",
      "epochs 140/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06667745453133075, val_loss: 0.04547936539084832\n",
      "epochs 141/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06671658805054095, val_loss: 0.04713141062610601\n",
      "epochs 142/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.070709731084881, val_loss: 0.05047071036581959\n",
      "epochs 143/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06427435350121448, val_loss: 0.04344616018028723\n",
      "epochs 144/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06508919750167816, val_loss: 0.046581686338969225\n",
      "epochs 145/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06512701349263941, val_loss: 0.04206549969603657\n",
      "epochs 146/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06503913432566656, val_loss: 0.043357862571509836\n",
      "epochs 147/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06504746235010249, val_loss: 0.040227182621796\n",
      "epochs 148/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06423722350693963, val_loss: 0.04475455094658779\n",
      "epochs 149/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06283516856772756, val_loss: 0.04191281606634018\n",
      "epochs 150/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06513653068665277, val_loss: 0.04239340474224365\n",
      "epochs 151/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06256054988544849, val_loss: 0.04542321645971646\n",
      "epochs 152/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06480583677896196, val_loss: 0.04511255387963805\n",
      "epochs 153/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06301812344679127, val_loss: 0.04165366893155604\n",
      "epochs 154/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06333427529575096, val_loss: 0.04171048585582563\n",
      "epochs 155/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06248600360144068, val_loss: 0.03996412617642717\n",
      "epochs 156/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06351164760309513, val_loss: 0.041405144128172346\n",
      "epochs 157/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06512209935182775, val_loss: 0.040096015847262204\n",
      "epochs 158/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06439953972585499, val_loss: 0.03883822382906348\n",
      "epochs 159/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06566066950283668, val_loss: 0.037314557831753824\n",
      "epochs 160/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0645110559804986, val_loss: 0.03765706446251392\n",
      "epochs 161/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06467697940145929, val_loss: 0.03770173825937996\n",
      "epochs 162/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06390865611050416, val_loss: 0.03732903186513189\n",
      "epochs 163/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0658591651806125, val_loss: 0.03623028128479038\n",
      "epochs 164/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06655594272780474, val_loss: 0.0349220578350974\n",
      "epochs 165/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06521090678870678, val_loss: 0.03531795153362004\n",
      "epochs 166/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06706651213958308, val_loss: 0.035366310278024886\n",
      "epochs 167/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06832171579891885, val_loss: 0.0345356927551216\n",
      "epochs 168/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06741523109721365, val_loss: 0.034594598605811676\n",
      "epochs 169/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06760241147303195, val_loss: 0.03455269021469576\n",
      "epochs 170/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0691791695577127, val_loss: 0.03474107541680698\n",
      "epochs 171/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06834269862587529, val_loss: 0.03476126830840561\n",
      "epochs 172/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06938880513836113, val_loss: 0.03541162689604486\n",
      "epochs 173/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06969318732929726, val_loss: 0.03509941863901784\n",
      "epochs 174/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07090639211754832, val_loss: 0.036229454668096475\n",
      "epochs 175/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07299137346584487, val_loss: 0.04059260970340498\n",
      "epochs 176/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06925484858004859, val_loss: 0.03894234367180616\n",
      "epochs 177/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07039831364009944, val_loss: 0.04344853276173429\n",
      "epochs 178/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07006548775394482, val_loss: 0.04240387352021773\n",
      "epochs 179/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06955466255614604, val_loss: 0.043982117147404805\n",
      "epochs 180/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0711531381174508, val_loss: 0.04753967939258372\n",
      "epochs 181/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07130355882044467, val_loss: 0.04707916726409975\n",
      "epochs 182/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07128295234697699, val_loss: 0.049548255041170504\n",
      "epochs 183/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07017545583140519, val_loss: 0.04824963389061546\n",
      "epochs 184/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06893049816049084, val_loss: 0.047790905299027346\n",
      "epochs 185/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06952928991436406, val_loss: 0.048691919864924986\n",
      "epochs 186/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07082904747221619, val_loss: 0.04934416107425932\n",
      "epochs 187/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06985017099348759, val_loss: 0.04932404473301075\n",
      "epochs 188/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07045067195711588, val_loss: 0.053813734821030974\n",
      "epochs 189/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07034783402492327, val_loss: 0.051076504169840414\n",
      "epochs 190/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06931859294504479, val_loss: 0.05249252785304432\n",
      "epochs 191/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.07033105528292556, val_loss: 0.053980311982336895\n",
      "epochs 192/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.0691208842003511, val_loss: 0.05209039094754391\n",
      "epochs 193/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06931979073366772, val_loss: 0.05043032620920308\n",
      "epochs 194/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06825864168435887, val_loss: 0.05269504652824253\n",
      "epochs 195/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06787161783453215, val_loss: 0.052410506694286596\n",
      "epochs 196/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06869844717611731, val_loss: 0.053417886216872525\n",
      "epochs 197/213\n",
      "Current Learning Rate: 0.00380843206341203\n",
      "train_loss: 0.06948815780277881, val_loss: 0.05514169992036639\n",
      "epochs 198/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.07065563542440671, val_loss: 0.05314754236557443\n",
      "epochs 199/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.054699419514724505, val_loss: 0.04435099868391666\n",
      "epochs 200/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.054921967563805754, val_loss: 0.04432045793348354\n",
      "epochs 201/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05371087705026622, val_loss: 0.04516163782601426\n",
      "epochs 202/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05517041706480086, val_loss: 0.04509834121240096\n",
      "epochs 203/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05437724992494892, val_loss: 0.045190352812318854\n",
      "epochs 204/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05488713246045841, val_loss: 0.04529419995540391\n",
      "epochs 205/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05474208748933894, val_loss: 0.04556359300558041\n",
      "epochs 206/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05471558113478952, val_loss: 0.0457258511451073\n",
      "epochs 207/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05530873627436382, val_loss: 0.045410069294222116\n",
      "epochs 208/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05441776973505815, val_loss: 0.045294274597027756\n",
      "epochs 209/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.055616883354054555, val_loss: 0.04484517595200385\n",
      "epochs 210/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05485468490601138, val_loss: 0.0453040069955427\n",
      "epochs 211/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05577854644852104, val_loss: 0.04618780763121322\n",
      "epochs 212/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.05524460966181424, val_loss: 0.0452064549552031\n",
      "epochs 213/213\n",
      "Current Learning Rate: 0.00036693225483870986\n",
      "train_loss: 0.0535540625590969, val_loss: 0.045586081715656396\n",
      "Fold: 3/4\n",
      "epochs 1/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 30.540296946594744, val_loss: 0.05799428068470055\n",
      "epochs 2/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.957471109006876, val_loss: 7.913332713974847\n",
      "epochs 3/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.5228367484876033, val_loss: 0.0718484185442018\n",
      "epochs 4/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7572451680438754, val_loss: 0.18177439687294988\n",
      "epochs 5/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6500748023160814, val_loss: 2.323241889476776\n",
      "epochs 6/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.394896144438305, val_loss: 0.33165212953463197\n",
      "epochs 7/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.957923252947548, val_loss: 0.07186212991493651\n",
      "epochs 8/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.6641359406029, val_loss: 1.4448907830648952\n",
      "epochs 9/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5277461770600007, val_loss: 0.051428889304386556\n",
      "epochs 10/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3862701402402218, val_loss: 0.5114175395833122\n",
      "epochs 11/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5665209754197685, val_loss: 0.27241125692509943\n",
      "epochs 12/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7777694251825228, val_loss: 1.202226147055626\n",
      "epochs 13/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.7763295081432586, val_loss: 0.544181359724866\n",
      "epochs 14/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6947761654025979, val_loss: 1.2012989454799228\n",
      "epochs 15/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5428391543366475, val_loss: 0.16236640902287844\n",
      "epochs 16/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.0439353223407157, val_loss: 0.09557786912126984\n",
      "epochs 17/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.24515825374350872, val_loss: 0.37999555830740267\n",
      "epochs 18/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.913395036632816, val_loss: 0.2404535822570324\n",
      "epochs 19/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3783447004072828, val_loss: 0.09213210701677275\n",
      "epochs 20/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.1687888238050135, val_loss: 0.7754012391799026\n",
      "epochs 21/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.43911276378289416, val_loss: 0.04405401282807967\n",
      "epochs 22/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5617323577587987, val_loss: 1.9857895970344543\n",
      "epochs 23/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.43647864575923223, val_loss: 0.284981263620365\n",
      "epochs 24/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3942535816185912, val_loss: 1.1300443365342088\n",
      "epochs 25/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5292403633004905, val_loss: 0.8271611618498961\n",
      "epochs 26/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.48008899785072345, val_loss: 0.7095169733381934\n",
      "epochs 27/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.22164460642976158, val_loss: 0.0629408907585053\n",
      "epochs 28/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6697845998691556, val_loss: 0.3613861987574233\n",
      "epochs 29/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1344845033377225, val_loss: 0.5315972001602253\n",
      "epochs 30/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.952699772829627, val_loss: 0.5356331049568124\n",
      "epochs 31/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6184613306167317, val_loss: 0.3368432693597343\n",
      "epochs 32/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3584800694413759, val_loss: 0.1519115862255502\n",
      "epochs 33/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4981588953761039, val_loss: 0.25482878973707557\n",
      "epochs 34/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.33451743533167944, val_loss: 1.5507462115751371\n",
      "epochs 35/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3172736242758455, val_loss: 0.6087983167833753\n",
      "epochs 36/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 35.34301290283968, val_loss: 26.95809347099728\n",
      "epochs 37/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 34.186766021045635, val_loss: 59.47872967190213\n",
      "epochs 38/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 82.86334120785749, val_loss: 1.6437845072812505\n",
      "epochs 39/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 18.216728338488824, val_loss: 29.26778705914815\n",
      "epochs 40/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 12.265327624332757, val_loss: 0.37083894987073207\n",
      "epochs 41/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.5612932899851857, val_loss: 19.443289346165127\n",
      "epochs 42/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 4.2863549292087555, val_loss: 0.6352333968712224\n",
      "epochs 43/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 3.0647574346742514, val_loss: 1.828223102622562\n",
      "epochs 44/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.076140954170698, val_loss: 2.6841569907135434\n",
      "epochs 45/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.3356133506253913, val_loss: 0.05345057811387556\n",
      "epochs 46/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6906157936210986, val_loss: 12.860595928298103\n",
      "epochs 47/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.6617321482411138, val_loss: 0.043616265330153205\n",
      "epochs 48/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.9213095450842822, val_loss: 0.12742208206651007\n",
      "epochs 49/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5964929877239981, val_loss: 0.309620623373323\n",
      "epochs 50/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4642852852925842, val_loss: 0.935081261727545\n",
      "epochs 51/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 14.618152254709491, val_loss: 1.2660517514579825\n",
      "epochs 52/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 1.3976354955891033, val_loss: 2.0823537988795175\n",
      "epochs 53/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7747018661028073, val_loss: 0.7795867483235068\n",
      "epochs 54/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5171274943392218, val_loss: 0.563767757680681\n",
      "epochs 55/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6672482760968032, val_loss: 0.23164073863558265\n",
      "epochs 56/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.5251750969500454, val_loss: 0.2704189328425046\n",
      "epochs 57/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.6415214631936432, val_loss: 1.8783050527175267\n",
      "epochs 58/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.22290309467985306, val_loss: 0.5121047494726049\n",
      "epochs 59/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.19602447939047843, val_loss: 0.06826738778100763\n",
      "epochs 60/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.15129278492136503, val_loss: 0.48028955867307055\n",
      "epochs 61/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.14918087360759577, val_loss: 0.3615203184179134\n",
      "epochs 62/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1272317164313094, val_loss: 0.4989606536303957\n",
      "epochs 63/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.22386893096529406, val_loss: 0.35612617443419164\n",
      "epochs 64/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.15048785829617653, val_loss: 0.477353445150786\n",
      "epochs 65/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.162130030584924, val_loss: 0.8337857880526118\n",
      "epochs 66/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.15486756808779858, val_loss: 0.06345107055191572\n",
      "epochs 67/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.13215593433720463, val_loss: 0.1209128688351484\n",
      "epochs 68/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.11777591051096901, val_loss: 0.3710513689244787\n",
      "epochs 69/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.3702093390173014, val_loss: 0.1797893108580158\n",
      "epochs 70/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.16760440952615974, val_loss: 0.43471512891766095\n",
      "epochs 71/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.15893443610611152, val_loss: 0.10518298841553689\n",
      "epochs 72/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1284485114623367, val_loss: 0.42171011011426646\n",
      "epochs 73/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.15408369732077842, val_loss: 0.22509522479958832\n",
      "epochs 74/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.12530329859137168, val_loss: 0.8666461822059419\n",
      "epochs 75/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.14724606366391169, val_loss: 0.21573524730693963\n",
      "epochs 76/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.13718786956397472, val_loss: 0.6062780575205883\n",
      "epochs 77/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1407055322908693, val_loss: 0.1408867286760748\n",
      "epochs 78/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.14719491141537824, val_loss: 0.1953484530095011\n",
      "epochs 79/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1111465416457734, val_loss: 0.38046891811407274\n",
      "epochs 80/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.11261924618372211, val_loss: 0.2503047144661347\n",
      "epochs 81/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.08853982252149302, val_loss: 0.28536763368174434\n",
      "epochs 82/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.12180305625323529, val_loss: 0.2566741670533601\n",
      "epochs 83/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 2.086445053970372, val_loss: 0.10788272036653426\n",
      "epochs 84/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.7280590667584796, val_loss: 2.3591291689210467\n",
      "epochs 85/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.4094835670181998, val_loss: 0.1633309607859701\n",
      "epochs 86/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.21533618001612248, val_loss: 0.9609948359429836\n",
      "epochs 87/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.16584646025741542, val_loss: 0.6119736306783226\n",
      "epochs 88/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.12792809566275942, val_loss: 0.49868531794183785\n",
      "epochs 89/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.18924474575913247, val_loss: 0.04929885378401701\n",
      "epochs 90/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.14486123394956926, val_loss: 0.5434940662235022\n",
      "epochs 91/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1469289868549777, val_loss: 0.42865688788394135\n",
      "epochs 92/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1305432533155437, val_loss: 0.2689935501354436\n",
      "epochs 93/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.22596567612785248, val_loss: 0.7340903464290831\n",
      "epochs 94/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.14745867022393663, val_loss: 0.4409398546235429\n",
      "epochs 95/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.31986532675355306, val_loss: 0.29350115534745985\n",
      "epochs 96/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.13348915964296018, val_loss: 0.540429094599353\n",
      "epochs 97/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1859951032049678, val_loss: 0.1772987904259935\n",
      "epochs 98/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.2141532863570768, val_loss: 0.46704499423503876\n",
      "epochs 99/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.11413711739451061, val_loss: 0.5714049829790989\n",
      "epochs 100/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1297467283985037, val_loss: 0.5821471738939484\n",
      "epochs 101/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1060871605899323, val_loss: 0.2750688190054562\n",
      "epochs 102/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.11150390590414588, val_loss: 0.21835558604086852\n",
      "epochs 103/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.14537730338166893, val_loss: 0.17047818550296748\n",
      "epochs 104/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09791727106512328, val_loss: 0.1641011171337191\n",
      "epochs 105/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09621766789091958, val_loss: 0.18013443581164917\n",
      "epochs 106/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09007882383180621, val_loss: 0.3165147000302871\n",
      "epochs 107/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1017896447468687, val_loss: 0.5984380133450031\n",
      "epochs 108/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.122750021804722, val_loss: 0.3769839612974061\n",
      "epochs 109/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1818023289864262, val_loss: 0.12928927615252583\n",
      "epochs 110/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.10818715147490118, val_loss: 0.6266226896809207\n",
      "epochs 111/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.10389916464847362, val_loss: 0.38119561556312775\n",
      "epochs 112/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.08440398572035777, val_loss: 0.2396740110642794\n",
      "epochs 113/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.10440096849529648, val_loss: 0.39016612908906406\n",
      "epochs 114/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09935081583436256, val_loss: 0.2174672402519112\n",
      "epochs 115/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1395182053753991, val_loss: 0.17173389173048134\n",
      "epochs 116/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09395084531264908, val_loss: 0.409708784479234\n",
      "epochs 117/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.10029877902204055, val_loss: 0.2572396287642833\n",
      "epochs 118/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.10587860119563562, val_loss: 0.3345892291205625\n",
      "epochs 119/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09804705651132045, val_loss: 0.480045298838781\n",
      "epochs 120/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.1164274054383974, val_loss: 0.3759205090916819\n",
      "epochs 121/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.08295359815482373, val_loss: 0.1522906787043515\n",
      "epochs 122/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.08484595961132903, val_loss: 0.3152917764770488\n",
      "epochs 123/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.10379698595469013, val_loss: 0.20764662437917045\n",
      "epochs 124/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.08355363712496595, val_loss: 0.3265173367431594\n",
      "epochs 125/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09864179585358979, val_loss: 0.2462805681861937\n",
      "epochs 126/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.09359987262624925, val_loss: 0.363597529526386\n",
      "epochs 127/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.07142289378392844, val_loss: 0.054390385793036934\n",
      "epochs 128/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.0872526366930501, val_loss: 0.22942699422128499\n",
      "epochs 129/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.0915025210203488, val_loss: 0.41607356951054597\n",
      "epochs 130/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.06941326392561566, val_loss: 0.1605051509816096\n",
      "epochs 131/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.06740938720902727, val_loss: 0.05244989327750065\n",
      "epochs 132/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.06861906436582406, val_loss: 0.0797613719672275\n",
      "epochs 133/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.059166761943035655, val_loss: 0.0661210779669798\n",
      "epochs 134/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05769129852867789, val_loss: 0.07205134566821572\n",
      "epochs 135/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05621021439861736, val_loss: 0.0701233680107786\n",
      "epochs 136/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.0557397546959512, val_loss: 0.0810238925267994\n",
      "epochs 137/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.054448556936817404, val_loss: 0.08293889614934516\n",
      "epochs 138/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05381518175802481, val_loss: 0.07607852016291064\n",
      "epochs 139/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05475050528292303, val_loss: 0.0834428260358335\n",
      "epochs 140/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.053979708870619904, val_loss: 0.0932989225580564\n",
      "epochs 141/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05316295662181981, val_loss: 0.09280436426666307\n",
      "epochs 142/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05341966186914547, val_loss: 0.0954870019631926\n",
      "epochs 143/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05310890761514505, val_loss: 0.08969552569841552\n",
      "epochs 144/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.053247007115939514, val_loss: 0.08728773595713493\n",
      "epochs 145/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.0532778979819498, val_loss: 0.09276654399258809\n",
      "epochs 146/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.053192281497665396, val_loss: 0.08329679615013043\n",
      "epochs 147/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.052945426745731154, val_loss: 0.08749210443218342\n",
      "epochs 148/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05296224784565928, val_loss: 0.09408782510500815\n",
      "epochs 149/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05255030120872421, val_loss: 0.09863850626829339\n",
      "epochs 150/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.052619455865135896, val_loss: 0.10136970060476516\n",
      "epochs 151/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05247059497798299, val_loss: 0.10806407680825537\n",
      "epochs 152/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.052375533480059216, val_loss: 0.11439026499606876\n",
      "epochs 153/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05185192274964518, val_loss: 0.11807581156770336\n",
      "epochs 154/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05199483870011237, val_loss: 0.12213345879313743\n",
      "epochs 155/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05151014027680144, val_loss: 0.12253677572880406\n",
      "epochs 156/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.051708497664672726, val_loss: 0.12641760891791717\n",
      "epochs 157/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05124656411094798, val_loss: 0.12475495079868576\n",
      "epochs 158/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05136911048450404, val_loss: 0.12758593024207382\n",
      "epochs 159/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.051028488859258314, val_loss: 0.1263057949860619\n",
      "epochs 160/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05100007984143348, val_loss: 0.1269324486776087\n",
      "epochs 161/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05101060988611461, val_loss: 0.1279249871594301\n",
      "epochs 162/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05078948396085589, val_loss: 0.12532242733141175\n",
      "epochs 163/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050815722514173864, val_loss: 0.12666120201012948\n",
      "epochs 164/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05074866570210383, val_loss: 0.12575132629636857\n",
      "epochs 165/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050719556444680985, val_loss: 0.12632055504541817\n",
      "epochs 166/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.0505831884795133, val_loss: 0.12476847280049697\n",
      "epochs 167/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050674224807018484, val_loss: 0.1255463107655588\n",
      "epochs 168/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05062610230029181, val_loss: 0.12417869302670523\n",
      "epochs 169/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05062225328206464, val_loss: 0.12593529705029344\n",
      "epochs 170/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05059809840778694, val_loss: 0.12630002025495438\n",
      "epochs 171/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05042455197070484, val_loss: 0.123642282435968\n",
      "epochs 172/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05056834814748094, val_loss: 0.12631742419034708\n",
      "epochs 173/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05038674563215471, val_loss: 0.12401026188227762\n",
      "epochs 174/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050473508488294884, val_loss: 0.12558046879631649\n",
      "epochs 175/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05041585313041269, val_loss: 0.12582543129651136\n",
      "epochs 176/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05035678301305498, val_loss: 0.12502493110029414\n",
      "epochs 177/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050301073797415066, val_loss: 0.12474003420922802\n",
      "epochs 178/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05039648060330454, val_loss: 0.12648923541726415\n",
      "epochs 179/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05025224516421189, val_loss: 0.12425149811153663\n",
      "epochs 180/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05036406400670976, val_loss: 0.1263723973841277\n",
      "epochs 181/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05036409882207712, val_loss: 0.12533922332093222\n",
      "epochs 182/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05024404365017458, val_loss: 0.12489656010034701\n",
      "epochs 183/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.0503571044055768, val_loss: 0.1262471081330053\n",
      "epochs 184/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050228627410107925, val_loss: 0.12381478557330815\n",
      "epochs 185/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050321969818783764, val_loss: 0.12542301022121036\n",
      "epochs 186/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050375451517785774, val_loss: 0.1255317669452375\n",
      "epochs 187/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05025110658212208, val_loss: 0.12414545260819271\n",
      "epochs 188/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05038751428946853, val_loss: 0.12420842721864271\n",
      "epochs 189/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05034584223787174, val_loss: 0.12370381933902132\n",
      "epochs 190/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05044082743058234, val_loss: 0.1238619657589071\n",
      "epochs 191/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050391996641539864, val_loss: 0.12020776173125745\n",
      "epochs 192/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05055352572903589, val_loss: 0.12065127355001298\n",
      "epochs 193/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05065857993875757, val_loss: 0.12076092067258691\n",
      "epochs 194/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050547961346851085, val_loss: 0.11771408576169051\n",
      "epochs 195/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050679722938824584, val_loss: 0.11983262534421454\n",
      "epochs 196/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050782701236276345, val_loss: 0.12112866680803967\n",
      "epochs 197/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05072920892479611, val_loss: 0.11846231348075283\n",
      "epochs 198/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05070223989838987, val_loss: 0.11737099972217241\n",
      "epochs 199/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05072462215536722, val_loss: 0.12076604773271053\n",
      "epochs 200/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05088233265933799, val_loss: 0.1227721336536989\n",
      "epochs 201/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05057934535391353, val_loss: 0.11963101475784141\n",
      "epochs 202/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05064272225179054, val_loss: 0.12038596252871987\n",
      "epochs 203/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050745778176703574, val_loss: 0.12250168335352403\n",
      "epochs 204/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050668850253669566, val_loss: 0.1206042287958553\n",
      "epochs 205/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05063828836298651, val_loss: 0.1182268570701126\n",
      "epochs 206/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05070486809069544, val_loss: 0.12150945729970569\n",
      "epochs 207/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05078640190401563, val_loss: 0.12081111157507014\n",
      "epochs 208/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05074095635755378, val_loss: 0.1194785847813667\n",
      "epochs 209/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05084109862829432, val_loss: 0.12204967809763427\n",
      "epochs 210/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05083599007102074, val_loss: 0.11983116098175135\n",
      "epochs 211/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050809241359892454, val_loss: 0.12108817464064082\n",
      "epochs 212/213\n",
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.05075521586623825, val_loss: 0.12023552430845383\n",
      "epochs 213/213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:39:43,454] Trial 0 finished with value: 1.022800977753545 and parameters: {'batch_size': 28, 'epochs': 213, 'hidden_size': 879, 'learning_rate': 0.039528154285592344, 'dropout_prob': 0.12381526884264604, 'weight_decay': 0.014200215743600426, 'gamma': 0.09634732843572635, 'conv_channels': 48, 'kernel_size': 1, 'num_layers': 3}. Best is trial 0 with value: 1.022800977753545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.039528154285592344\n",
      "train_loss: 0.050862682028961034, val_loss: 0.12098121725042195\n",
      "Mean validation loss: 1.022800977753545\n",
      "Fold: 1/4\n",
      "epochs 1/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 308.36090631907183, val_loss: 41.56503831015693\n",
      "epochs 2/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 19.84247510093782, val_loss: 11.198334415753683\n",
      "epochs 3/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 6.888645282498112, val_loss: 0.7513129899485244\n",
      "epochs 4/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 3.282848031708488, val_loss: 0.11103988135841468\n",
      "epochs 5/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 5.580813625344524, val_loss: 0.8536954315172302\n",
      "epochs 6/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 11.191070603551688, val_loss: 0.28433191450312734\n",
      "epochs 7/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 3.5950109086654805, val_loss: 0.145727576634575\n",
      "epochs 8/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 11.749374717473984, val_loss: 0.9822736788127158\n",
      "epochs 9/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 7.170656140203829, val_loss: 0.9392239459686809\n",
      "epochs 10/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 3.8854568390934556, val_loss: 0.10039593426174381\n",
      "epochs 11/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 5.442810114887026, val_loss: 2.4419767691029444\n",
      "epochs 12/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 3.376817500149762, val_loss: 0.38915011048730874\n",
      "epochs 13/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.6841585293964103, val_loss: 1.1663693293101258\n",
      "epochs 14/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 2.4600817020292634, val_loss: 0.11032027973609122\n",
      "epochs 15/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 2.9674129999346204, val_loss: 0.06593470028504574\n",
      "epochs 16/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.6279378169112735, val_loss: 1.5523084907068148\n",
      "epochs 17/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 2.6694737209214106, val_loss: 0.7270181303222975\n",
      "epochs 18/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.4956195061957394, val_loss: 1.218588561647468\n",
      "epochs 19/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 4.030424847646996, val_loss: 0.671417041371266\n",
      "epochs 20/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.8300582910025562, val_loss: 0.6361659602779481\n",
      "epochs 21/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.6734198409098167, val_loss: 0.4404753804620769\n",
      "epochs 22/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.1608386387427647, val_loss: 0.062200435844311466\n",
      "epochs 23/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 2.1217693675447395, val_loss: 4.260958012607363\n",
      "epochs 24/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 4.772375569299415, val_loss: 0.6881371604071723\n",
      "epochs 25/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.680150344967842, val_loss: 0.09151157180248345\n",
      "epochs 26/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 0.9715780657198694, val_loss: 0.26934976565341157\n",
      "epochs 27/235\n",
      "Current Learning Rate: 0.06969557121198462\n",
      "train_loss: 1.9515566814828802, val_loss: 0.05858639501255109\n",
      "epochs 28/235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:39:54,594] Trial 1 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.06081987885516827, val_loss: 0.0798070468428907\n",
      "epochs 2/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.046947594464722055, val_loss: 0.08833234814018295\n",
      "epochs 3/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04271417440570615, val_loss: 0.07185147165404891\n",
      "epochs 4/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04422987847485476, val_loss: 0.08256059615652905\n",
      "epochs 5/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.042217016435676706, val_loss: 0.07254124078058845\n",
      "epochs 6/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04324128493425195, val_loss: 0.07874960193940853\n",
      "epochs 7/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04216987997534926, val_loss: 0.07267608187652917\n",
      "epochs 8/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.07118151031641497, val_loss: 0.08100081859609215\n",
      "epochs 9/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04413658617650745, val_loss: 0.07037073118004224\n",
      "epochs 10/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04854713871005784, val_loss: 0.09147081130625641\n",
      "epochs 11/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.039997586126749717, val_loss: 0.047067779344312534\n",
      "epochs 12/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.030381325033010432, val_loss: 0.03552038229423084\n",
      "epochs 13/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02173385679445885, val_loss: 0.025642289420324635\n",
      "epochs 14/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02607456715432582, val_loss: 0.03391758050161621\n",
      "epochs 15/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02971976283610005, val_loss: 0.024876350428611558\n",
      "epochs 16/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017923359427152685, val_loss: 0.0345426116499665\n",
      "epochs 17/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.025142435320756502, val_loss: 0.028719799666659027\n",
      "epochs 18/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01624999883265614, val_loss: 0.023606055761926353\n",
      "epochs 19/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019450191858966178, val_loss: 0.027389504346097562\n",
      "epochs 20/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01943097028588117, val_loss: 0.02454037863289058\n",
      "epochs 21/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016877546856886947, val_loss: 0.025596081494061056\n",
      "epochs 22/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016825687722302973, val_loss: 0.024341556693090953\n",
      "epochs 23/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018546443294595788, val_loss: 0.024494105310522072\n",
      "epochs 24/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016535778103085857, val_loss: 0.023735246806204993\n",
      "epochs 25/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01488729911269965, val_loss: 0.023970320199926693\n",
      "epochs 26/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015426624324862604, val_loss: 0.024163836786303565\n",
      "epochs 27/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014803413124927492, val_loss: 0.023202057566474348\n",
      "epochs 28/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014522269150431923, val_loss: 0.023276811934339194\n",
      "epochs 29/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01554363749757478, val_loss: 0.02247731609895709\n",
      "epochs 30/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014295967430505832, val_loss: 0.023076620728615025\n",
      "epochs 31/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014153913103250993, val_loss: 0.023907248694538592\n",
      "epochs 32/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015564055380376952, val_loss: 0.023390199816882767\n",
      "epochs 33/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014029879345023935, val_loss: 0.022379872508786827\n",
      "epochs 34/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014348843510055708, val_loss: 0.022384746217640996\n",
      "epochs 35/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016164004878158233, val_loss: 0.02209288437734358\n",
      "epochs 36/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015191183947199196, val_loss: 0.02546453159852212\n",
      "epochs 37/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016852291781644994, val_loss: 0.02497165962025368\n",
      "epochs 38/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016704768621517967, val_loss: 0.02389027272713267\n",
      "epochs 39/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017225999311819947, val_loss: 0.023151045921420317\n",
      "epochs 40/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01460244795911152, val_loss: 0.023748841104760585\n",
      "epochs 41/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018492796833419964, val_loss: 0.02443138747346691\n",
      "epochs 42/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01699465595582431, val_loss: 0.025336069629879096\n",
      "epochs 43/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014658337573227645, val_loss: 0.022692015087361343\n",
      "epochs 44/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014281275591606067, val_loss: 0.022510393556634273\n",
      "epochs 45/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017502082636821324, val_loss: 0.02571104072275274\n",
      "epochs 46/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014936096998604221, val_loss: 0.025805272061714075\n",
      "epochs 47/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01213501114398241, val_loss: 0.021012658159533102\n",
      "epochs 48/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016353845236288314, val_loss: 0.02336976624388222\n",
      "epochs 49/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013360714731755218, val_loss: 0.024764037828087265\n",
      "epochs 50/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013261159963871318, val_loss: 0.024107491123383323\n",
      "epochs 51/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015708271702923986, val_loss: 0.025543458376584265\n",
      "epochs 52/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01328491705418047, val_loss: 0.024611231264619466\n",
      "epochs 53/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013549864406404257, val_loss: 0.02484546883958198\n",
      "epochs 54/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015154895415805556, val_loss: 0.023785752651343098\n",
      "epochs 55/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013482116195114536, val_loss: 0.022787980240536854\n",
      "epochs 56/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015562909297403638, val_loss: 0.025780714459769014\n",
      "epochs 57/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01609078588835136, val_loss: 0.027928591664628282\n",
      "epochs 58/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.011342751230889311, val_loss: 0.024759499695543735\n",
      "epochs 59/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01787154743861821, val_loss: 0.02222042759603937\n",
      "epochs 60/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013388662109651638, val_loss: 0.025378183964979446\n",
      "epochs 61/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.011749047473400694, val_loss: 0.024301365436056384\n",
      "epochs 62/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013014447194075695, val_loss: 0.024224883225946944\n",
      "epochs 63/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012705500897330543, val_loss: 0.023686957342660107\n",
      "epochs 64/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01270846756502848, val_loss: 0.022958686636734962\n",
      "epochs 65/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012429154969544874, val_loss: 0.02350582707246455\n",
      "epochs 66/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013778489093830878, val_loss: 0.024706203240687803\n",
      "epochs 67/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01485661477609365, val_loss: 0.026379568402414105\n",
      "epochs 68/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.011819453787110332, val_loss: 0.02911510455305688\n",
      "epochs 69/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01819930326288428, val_loss: 0.02414198660780029\n",
      "epochs 70/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012931614954679928, val_loss: 0.023058773749653483\n",
      "epochs 71/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01586301242544626, val_loss: 0.025653643523886178\n",
      "epochs 72/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015512477770378744, val_loss: 0.02816863838233985\n",
      "epochs 73/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.011563992645177577, val_loss: 0.025820034829747682\n",
      "epochs 74/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016374066993766638, val_loss: 0.022042486354217142\n",
      "epochs 75/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012460781537272312, val_loss: 0.02317386455656055\n",
      "epochs 76/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012211685183164836, val_loss: 0.024051272560932882\n",
      "epochs 77/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01198438117798004, val_loss: 0.023868386903713044\n",
      "epochs 78/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.011838075016081953, val_loss: 0.024266046752422374\n",
      "epochs 79/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013782007865504257, val_loss: 0.023767101957168454\n",
      "epochs 80/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01299448969506624, val_loss: 0.024236429503717873\n",
      "epochs 81/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014187570501774273, val_loss: 0.02534852274887574\n",
      "epochs 82/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013823762035894173, val_loss: 0.026854599790163372\n",
      "epochs 83/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012030430312734097, val_loss: 0.025171422053164052\n",
      "epochs 84/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01585781599903755, val_loss: 0.022450833621809982\n",
      "epochs 85/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012848951901240205, val_loss: 0.02268256453519118\n",
      "epochs 86/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012905145387372209, val_loss: 0.024108468314807396\n",
      "epochs 87/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012079723986702177, val_loss: 0.02404431610759477\n",
      "epochs 88/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011483671279468885, val_loss: 0.0241128018792046\n",
      "epochs 89/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011939214732221983, val_loss: 0.02440283257844082\n",
      "epochs 90/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010281293640016682, val_loss: 0.02469181874928634\n",
      "epochs 91/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011176987980164725, val_loss: 0.024889412212158075\n",
      "epochs 92/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010965087862806049, val_loss: 0.024946679284362797\n",
      "epochs 93/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011316220698602221, val_loss: 0.024924090628095048\n",
      "epochs 94/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011046148698638987, val_loss: 0.024886707213833386\n",
      "epochs 95/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010540435388166871, val_loss: 0.02481970505363683\n",
      "epochs 96/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010179812963879495, val_loss: 0.024710031829777615\n",
      "epochs 97/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010935127922695957, val_loss: 0.024652550719312986\n",
      "epochs 98/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010517750402127771, val_loss: 0.02463412475117366\n",
      "epochs 99/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011029515676286624, val_loss: 0.024605448998045176\n",
      "epochs 100/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01127582999955242, val_loss: 0.02452905538262308\n",
      "epochs 101/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011388709158550395, val_loss: 0.024476514543696086\n",
      "epochs 102/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01066802436037472, val_loss: 0.024428029346583773\n",
      "epochs 103/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011077183550999811, val_loss: 0.024325921140795497\n",
      "epochs 104/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010996342442619303, val_loss: 0.024245569589563983\n",
      "epochs 105/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010338630236219615, val_loss: 0.024186494233921014\n",
      "epochs 106/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011224343107271663, val_loss: 0.024195304578218686\n",
      "epochs 107/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010969489396773969, val_loss: 0.024211661409127474\n",
      "epochs 108/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011403659776853467, val_loss: 0.024259711084394238\n",
      "epochs 109/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010452349167920786, val_loss: 0.024236587990041396\n",
      "epochs 110/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010947233939077705, val_loss: 0.024169801818061387\n",
      "epochs 111/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01055178340714149, val_loss: 0.024112680673877347\n",
      "epochs 112/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010829131556588604, val_loss: 0.024065792755411368\n",
      "epochs 113/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010343845937987444, val_loss: 0.023967968050177053\n",
      "epochs 114/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010690553919024145, val_loss: 0.023898194151924044\n",
      "epochs 115/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011187105767290902, val_loss: 0.023881449432034667\n",
      "epochs 116/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010150650688619525, val_loss: 0.023807730576663744\n",
      "epochs 117/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01111488335300237, val_loss: 0.0237757317194741\n",
      "epochs 118/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010174267125505678, val_loss: 0.023723938427994855\n",
      "epochs 119/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.00994946806329406, val_loss: 0.023650349690999266\n",
      "epochs 120/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010807095607527083, val_loss: 0.023651048712458253\n",
      "epochs 121/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010507018956424738, val_loss: 0.023721667532906093\n",
      "epochs 122/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011330353603827663, val_loss: 0.023808658013876993\n",
      "epochs 123/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011187679478165659, val_loss: 0.023883833630760718\n",
      "epochs 124/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011274468992336618, val_loss: 0.023920300081954338\n",
      "epochs 125/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011163041491440876, val_loss: 0.02386904866155722\n",
      "epochs 126/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010848067144656347, val_loss: 0.023793795709530566\n",
      "epochs 127/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010672005568199826, val_loss: 0.023744948879337043\n",
      "epochs 128/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010726418329997815, val_loss: 0.0237062646467368\n",
      "epochs 129/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010524518312498307, val_loss: 0.023657938507919123\n",
      "epochs 130/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010632699727365334, val_loss: 0.023603364005086606\n",
      "epochs 131/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010157615248182858, val_loss: 0.02356703612208043\n",
      "epochs 132/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.009770481243591618, val_loss: 0.023516921551264305\n",
      "epochs 133/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010398037212090221, val_loss: 0.023495541678130394\n",
      "epochs 134/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011341167277553015, val_loss: 0.02349397928345651\n",
      "epochs 135/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.010886508224454397, val_loss: 0.023460336057016522\n",
      "epochs 136/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.009973360389402067, val_loss: 0.023400339409514952\n",
      "Fold: 2/4\n",
      "epochs 1/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.2271321304401176, val_loss: 0.16023060727780425\n",
      "epochs 2/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.10878338522053566, val_loss: 0.11262456217567281\n",
      "epochs 3/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0764313992112875, val_loss: 0.08189508410093065\n",
      "epochs 4/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0582878990314418, val_loss: 0.039573832212934375\n",
      "epochs 5/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.059297299085840305, val_loss: 0.05682327991356336\n",
      "epochs 6/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05520153540850582, val_loss: 0.04469414860876794\n",
      "epochs 7/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.057188197056314456, val_loss: 0.05172498714334021\n",
      "epochs 8/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05558602776188679, val_loss: 0.04673678512174067\n",
      "epochs 9/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05620787618250621, val_loss: 0.03609366658686971\n",
      "epochs 10/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.06088032278973885, val_loss: 0.06178228236058203\n",
      "epochs 11/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.053877669084748184, val_loss: 0.03647563337936946\n",
      "epochs 12/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.039911333835873475, val_loss: 0.04582569952860164\n",
      "epochs 13/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05529448264537172, val_loss: 0.04394408141363076\n",
      "epochs 14/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.046782314016363, val_loss: 0.04292069854272995\n",
      "epochs 15/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04211040255096224, val_loss: 0.04747552332507136\n",
      "epochs 16/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.024090224301390763, val_loss: 0.041109064620589685\n",
      "epochs 17/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02162761766575622, val_loss: 0.02990465040885384\n",
      "epochs 18/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0208724989034063, val_loss: 0.02935752881391333\n",
      "epochs 19/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02084698392772461, val_loss: 0.028278553108167317\n",
      "epochs 20/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019035074740208477, val_loss: 0.03204800938106524\n",
      "epochs 21/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019251296794714615, val_loss: 0.02731153845666591\n",
      "epochs 22/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01900630553050346, val_loss: 0.02834739719199004\n",
      "epochs 23/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019055065679519128, val_loss: 0.02748144667485677\n",
      "epochs 24/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018113728927413898, val_loss: 0.03243992705392884\n",
      "epochs 25/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01865778065338317, val_loss: 0.02997963592537821\n",
      "epochs 26/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017716533338426852, val_loss: 0.033923607586378544\n",
      "epochs 27/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017487444953682523, val_loss: 0.03274007317749136\n",
      "epochs 28/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017347747588280105, val_loss: 0.03268986574403243\n",
      "epochs 29/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016676162907431177, val_loss: 0.03377747204164431\n",
      "epochs 30/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015972719564430277, val_loss: 0.03568065476105807\n",
      "epochs 31/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017265359057260125, val_loss: 0.03673542619233356\n",
      "epochs 32/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016190708320613742, val_loss: 0.03545534345175838\n",
      "epochs 33/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016387624949587648, val_loss: 0.048391501306064635\n",
      "epochs 34/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017227507812936825, val_loss: 0.038311213794562114\n",
      "epochs 35/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016832530111531487, val_loss: 0.028142573710485723\n",
      "epochs 36/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017065646053567803, val_loss: 0.05085144682864969\n",
      "epochs 37/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019920571611254236, val_loss: 0.05487451222668622\n",
      "epochs 38/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017656043032830995, val_loss: 0.026303055903149977\n",
      "epochs 39/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019629105536431005, val_loss: 0.03539422168493426\n",
      "epochs 40/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01867537741243184, val_loss: 0.026154814480428792\n",
      "epochs 41/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02359634981365633, val_loss: 0.03262688057212573\n",
      "epochs 42/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02213587310419556, val_loss: 0.028314377821516246\n",
      "epochs 43/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01683378167972141, val_loss: 0.03275727863850383\n",
      "epochs 44/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017305117695488863, val_loss: 0.024878950979554146\n",
      "epochs 45/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01589744054406226, val_loss: 0.0344915409069573\n",
      "epochs 46/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014975715911498776, val_loss: 0.03493470019535986\n",
      "epochs 47/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014714113387055005, val_loss: 0.0292049269664858\n",
      "epochs 48/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014447527976396183, val_loss: 0.031864472774436256\n",
      "epochs 49/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01403736456780246, val_loss: 0.03532667245276065\n",
      "epochs 50/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012850459832137588, val_loss: 0.03595885665143012\n",
      "epochs 51/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013408838033986589, val_loss: 0.03333543566744387\n",
      "epochs 52/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015144725387303711, val_loss: 0.050396969855177706\n",
      "epochs 53/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017603760092579586, val_loss: 0.03281310515497656\n",
      "epochs 54/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015814179718425428, val_loss: 0.02367765012281274\n",
      "epochs 55/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014465478976489976, val_loss: 0.04069915166029306\n",
      "epochs 56/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01548117766139546, val_loss: 0.027559895903323195\n",
      "epochs 57/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013563748076126945, val_loss: 0.03224897616140273\n",
      "epochs 58/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01322067330302498, val_loss: 0.03184053176745591\n",
      "epochs 59/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013272059272922989, val_loss: 0.03279250019356065\n",
      "epochs 60/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012895436438999174, val_loss: 0.03658576338865613\n",
      "epochs 61/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014061885839543754, val_loss: 0.03707636944131486\n",
      "epochs 62/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01666059807757847, val_loss: 0.024095975058217946\n",
      "epochs 63/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016757182877174474, val_loss: 0.047256513505999465\n",
      "epochs 64/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017111457184741826, val_loss: 0.03390896868141782\n",
      "epochs 65/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013779031528867091, val_loss: 0.03196655635838397\n",
      "epochs 66/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014278455716083516, val_loss: 0.026197949990293838\n",
      "epochs 67/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014941598381415976, val_loss: 0.02607794485690344\n",
      "epochs 68/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015299159792010431, val_loss: 0.027231606108721787\n",
      "epochs 69/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015246337654793428, val_loss: 0.035922434092652916\n",
      "epochs 70/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013739064081434023, val_loss: 0.030430804423101816\n",
      "epochs 71/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013987317640351615, val_loss: 0.03731294635670363\n",
      "epochs 72/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01413410360476485, val_loss: 0.03862079680968438\n",
      "epochs 73/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01290091838584178, val_loss: 0.036926342195430256\n",
      "epochs 74/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013901753901909277, val_loss: 0.04713608195035906\n",
      "epochs 75/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01341278259901123, val_loss: 0.052189287896706245\n",
      "epochs 76/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014589470792863588, val_loss: 0.04174980223049513\n",
      "epochs 77/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013403343585431922, val_loss: 0.05918911312700301\n",
      "epochs 78/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.014591189032351529, val_loss: 0.048227946747728206\n",
      "epochs 79/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.017972249860004348, val_loss: 0.035692591812322566\n",
      "epochs 80/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.013688750652363524, val_loss: 0.02943171785571192\n",
      "epochs 81/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012538255250547081, val_loss: 0.028524825104492873\n",
      "epochs 82/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012741906485408199, val_loss: 0.02895840942377011\n",
      "epochs 83/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012718055483505682, val_loss: 0.02949400023357561\n",
      "epochs 84/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01203975952726237, val_loss: 0.029947098358510022\n",
      "epochs 85/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012729207233793137, val_loss: 0.02984618460686761\n",
      "epochs 86/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011947221220772783, val_loss: 0.030127743253211117\n",
      "epochs 87/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011484268178551079, val_loss: 0.030315919853415432\n",
      "epochs 88/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011997720392645095, val_loss: 0.03045980499074277\n",
      "epochs 89/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01219741078373907, val_loss: 0.03030130914329574\n",
      "epochs 90/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012271507453449347, val_loss: 0.030301715688134816\n",
      "epochs 91/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011486586671607155, val_loss: 0.030494730428674503\n",
      "epochs 92/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011699139956531494, val_loss: 0.030921360699923954\n",
      "epochs 93/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012027676166595754, val_loss: 0.030806304569624545\n",
      "epochs 94/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011450991348182369, val_loss: 0.030680498886997358\n",
      "epochs 95/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011359659054842606, val_loss: 0.030838543354346055\n",
      "epochs 96/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011691148272129121, val_loss: 0.03092650642954848\n",
      "epochs 97/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012028962581772011, val_loss: 0.03087970057499155\n",
      "epochs 98/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011748521640482876, val_loss: 0.03090342333820849\n",
      "epochs 99/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011826595251612296, val_loss: 0.030616774444853136\n",
      "epochs 100/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01152122897824652, val_loss: 0.030907851454685442\n",
      "epochs 101/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012183681201551937, val_loss: 0.031207468223834037\n",
      "epochs 102/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.0108429755534787, val_loss: 0.031157735099946474\n",
      "epochs 103/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011816465799239706, val_loss: 0.03138112637740657\n",
      "epochs 104/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011288346299099632, val_loss: 0.031433194983845975\n",
      "epochs 105/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012065297605995848, val_loss: 0.031053720454211644\n",
      "epochs 106/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011730495945947175, val_loss: 0.030834723860203264\n",
      "epochs 107/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011671635022611116, val_loss: 0.03092987884519971\n",
      "epochs 108/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011767249475707542, val_loss: 0.031167045290203532\n",
      "epochs 109/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011362188612110913, val_loss: 0.03130156507844125\n",
      "epochs 110/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011716212625658416, val_loss: 0.0314393904708494\n",
      "epochs 111/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011139599853238367, val_loss: 0.0316658341721955\n",
      "epochs 112/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011449488934077736, val_loss: 0.03134827567211889\n",
      "epochs 113/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012296988635272201, val_loss: 0.031206684958912472\n",
      "epochs 114/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011392520960523851, val_loss: 0.031035158845851483\n",
      "epochs 115/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01228086810550411, val_loss: 0.03077759296118327\n",
      "epochs 116/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011771609106627121, val_loss: 0.030706550072055607\n",
      "epochs 117/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011721450452804911, val_loss: 0.030941742234265095\n",
      "epochs 118/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011315936967514938, val_loss: 0.03095516583885506\n",
      "epochs 119/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011946774043651574, val_loss: 0.03130912768230903\n",
      "epochs 120/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011149796698225179, val_loss: 0.031054154391313205\n",
      "epochs 121/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011578181708938684, val_loss: 0.0311604001520512\n",
      "epochs 122/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011355774382267285, val_loss: 0.03134442709496296\n",
      "epochs 123/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.012051452693951971, val_loss: 0.0313870003841051\n",
      "epochs 124/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011391601276247658, val_loss: 0.03149759877164292\n",
      "epochs 125/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011065245025006502, val_loss: 0.031237709012202686\n",
      "epochs 126/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.01147645646900249, val_loss: 0.0310431334332356\n",
      "epochs 127/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011201413230823905, val_loss: 0.03127665745281168\n",
      "epochs 128/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011613677057472092, val_loss: 0.0317065786350415\n",
      "epochs 129/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.011274709738144238, val_loss: 0.03173872072713695\n",
      "epochs 130/136\n",
      "Current Learning Rate: 6.133238136111853e-07\n",
      "train_loss: 0.011032347727989502, val_loss: 0.031612177208848555\n",
      "epochs 131/136\n",
      "Current Learning Rate: 6.133238136111853e-07\n",
      "train_loss: 0.010981382222208022, val_loss: 0.03160707582396248\n",
      "epochs 132/136\n",
      "Current Learning Rate: 6.133238136111853e-07\n",
      "train_loss: 0.010954022079844165, val_loss: 0.031603193094472064\n",
      "epochs 133/136\n",
      "Current Learning Rate: 6.133238136111853e-07\n",
      "train_loss: 0.011507045432356082, val_loss: 0.031599674920370385\n",
      "epochs 134/136\n",
      "Current Learning Rate: 6.133238136111853e-07\n",
      "train_loss: 0.011282708710361342, val_loss: 0.031600510163924705\n",
      "epochs 135/136\n",
      "Current Learning Rate: 6.133238136111853e-07\n",
      "train_loss: 0.010937599725029603, val_loss: 0.03160514262749024\n",
      "epochs 136/136\n",
      "Current Learning Rate: 6.133238136111853e-07\n",
      "train_loss: 0.01144929422729614, val_loss: 0.03160680162606392\n",
      "Fold: 3/4\n",
      "epochs 1/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.11152027399612614, val_loss: 0.05412167438690732\n",
      "epochs 2/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0551025921329396, val_loss: 0.07173867308301851\n",
      "epochs 3/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05470709097974094, val_loss: 0.09346378286298002\n",
      "epochs 4/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05460478379506112, val_loss: 0.11134726153977681\n",
      "epochs 5/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05210802721342555, val_loss: 0.09213209197494304\n",
      "epochs 6/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04627402155964785, val_loss: 0.053109138749682136\n",
      "epochs 7/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04790891659515047, val_loss: 0.038422974166072286\n",
      "epochs 8/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04441003802370418, val_loss: 0.048522104359613474\n",
      "epochs 9/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.03571431174368402, val_loss: 0.04834583745954054\n",
      "epochs 10/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02797119552988191, val_loss: 0.0441660572549962\n",
      "epochs 11/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.025876197392655606, val_loss: 0.040282466587086674\n",
      "epochs 12/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.024762366951535237, val_loss: 0.04112815391029775\n",
      "epochs 13/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.023685055120164, val_loss: 0.03732495840166747\n",
      "epochs 14/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.023081020952236873, val_loss: 0.04629141483585247\n",
      "epochs 15/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02129129395621863, val_loss: 0.03499438090269299\n",
      "epochs 16/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0243559000959022, val_loss: 0.05522047428530641\n",
      "epochs 17/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01878200619507374, val_loss: 0.035765350970905274\n",
      "epochs 18/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02101691867897494, val_loss: 0.048155996445934254\n",
      "epochs 19/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018722384676544204, val_loss: 0.041881362142804496\n",
      "epochs 20/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01926869934965155, val_loss: 0.03410661014560093\n",
      "epochs 21/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019940746979741953, val_loss: 0.044887923530445226\n",
      "epochs 22/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017757894165877162, val_loss: 0.03888962862321124\n",
      "epochs 23/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018813490440039955, val_loss: 0.03291112890939177\n",
      "epochs 24/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02201466157135588, val_loss: 0.047588541794539196\n",
      "epochs 25/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018384017240698738, val_loss: 0.04337628460618564\n",
      "epochs 26/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018116214548318105, val_loss: 0.03368084398809717\n",
      "epochs 27/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02015547726424066, val_loss: 0.037355517025894694\n",
      "epochs 28/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018447118268037837, val_loss: 0.04340486110838052\n",
      "epochs 29/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01648593003887476, val_loss: 0.03275456708636031\n",
      "epochs 30/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01934425924461979, val_loss: 0.03595672527929613\n",
      "epochs 31/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019017889119834167, val_loss: 0.04518983965802666\n",
      "epochs 32/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01733903268950037, val_loss: 0.03617625516360729\n",
      "epochs 33/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018019709197148955, val_loss: 0.036132885430965364\n",
      "epochs 34/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020172211638347096, val_loss: 0.037700993608773894\n",
      "epochs 35/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016776645294517094, val_loss: 0.03518013948754136\n",
      "epochs 36/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016264739077693473, val_loss: 0.044193323572269745\n",
      "epochs 37/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016347942081205317, val_loss: 0.04760069057859558\n",
      "epochs 38/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01567388385430806, val_loss: 0.03505839167741619\n",
      "epochs 39/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018531147104596007, val_loss: 0.034858728546371114\n",
      "epochs 40/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015808506676909962, val_loss: 0.05313798593447751\n",
      "epochs 41/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01573610142301653, val_loss: 0.03411610828657609\n",
      "epochs 42/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016888146249887843, val_loss: 0.037930040005044754\n",
      "epochs 43/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01662640281619481, val_loss: 0.03670152834173576\n",
      "epochs 44/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01882059712474591, val_loss: 0.0344203430124455\n",
      "epochs 45/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01656017101507772, val_loss: 0.06591866784664388\n",
      "epochs 46/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019261619171708315, val_loss: 0.03686199322975073\n",
      "epochs 47/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019330886965847494, val_loss: 0.034811307013215914\n",
      "epochs 48/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018326892656199405, val_loss: 0.04277379134631095\n",
      "epochs 49/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017957208332473436, val_loss: 0.05806293299732109\n",
      "epochs 50/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.021285455607102013, val_loss: 0.03751744750600968\n",
      "epochs 51/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016276321834510905, val_loss: 0.04222983808237283\n",
      "epochs 52/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014633081376230643, val_loss: 0.035308565397927955\n",
      "epochs 53/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014095749161321164, val_loss: 0.03702205738712413\n",
      "epochs 54/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014157735282716192, val_loss: 0.03700733438341154\n",
      "epochs 55/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014115236586040277, val_loss: 0.03578208187860178\n",
      "epochs 56/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014222535071894526, val_loss: 0.03588857799412734\n",
      "epochs 57/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01605724163689754, val_loss: 0.06931504628785963\n",
      "epochs 58/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018064139004836985, val_loss: 0.046557298133848235\n",
      "epochs 59/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02469421743147972, val_loss: 0.035705934453289956\n",
      "epochs 60/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02146512235264742, val_loss: 0.0390285540456211\n",
      "epochs 61/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018112937526714147, val_loss: 0.04834220737555168\n",
      "epochs 62/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014280931094235935, val_loss: 0.035540592708356494\n",
      "epochs 63/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014420528198379655, val_loss: 0.040498949404637746\n",
      "epochs 64/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013775156469538486, val_loss: 0.03463327023362379\n",
      "epochs 65/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013784806649373085, val_loss: 0.0368286887830133\n",
      "epochs 66/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013704292727691799, val_loss: 0.03737656447780561\n",
      "epochs 67/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013696664097046273, val_loss: 0.06432408395307397\n",
      "epochs 68/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015607224363419745, val_loss: 0.03848318015338413\n",
      "epochs 69/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013554073278763458, val_loss: 0.039476580485319976\n",
      "epochs 70/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01719768522206671, val_loss: 0.0378769177582904\n",
      "epochs 71/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01503323156380865, val_loss: 0.054153251458046725\n",
      "epochs 72/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013624467314968322, val_loss: 0.038931865547460295\n",
      "epochs 73/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016148169578863652, val_loss: 0.053345066548192537\n",
      "epochs 74/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014289715947485586, val_loss: 0.04070321481817195\n",
      "epochs 75/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01333349979666556, val_loss: 0.04998522253380441\n",
      "epochs 76/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012827249391889775, val_loss: 0.03772710728824475\n",
      "epochs 77/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014111595118366595, val_loss: 0.036933970556775525\n",
      "epochs 78/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015036679055530256, val_loss: 0.03489381524074512\n",
      "epochs 79/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01478591568072407, val_loss: 0.048746373035505206\n",
      "epochs 80/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013339974653021789, val_loss: 0.03876832746350879\n",
      "epochs 81/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013136264752383907, val_loss: 0.04092043852304212\n",
      "epochs 82/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.011915617917066463, val_loss: 0.039860087707527074\n",
      "epochs 83/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01333429096309744, val_loss: 0.040384662586398515\n",
      "epochs 84/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013653051777150666, val_loss: 0.04036421975130603\n",
      "epochs 85/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013616562533525773, val_loss: 0.08975951626457067\n",
      "epochs 86/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01589314354278929, val_loss: 0.04247775786254553\n",
      "epochs 87/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013099909267807954, val_loss: 0.040093772656594716\n",
      "epochs 88/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01488630409585312, val_loss: 0.036140982948078694\n",
      "epochs 89/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.015066352205776415, val_loss: 0.038662720788225774\n",
      "epochs 90/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014859836410593104, val_loss: 0.05454784951345775\n",
      "epochs 91/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01296606911331944, val_loss: 0.04118620572222022\n",
      "epochs 92/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012753074766671355, val_loss: 0.051859131842219436\n",
      "epochs 93/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.012706425465660653, val_loss: 0.041111519610341124\n",
      "epochs 94/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.013248259283567744, val_loss: 0.046477240411655255\n",
      "epochs 95/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01398884196964058, val_loss: 0.05049004052125383\n",
      "epochs 96/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.014701504193470572, val_loss: 0.04674573035259123\n",
      "epochs 97/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017774656913703148, val_loss: 0.040309346403874874\n",
      "epochs 98/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05595326346753418, val_loss: 0.07220806024593508\n",
      "epochs 99/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.11759465052401302, val_loss: 0.12161666615187035\n",
      "epochs 100/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.06059375973878267, val_loss: 0.07316604173684027\n",
      "epochs 101/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05671344265157794, val_loss: 0.12732173648611125\n",
      "epochs 102/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0544961400554274, val_loss: 0.14135452235576748\n",
      "epochs 103/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05368729655912583, val_loss: 0.11652826682903752\n",
      "epochs 104/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0534925999229107, val_loss: 0.12580730741966464\n",
      "epochs 105/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05272188461328178, val_loss: 0.13322259152644417\n",
      "epochs 106/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05297027340296794, val_loss: 0.1334739613003977\n",
      "epochs 107/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05298096573923105, val_loss: 0.1286668102669258\n",
      "epochs 108/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05286656693198982, val_loss: 0.13364139412361611\n",
      "epochs 109/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05639135751033915, val_loss: 0.12605834963324014\n",
      "epochs 110/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.052329138924517196, val_loss: 0.1337156172537814\n",
      "epochs 111/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05020342795983141, val_loss: 0.13305515406278168\n",
      "epochs 112/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05030416503525627, val_loss: 0.13232305838836408\n",
      "epochs 113/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.050582938002606416, val_loss: 0.13166440984140435\n",
      "epochs 114/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.049642222824242015, val_loss: 0.13106417750855648\n",
      "epochs 115/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04947725706158873, val_loss: 0.13049859641452916\n",
      "epochs 116/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05042910287559124, val_loss: 0.1300619550489096\n",
      "epochs 117/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04967253816452016, val_loss: 0.12960923682274167\n",
      "epochs 118/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04917638190952992, val_loss: 0.12910477040633042\n",
      "epochs 119/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.049850546990601735, val_loss: 0.12870546757201534\n",
      "epochs 120/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04995532948408782, val_loss: 0.12831735008512624\n",
      "epochs 121/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04993630128763156, val_loss: 0.12795255742579079\n",
      "epochs 122/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05005941142034889, val_loss: 0.12768520135432482\n",
      "epochs 123/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04965336606435386, val_loss: 0.12739328696221086\n",
      "epochs 124/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04993632758312203, val_loss: 0.12711428801978603\n",
      "epochs 125/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05000950632053485, val_loss: 0.1268605629148725\n",
      "epochs 126/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05031004183017361, val_loss: 0.12661080601376146\n",
      "epochs 127/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.050046243424596334, val_loss: 0.1264128590822414\n",
      "epochs 128/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05018150421825272, val_loss: 0.12620334720284315\n",
      "epochs 129/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04999255241084568, val_loss: 0.1260224690356861\n",
      "epochs 130/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04981772561937019, val_loss: 0.12584137676781715\n",
      "epochs 131/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.049174892960438206, val_loss: 0.12570158484838126\n",
      "epochs 132/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04954228190314255, val_loss: 0.12554987478829893\n",
      "epochs 133/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04943444301017824, val_loss: 0.12534276472918768\n",
      "epochs 134/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.05016882192446954, val_loss: 0.1252430726340713\n",
      "epochs 135/136\n",
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.04969544329732242, val_loss: 0.1251524565442297\n",
      "epochs 136/136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:44:46,414] Trial 2 finished with value: 0.04417854509752057 and parameters: {'batch_size': 29, 'epochs': 136, 'hidden_size': 401, 'learning_rate': 0.002173913346612086, 'dropout_prob': 0.13670260827829842, 'weight_decay': 5.971344548682018e-05, 'gamma': 0.0167966935710767, 'conv_channels': 80, 'kernel_size': 1, 'num_layers': 5}. Best is trial 2 with value: 0.04417854509752057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 3.6514556333117056e-05\n",
      "train_loss: 0.0502392677044105, val_loss: 0.1250701348552765\n",
      "Mean validation loss: 0.04417854509752057\n",
      "Fold: 1/4\n",
      "epochs 1/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 21.397802938120783, val_loss: 0.4542963970969949\n",
      "epochs 2/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.3550400884339103, val_loss: 0.29928921348053134\n",
      "epochs 3/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.2901858663255418, val_loss: 0.5139006718786227\n",
      "epochs 4/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.19647705727429302, val_loss: 0.15799155962789277\n",
      "epochs 5/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.2402548094590505, val_loss: 0.19883928827066988\n",
      "epochs 6/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.1500259266683349, val_loss: 0.05657283033684103\n",
      "epochs 7/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.1542217506578675, val_loss: 0.1015887735823829\n",
      "epochs 8/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.13425894095389931, val_loss: 0.11303295807051149\n",
      "epochs 9/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.12363177986332664, val_loss: 0.07368374604912889\n",
      "epochs 10/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.18837461813732428, val_loss: 0.20151032176272565\n",
      "epochs 11/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.1122400954503704, val_loss: 0.0562061338698388\n",
      "epochs 12/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.11190444120654354, val_loss: 0.056164637870905504\n",
      "epochs 13/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.10521218021986661, val_loss: 0.1106640197083632\n",
      "epochs 14/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.0849053173981331, val_loss: 0.05917162929997883\n",
      "epochs 15/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.10101539903768787, val_loss: 0.06299101426945224\n",
      "epochs 16/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.11888456599855865, val_loss: 0.13196443466424373\n",
      "epochs 17/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.0790156407488717, val_loss: 0.06239466552506201\n",
      "epochs 18/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.07071171773390637, val_loss: 0.061826408894072905\n",
      "epochs 19/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.07576930902346417, val_loss: 0.06998086457962119\n",
      "epochs 20/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.08982090438129725, val_loss: 0.08588394445622624\n",
      "epochs 21/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.07636102709781241, val_loss: 0.08904039751885445\n",
      "epochs 22/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.0697720419201586, val_loss: 0.06086461841672038\n",
      "epochs 23/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.07586379849386436, val_loss: 0.059378697921703055\n",
      "epochs 24/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.09731827107154661, val_loss: 0.08158468224670894\n",
      "epochs 25/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.07655890451537238, val_loss: 0.08679333560091133\n",
      "epochs 26/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.06025943308378811, val_loss: 0.05927567677443019\n",
      "epochs 27/317\n",
      "Current Learning Rate: 0.0284196024770332\n",
      "train_loss: 0.06967265596958222, val_loss: 0.07495898063967212\n",
      "epochs 28/317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:44:54,962] Trial 3 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 63.068703890398695, val_loss: 16.597323396139675\n",
      "epochs 2/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 8.112313995206797, val_loss: 7.553427598154586\n",
      "epochs 3/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 1.5312517837241844, val_loss: 0.06317150770337321\n",
      "epochs 4/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.1639113853927012, val_loss: 0.09993668089171276\n",
      "epochs 5/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.25094057950708604, val_loss: 0.07219597061420144\n",
      "epochs 6/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.13653203231041078, val_loss: 0.23504702413790962\n",
      "epochs 7/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.15894926691220868, val_loss: 0.10401731048154438\n",
      "epochs 8/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.10601125360914955, val_loss: 0.1310959158856551\n",
      "epochs 9/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.13936531598921176, val_loss: 0.058076714172784705\n",
      "epochs 10/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.1215274825140282, val_loss: 0.11425100834700667\n",
      "epochs 11/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.1341565841877902, val_loss: 0.08875220290954328\n",
      "epochs 12/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.0953840120109143, val_loss: 0.0631139897692871\n",
      "epochs 13/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.12234403427552294, val_loss: 0.06652037888286738\n",
      "epochs 14/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.1059506158861849, val_loss: 0.08813331700892174\n",
      "epochs 15/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.10164603142550697, val_loss: 0.10463414100377122\n",
      "epochs 16/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.1015469743007863, val_loss: 0.08577305790095124\n",
      "epochs 17/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.09480197851856549, val_loss: 0.06919087250086402\n",
      "epochs 18/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.1182584719111522, val_loss: 0.06363733539991598\n",
      "epochs 19/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.10313540359062177, val_loss: 0.06528537778649479\n",
      "epochs 20/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.08499500300321314, val_loss: 0.070485892763827\n",
      "epochs 21/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.09001493798913779, val_loss: 0.07842067448816831\n",
      "epochs 22/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.09172425826114637, val_loss: 0.08929599366254276\n",
      "epochs 23/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.07198790798860567, val_loss: 0.08808946405871312\n",
      "epochs 24/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.07056522421124908, val_loss: 0.06927873352591025\n",
      "epochs 25/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.06585928742532376, val_loss: 0.06537363065257927\n",
      "epochs 26/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.07840433475319986, val_loss: 0.07017289277266375\n",
      "epochs 27/251\n",
      "Current Learning Rate: 0.04483157872663286\n",
      "train_loss: 0.06601608213451174, val_loss: 0.06471882968617138\n",
      "epochs 28/251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:44:59,185] Trial 4 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 3.6192411391271486, val_loss: 1.397625824507688\n",
      "epochs 2/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.3935157307596118, val_loss: 0.39001832405726117\n",
      "epochs 3/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.23473966332083498, val_loss: 0.20468355178147452\n",
      "epochs 4/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.20588530755291382, val_loss: 16.03465868367089\n",
      "epochs 5/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 3.453149989126595, val_loss: 0.10571800304953488\n",
      "epochs 6/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 1.0146430956406727, val_loss: 0.5474915936582774\n",
      "epochs 7/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.8999446178958924, val_loss: 0.8601160429178789\n",
      "epochs 8/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.9945667851371346, val_loss: 0.2156338790066204\n",
      "epochs 9/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.11674266056744037, val_loss: 0.1887160704484106\n",
      "epochs 10/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.24865726810983485, val_loss: 0.12575978693429432\n",
      "epochs 11/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.15345680223846878, val_loss: 0.18927538824068002\n",
      "epochs 12/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.1260715230471558, val_loss: 0.10725896268639998\n",
      "epochs 13/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.05008586421953859, val_loss: 0.06127542029490643\n",
      "epochs 14/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.06634332584562125, val_loss: 0.06740329893202418\n",
      "epochs 15/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.06535290009169667, val_loss: 0.09008541435792318\n",
      "epochs 16/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.0619678387797817, val_loss: 0.09972694137347086\n",
      "epochs 17/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.05154358490404708, val_loss: 0.0726081433175649\n",
      "epochs 18/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.04819113745664557, val_loss: 0.06834493913710402\n",
      "epochs 19/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.054563136926541723, val_loss: 0.07502661494234214\n",
      "epochs 20/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.046533946310066514, val_loss: 0.07724663855737567\n",
      "epochs 21/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.04660272667253459, val_loss: 0.07217728929309589\n",
      "epochs 22/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.04237348996792679, val_loss: 0.07732155535389514\n",
      "epochs 23/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.04826972076531361, val_loss: 0.07674490386812573\n",
      "epochs 24/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.04845902259909996, val_loss: 0.06900310842967075\n",
      "epochs 25/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.04913941788038722, val_loss: 0.0656826207058556\n",
      "epochs 26/369\n",
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.0472508288781952, val_loss: 0.07019501760886568\n",
      "epochs 27/369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:45:01,995] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.030320398580014105\n",
      "train_loss: 0.04039623495191336, val_loss: 0.06427806204414487\n",
      "epochs 28/369\n",
      "Fold: 1/4\n",
      "epochs 1/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 47.50385922272862, val_loss: 9.420818070570627\n",
      "epochs 2/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 8.263472613637093, val_loss: 9.99511346552107\n",
      "epochs 3/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 5.001726445155563, val_loss: 2.2561926543712616\n",
      "epochs 4/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 3.1041665816748583, val_loss: 1.1236133389174938\n",
      "epochs 5/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.780093822973194, val_loss: 2.64290804333157\n",
      "epochs 6/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 2.1142347704067275, val_loss: 1.4235168248414993\n",
      "epochs 7/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 1.4994538293944464, val_loss: 0.45379904771430624\n",
      "epochs 8/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.5117420899095358, val_loss: 0.6258883927431371\n",
      "epochs 9/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.6508006964016844, val_loss: 0.294676223782719\n",
      "epochs 10/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.44429881608596555, val_loss: 0.27904379938263446\n",
      "epochs 11/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.30375341132835104, val_loss: 0.1301824301764201\n",
      "epochs 12/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.30786644915739697, val_loss: 0.16294210938212927\n",
      "epochs 13/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.33547944879090347, val_loss: 0.09778152475498952\n",
      "epochs 14/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.2321992072932146, val_loss: 0.05579856117320661\n",
      "epochs 15/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.21730361996149575, val_loss: 0.12263122845130663\n",
      "epochs 16/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.23898673016164038, val_loss: 0.06056270858971402\n",
      "epochs 17/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.19295817613601685, val_loss: 0.08995293419421715\n",
      "epochs 18/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.19495505611929628, val_loss: 0.062352057168027386\n",
      "epochs 19/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.2172827641307204, val_loss: 0.0664422213796772\n",
      "epochs 20/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.17866377449697918, val_loss: 0.1018005204201068\n",
      "epochs 21/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.18089462775323126, val_loss: 0.05640701224183431\n",
      "epochs 22/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.18416471224002265, val_loss: 0.09322442771089198\n",
      "epochs 23/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.16358432763566574, val_loss: 0.057362833418462995\n",
      "epochs 24/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.1462748252131321, val_loss: 0.06810419251188352\n",
      "epochs 25/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.17916093456248441, val_loss: 0.07723480267708914\n",
      "epochs 26/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.1485017582912136, val_loss: 0.0790665102411165\n",
      "epochs 27/247\n",
      "Current Learning Rate: 0.06825680096927085\n",
      "train_loss: 0.14362228641079533, val_loss: 0.07999129532254301\n",
      "epochs 28/247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:45:07,806] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 678.6878705345163, val_loss: 4.716420878966649\n",
      "epochs 2/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 364.65172825919257, val_loss: 396.00599077012805\n",
      "epochs 3/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 163.26011650236669, val_loss: 185.74582778082953\n",
      "epochs 4/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 135.7876079524005, val_loss: 0.49834129783428377\n",
      "epochs 5/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 14.210306745988351, val_loss: 2.0806291931205325\n",
      "epochs 6/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 1.8533485531806946, val_loss: 225.3659672109255\n",
      "epochs 7/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 70.12205852733717, val_loss: 0.5317014674138691\n",
      "epochs 8/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 1.3256084273258846, val_loss: 0.07627783856716835\n",
      "epochs 9/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.7365273430391595, val_loss: 1.9469887630807028\n",
      "epochs 10/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 6.958716425078887, val_loss: 393.5857302347819\n",
      "epochs 11/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 127.52259903759868, val_loss: 53.36990552478366\n",
      "epochs 12/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 19.759651719420045, val_loss: 12.084558579656813\n",
      "epochs 13/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 5.878562853292182, val_loss: 1.8018650859594345\n",
      "epochs 14/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 5.3691500199061855, val_loss: 0.07514702605112689\n",
      "epochs 15/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.4960152760699943, val_loss: 0.35245239728182143\n",
      "epochs 16/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 3.4019505503001035, val_loss: 0.06229603127172191\n",
      "epochs 17/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.1833586516203702, val_loss: 1.275201153424051\n",
      "epochs 18/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 3.0894007086753845, val_loss: 0.7356196271203872\n",
      "epochs 19/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 9.75236706049354, val_loss: 56.12900813420614\n",
      "epochs 20/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 26.54365947290703, val_loss: 3.0456339567899704\n",
      "epochs 21/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 4.797953578057112, val_loss: 0.40335590878708494\n",
      "epochs 22/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.5541269144526235, val_loss: 0.06073795563133899\n",
      "epochs 23/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.0996445776135833, val_loss: 1.3969386311040983\n",
      "epochs 24/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 3.5748142059202546, val_loss: 0.056061515516679115\n",
      "epochs 25/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.683313634660509, val_loss: 0.15281572491383283\n",
      "epochs 26/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.0927548430584095, val_loss: 1.0120262176626258\n",
      "epochs 27/318\n",
      "Current Learning Rate: 0.06584471565984121\n",
      "train_loss: 2.247238634913056, val_loss: 0.08245710279960702\n",
      "epochs 28/318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:46:15,794] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 15.017316409283215, val_loss: 0.08629343352125336\n",
      "epochs 2/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.5021017872624927, val_loss: 0.0924113492360953\n",
      "epochs 3/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.30767569608158535, val_loss: 0.06021975995018794\n",
      "epochs 4/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.3371216853459676, val_loss: 0.05790865991730243\n",
      "epochs 5/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.42348116636276245, val_loss: 0.1290855452340717\n",
      "epochs 6/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.4323774923880895, val_loss: 0.09936383925054947\n",
      "epochs 7/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.44612043102582294, val_loss: 0.05646891952427621\n",
      "epochs 8/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.2572765482796563, val_loss: 0.05868433515085295\n",
      "epochs 9/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.25279281702306533, val_loss: 0.06229212457886509\n",
      "epochs 10/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.26842617988586426, val_loss: 0.16614787864253028\n",
      "epochs 11/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.18370873646603691, val_loss: 0.05993773777542325\n",
      "epochs 12/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.21556053186456361, val_loss: 0.10896023373788921\n",
      "epochs 13/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.18768037358919779, val_loss: 0.16393106355260695\n",
      "epochs 14/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.29273763216204113, val_loss: 0.09194755291294616\n",
      "epochs 15/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.18584494748049313, val_loss: 0.20410420200076057\n",
      "epochs 16/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.15652306108838981, val_loss: 0.1837171323358133\n",
      "epochs 17/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.1402849182486534, val_loss: 0.058858426163270555\n",
      "epochs 18/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.15552501711580488, val_loss: 0.062436502001623415\n",
      "epochs 19/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.19749251835876042, val_loss: 0.0657721216975915\n",
      "epochs 20/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.14798891958263186, val_loss: 0.12241492383650944\n",
      "epochs 21/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.09970108792185783, val_loss: 0.056897148282991514\n",
      "epochs 22/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.10253706781400575, val_loss: 0.07964312088248941\n",
      "epochs 23/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.10106411369310485, val_loss: 0.06666446043123465\n",
      "epochs 24/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05830074101686478, val_loss: 0.05735841860102179\n",
      "epochs 25/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.12855064061780772, val_loss: 0.12285146676569841\n",
      "epochs 26/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.07607961280478372, val_loss: 0.06581440181536083\n",
      "epochs 27/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.08699455650316344, val_loss: 0.08032747055727264\n",
      "epochs 28/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.09844614110059208, val_loss: 0.05593282979033473\n",
      "epochs 29/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.06782816350460052, val_loss: 0.05656028759727229\n",
      "epochs 30/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.11346042859885427, val_loss: 0.07194504491215209\n",
      "epochs 31/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.09684458499153455, val_loss: 0.0858263435269085\n",
      "epochs 32/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.0504462905228138, val_loss: 0.062464586889998626\n",
      "epochs 33/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.06450867735677296, val_loss: 0.06375681837784618\n",
      "epochs 34/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.06731153238150808, val_loss: 0.20106537586737736\n",
      "epochs 35/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.13761678420835072, val_loss: 0.08453792170863987\n",
      "epochs 36/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05212839113341437, val_loss: 0.06578815594265405\n",
      "epochs 37/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05642460286617279, val_loss: 0.055938731206374034\n",
      "epochs 38/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05332453238467375, val_loss: 0.05599095402026756\n",
      "epochs 39/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.08613463035888141, val_loss: 0.06861749311873508\n",
      "epochs 40/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.06498088128864765, val_loss: 0.055775438728030875\n",
      "epochs 41/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.06383629060453838, val_loss: 0.056611111471688166\n",
      "epochs 42/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05185665811101595, val_loss: 0.055783895830649674\n",
      "epochs 43/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05418287952327066, val_loss: 0.057041125789207094\n",
      "epochs 44/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.050649227367507085, val_loss: 0.0558253012649301\n",
      "epochs 45/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.06831365989314185, val_loss: 0.08582216270992325\n",
      "epochs 46/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.07498739494217767, val_loss: 0.07372114441952565\n",
      "epochs 47/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.08037456290589438, val_loss: 0.06737332873146645\n",
      "epochs 48/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05866261995914909, val_loss: 0.05800944225241741\n",
      "epochs 49/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05706402762896485, val_loss: 0.05686339114294646\n",
      "epochs 50/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05123505782749918, val_loss: 0.05781498954699297\n",
      "epochs 51/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.04663820916579829, val_loss: 0.05586022606520499\n",
      "epochs 52/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.049997810274362564, val_loss: 0.056428301251596875\n",
      "epochs 53/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.054306208052568965, val_loss: 0.060287800769502714\n",
      "epochs 54/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.050314218012823, val_loss: 0.05621377768370116\n",
      "epochs 55/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.052738045135305986, val_loss: 0.05646109573475163\n",
      "epochs 56/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.04658945629166232, val_loss: 0.05585422974480833\n",
      "epochs 57/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05169295002188948, val_loss: 0.0559319334926032\n",
      "epochs 58/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.07367381908827358, val_loss: 0.05994384615203469\n",
      "epochs 59/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05845676476342811, val_loss: 0.06019893300415586\n",
      "epochs 60/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.0525040150516563, val_loss: 0.06182281710789539\n",
      "epochs 61/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05637633386585447, val_loss: 0.05686586434765357\n",
      "epochs 62/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.09586097672581673, val_loss: 0.057464353844504025\n",
      "epochs 63/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.07026123338275486, val_loss: 0.05637662536173593\n",
      "epochs 64/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05612782285445266, val_loss: 0.05789593228999163\n",
      "epochs 65/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.04746576357218954, val_loss: 0.05791487909543017\n",
      "epochs 66/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.054953750429881945, val_loss: 0.055780572260523006\n",
      "epochs 67/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05236674928002887, val_loss: 0.05586869363065085\n",
      "epochs 68/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05262552615669039, val_loss: 0.05751125044157056\n",
      "epochs 69/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05015269501341714, val_loss: 0.05862341044606486\n",
      "epochs 70/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05748287753926383, val_loss: 0.056780573664582334\n",
      "epochs 71/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.049830489998890296, val_loss: 0.05636818458636602\n",
      "epochs 72/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05289639284213384, val_loss: 0.05643594431118496\n",
      "epochs 73/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.05109272483322355, val_loss: 0.0557642529488981\n",
      "epochs 74/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.17767995554539892, val_loss: 0.12382197682422379\n",
      "epochs 75/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.14428796991705894, val_loss: 0.14425985136121097\n",
      "epochs 76/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.1294351155973143, val_loss: 0.0932931974772752\n",
      "epochs 77/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.09737685095104906, val_loss: 0.09502510242034784\n",
      "epochs 78/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.09495371207594872, val_loss: 0.07658281370014366\n",
      "epochs 79/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.056102815394600235, val_loss: 0.07838796270597312\n",
      "epochs 80/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.07277972913450664, val_loss: 0.06516398695465694\n",
      "epochs 81/210\n",
      "Current Learning Rate: 0.05999355983700259\n",
      "train_loss: 0.04659508582618502, val_loss: 0.055991236194838874\n",
      "epochs 82/210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:46:27,230] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 179.97094295546412, val_loss: 2.558285484711329\n",
      "epochs 2/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 8.742291159927845, val_loss: 5.5817331042554645\n",
      "epochs 3/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 6.611200422048569, val_loss: 0.5509087259156836\n",
      "epochs 4/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 2.4036537408828735, val_loss: 2.0598785744773016\n",
      "epochs 5/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 2.201688691973686, val_loss: 0.9525899725655714\n",
      "epochs 6/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 1.8980829268693924, val_loss: 1.9642484759291012\n",
      "epochs 7/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 3.1951593309640884, val_loss: 0.25666257325145936\n",
      "epochs 8/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 1.57355298101902, val_loss: 0.2675861613566263\n",
      "epochs 9/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 1.2762933745980263, val_loss: 0.05710910033829148\n",
      "epochs 10/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 2.829837493598461, val_loss: 0.2463530649709153\n",
      "epochs 11/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 1.0980824530124664, val_loss: 0.7740283579462104\n",
      "epochs 12/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 1.214765764772892, val_loss: 0.07114771539798109\n",
      "epochs 13/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 2.4635603725910187, val_loss: 0.29318317505789715\n",
      "epochs 14/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.8292379081249237, val_loss: 0.09620660142496086\n",
      "epochs 15/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.8159705847501755, val_loss: 0.07694698703643452\n",
      "epochs 16/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.5751270763576031, val_loss: 0.09323561789298968\n",
      "epochs 17/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.6345611363649368, val_loss: 0.14338133471513478\n",
      "epochs 18/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.9851621389389038, val_loss: 0.0805366233333997\n",
      "epochs 19/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.53774144500494, val_loss: 0.08732906392995371\n",
      "epochs 20/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.5957419723272324, val_loss: 0.06522786250409102\n",
      "epochs 21/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.45864659175276756, val_loss: 0.16569277026590296\n",
      "epochs 22/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.36690047569572926, val_loss: 0.10184475643877199\n",
      "epochs 23/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.30535460636019707, val_loss: 0.06742099133306409\n",
      "epochs 24/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.45619307458400726, val_loss: 0.11199934074021359\n",
      "epochs 25/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.33616188541054726, val_loss: 0.16632076484772065\n",
      "epochs 26/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.3266725856810808, val_loss: 0.11025572268651256\n",
      "epochs 27/238\n",
      "Current Learning Rate: 0.06535738604806685\n",
      "train_loss: 0.2831452004611492, val_loss: 0.0671966607741423\n",
      "epochs 28/238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:46:35,087] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.11949084730197986, val_loss: 0.16439455734022582\n",
      "epochs 2/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.07046521206696828, val_loss: 0.11257879063123255\n",
      "epochs 3/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.049247102346271276, val_loss: 0.08443606204754259\n",
      "epochs 4/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04218178940936923, val_loss: 0.07255515123284163\n",
      "epochs 5/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04191350222875675, val_loss: 0.06961023136763186\n",
      "epochs 6/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04234166334693631, val_loss: 0.07039924933148238\n",
      "epochs 7/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04186796552191178, val_loss: 0.07232216879977689\n",
      "epochs 8/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.041880753046522536, val_loss: 0.07402063072368037\n",
      "epochs 9/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.0418030588577191, val_loss: 0.07530696727270778\n",
      "epochs 10/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.0416765653838714, val_loss: 0.07628166447264245\n",
      "epochs 11/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04193395748734474, val_loss: 0.07690852116987419\n",
      "epochs 12/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.041699963777015604, val_loss: 0.0771421423188359\n",
      "epochs 13/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04153689385081331, val_loss: 0.07717111090702626\n",
      "epochs 14/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04156622399265567, val_loss: 0.07699343719286844\n",
      "epochs 15/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04198115676020583, val_loss: 0.07686091769331445\n",
      "epochs 16/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.0419039367698133, val_loss: 0.07681970966384849\n",
      "epochs 17/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04143503603215019, val_loss: 0.07677011564050594\n",
      "epochs 18/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.041921003721654415, val_loss: 0.07664789564377214\n",
      "epochs 19/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04131612336883942, val_loss: 0.07647532690316439\n",
      "epochs 20/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.041438942309468985, val_loss: 0.07637028287151931\n",
      "epochs 21/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04202832095324993, val_loss: 0.07638626533055988\n",
      "epochs 22/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04176534432917833, val_loss: 0.07660615241426665\n",
      "epochs 23/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.0421774392016232, val_loss: 0.07691815380045834\n",
      "epochs 24/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.0409195264801383, val_loss: 0.0770341709915859\n",
      "epochs 25/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04178200506915649, val_loss: 0.07701221857375155\n",
      "epochs 26/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.042072241039325796, val_loss: 0.0769642645932941\n",
      "epochs 27/102\n",
      "Current Learning Rate: 0.0014109717027612255\n",
      "train_loss: 0.04197867168113589, val_loss: 0.07698475948483166\n",
      "epochs 28/102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:46:48,544] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 401.13160167572397, val_loss: 8.579392499393887\n",
      "epochs 2/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 18.291404165603495, val_loss: 7.556326233678394\n",
      "epochs 3/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 48.95711653983152, val_loss: 1.2772653005085886\n",
      "epochs 4/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 20.93334717883004, val_loss: 0.7847497035852737\n",
      "epochs 5/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 39.11521223077067, val_loss: 0.13786067048284975\n",
      "epochs 6/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 199.5565709979446, val_loss: 52.08348920610216\n",
      "epochs 7/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 17.378442609751666, val_loss: 5.562999996874067\n",
      "epochs 8/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 18.79050850868225, val_loss: 0.3018867353611212\n",
      "epochs 9/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 53.771379508354045, val_loss: 8.200403008196089\n",
      "epochs 10/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 73.39493155479431, val_loss: 205.4100719028049\n",
      "epochs 11/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 66.32822902555819, val_loss: 0.27784596513882004\n",
      "epochs 12/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 8.631866154847321, val_loss: 1.5798970925453533\n",
      "epochs 13/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 42.46473929175624, val_loss: 0.07385041586045797\n",
      "epochs 14/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 19.049338866163183, val_loss: 11.212946037451426\n",
      "epochs 15/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 25.27028249148969, val_loss: 7.173859139283498\n",
      "epochs 16/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 9.107662474667585, val_loss: 7.687985036108229\n",
      "epochs 17/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 29.04406578010983, val_loss: 2.2973514000574746\n",
      "epochs 18/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 7.879701495170593, val_loss: 49.488190441909765\n",
      "epochs 19/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 116.27601504546625, val_loss: 53.665756702423096\n",
      "epochs 20/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 13.883192093284041, val_loss: 1.0370732016033597\n",
      "epochs 21/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 10.121150261825985, val_loss: 7.448177900579241\n",
      "epochs 22/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 10.40661013788647, val_loss: 7.355230622821384\n",
      "epochs 23/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 6.7954233244613365, val_loss: 108.28353214263916\n",
      "epochs 24/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 30.762143448547082, val_loss: 7.387435687912835\n",
      "epochs 25/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 229.66356126025872, val_loss: 39.81744718551636\n",
      "epochs 26/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 21.281112750371296, val_loss: 19.965672148598564\n",
      "epochs 27/128\n",
      "Current Learning Rate: 0.09828683121978603\n",
      "train_loss: 20.979515541482854, val_loss: 7.133830971188015\n",
      "epochs 28/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:47:26,045] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.1045693196873698, val_loss: 0.16003691055650254\n",
      "epochs 2/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.05963819991383287, val_loss: 0.06591749434786227\n",
      "epochs 3/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.0629146403581318, val_loss: 0.12587046185540707\n",
      "epochs 4/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.05389096164920678, val_loss: 0.09259452746906997\n",
      "epochs 5/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04692804107132057, val_loss: 0.0702790180528407\n",
      "epochs 6/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04388569761067629, val_loss: 0.0809918485028902\n",
      "epochs 7/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04273510633760856, val_loss: 0.07288203176156255\n",
      "epochs 8/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04494637866607971, val_loss: 0.07673963756274639\n",
      "epochs 9/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.043088411964062184, val_loss: 0.0766738573115112\n",
      "epochs 10/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04351695135442747, val_loss: 0.07417738286150982\n",
      "epochs 11/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04373540447300507, val_loss: 0.07698021682315609\n",
      "epochs 12/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04327025696531766, val_loss: 0.07542579087506358\n",
      "epochs 13/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04337878569236232, val_loss: 0.07561108776846798\n",
      "epochs 14/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.043165779284512006, val_loss: 0.07610377547745076\n",
      "epochs 15/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04303308444408079, val_loss: 0.07519286923666692\n",
      "epochs 16/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04379688653474053, val_loss: 0.07596589916890177\n",
      "epochs 17/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.042811547276667423, val_loss: 0.07552644208772108\n",
      "epochs 18/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04325445420626137, val_loss: 0.07547253159443951\n",
      "epochs 19/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04341350726058914, val_loss: 0.07591596699462065\n",
      "epochs 20/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04350786992452211, val_loss: 0.07579159903171886\n",
      "epochs 21/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04329321543789572, val_loss: 0.07562356917898999\n",
      "epochs 22/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04335159557457599, val_loss: 0.07589199086780557\n",
      "epochs 23/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.043059467907167144, val_loss: 0.07562383257188937\n",
      "epochs 24/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.043097603781562716, val_loss: 0.07546737342878866\n",
      "epochs 25/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.043150072891472116, val_loss: 0.07535667352365433\n",
      "epochs 26/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.04340599236699442, val_loss: 0.0757547669895252\n",
      "epochs 27/176\n",
      "Current Learning Rate: 0.0021510768440619193\n",
      "train_loss: 0.042939512938674956, val_loss: 0.07532425827553703\n",
      "epochs 28/176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:47:48,068] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.07019845582544804, val_loss: 0.07718820451474231\n",
      "epochs 2/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.051929774383703865, val_loss: 0.10528484822538707\n",
      "epochs 3/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04382788886626562, val_loss: 0.06985527531732158\n",
      "epochs 4/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.044177692693968616, val_loss: 0.08432606146622372\n",
      "epochs 5/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.042361490738888584, val_loss: 0.07742300905778797\n",
      "epochs 6/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.0425416516760985, val_loss: 0.08085406060045999\n",
      "epochs 7/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.042069608035186924, val_loss: 0.07869716132214914\n",
      "epochs 8/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04202044724176327, val_loss: 0.07779351765121748\n",
      "epochs 9/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04217167881627878, val_loss: 0.07859605711483811\n",
      "epochs 10/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04218019482990106, val_loss: 0.07853133428984115\n",
      "epochs 11/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04205873887985945, val_loss: 0.07799164519787559\n",
      "epochs 12/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04221315930287043, val_loss: 0.07799356007146546\n",
      "epochs 13/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04186646081507206, val_loss: 0.07759632150474419\n",
      "epochs 14/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04201419992993275, val_loss: 0.07761831109463754\n",
      "epochs 15/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.0419873995706439, val_loss: 0.07759164226849356\n",
      "epochs 16/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04182550124824047, val_loss: 0.07718637011647742\n",
      "epochs 17/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.041864536702632904, val_loss: 0.07712821447704402\n",
      "epochs 18/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.041809408304591976, val_loss: 0.07701056960569178\n",
      "epochs 19/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04181879137953123, val_loss: 0.07685235362926808\n",
      "epochs 20/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04173923283815384, val_loss: 0.07670212721697883\n",
      "epochs 21/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04179382293174664, val_loss: 0.07656914095019197\n",
      "epochs 22/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.041806844994425774, val_loss: 0.0765877869950297\n",
      "epochs 23/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04172380889455477, val_loss: 0.0764387931752329\n",
      "epochs 24/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04172281641513109, val_loss: 0.07622095855509138\n",
      "epochs 25/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04172044154256582, val_loss: 0.07611441625502063\n",
      "epochs 26/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04172327214231094, val_loss: 0.07617211006840484\n",
      "epochs 27/167\n",
      "Current Learning Rate: 0.020163563501015606\n",
      "train_loss: 0.04171428953607877, val_loss: 0.07621866068479398\n",
      "epochs 28/167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:47:52,140] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 18.482021154180444, val_loss: 0.05613525670923991\n",
      "epochs 2/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.10601548784998832, val_loss: 0.10879628575680221\n",
      "epochs 3/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.1335418973531988, val_loss: 0.19370445962542565\n",
      "epochs 4/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.20684668290670272, val_loss: 0.16154040119242077\n",
      "epochs 5/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.15523393528053053, val_loss: 0.1132941775536488\n",
      "epochs 6/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.18107786664256342, val_loss: 0.0581677917240692\n",
      "epochs 7/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.11128658870304073, val_loss: 0.07066765734634828\n",
      "epochs 8/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.12737641524937418, val_loss: 0.06587482856250265\n",
      "epochs 9/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.13200272295486043, val_loss: 0.0847250188978958\n",
      "epochs 10/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.12794688347451114, val_loss: 0.07166984016011055\n",
      "epochs 11/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.10334493081878733, val_loss: 0.057587306706409436\n",
      "epochs 12/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.1267632844823378, val_loss: 0.07108515654464201\n",
      "epochs 13/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.11329703339961944, val_loss: 0.07079088164997908\n",
      "epochs 14/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.1036084617867514, val_loss: 0.19572994599810853\n",
      "epochs 15/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.21929727315350814, val_loss: 0.05593824300417004\n",
      "epochs 16/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.10683385941579386, val_loss: 0.12290025583044756\n",
      "epochs 17/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.08581826480588427, val_loss: 0.107579356274477\n",
      "epochs 18/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.14242017703751722, val_loss: 0.13442515375532416\n",
      "epochs 19/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.14023555459937564, val_loss: 0.0646721607336076\n",
      "epochs 20/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.13723576565583548, val_loss: 0.05596743867661442\n",
      "epochs 21/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.08985257497126306, val_loss: 0.05597835856476902\n",
      "epochs 22/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.07639661658969191, val_loss: 0.05787925224608949\n",
      "epochs 23/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.08442030184798771, val_loss: 0.057124995248159394\n",
      "epochs 24/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.0746109151729831, val_loss: 0.06516984793577446\n",
      "epochs 25/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.08907394321566378, val_loss: 0.06388279967864542\n",
      "epochs 26/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.10014084443725922, val_loss: 0.13395030537827146\n",
      "epochs 27/162\n",
      "Current Learning Rate: 0.01633426298577125\n",
      "train_loss: 0.29769058239266827, val_loss: 0.07438202352528202\n",
      "epochs 28/162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:48:13,066] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 19.83136990176583, val_loss: 8.952526085906559\n",
      "epochs 2/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 8.33441925711102, val_loss: 25.307034307056004\n",
      "epochs 3/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 12.606378740734524, val_loss: 1.7391654989785619\n",
      "epochs 4/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 5.320012640070032, val_loss: 0.5845384427035848\n",
      "epochs 5/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 3.2226888714013278, val_loss: 0.11617327111000325\n",
      "epochs 6/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 1.094363021629828, val_loss: 2.505389956964387\n",
      "epochs 7/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 2.081819968091117, val_loss: 1.5634222726027172\n",
      "epochs 8/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 1.5790581752856572, val_loss: 0.6571504697203636\n",
      "epochs 9/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 1.0701423281872715, val_loss: 0.09362788573101473\n",
      "epochs 10/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 1.1607872854780268, val_loss: 0.518185299096836\n",
      "epochs 11/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.788462472182733, val_loss: 2.2106951822837195\n",
      "epochs 12/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 2.9082987396805375, val_loss: 0.4152325917966664\n",
      "epochs 13/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.9353394411780216, val_loss: 0.056370207108557224\n",
      "epochs 14/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.6408995447887315, val_loss: 0.1518367498412974\n",
      "epochs 15/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.5716078148947822, val_loss: 0.10916131284870466\n",
      "epochs 16/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.535989076451019, val_loss: 0.06188576947978516\n",
      "epochs 17/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.5034779870399723, val_loss: 0.08947802095078966\n",
      "epochs 18/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.5162018796911946, val_loss: 0.056182251415318914\n",
      "epochs 19/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.35952183411077215, val_loss: 0.5973745496529672\n",
      "epochs 20/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.7769879072352692, val_loss: 0.058559852303005755\n",
      "epochs 21/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.28192084586178817, val_loss: 0.10448099177059096\n",
      "epochs 22/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.3522142585780885, val_loss: 0.06272840790188638\n",
      "epochs 23/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.294559429227202, val_loss: 0.06262006464304351\n",
      "epochs 24/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.34224466137863974, val_loss: 0.06712379144260518\n",
      "epochs 25/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.3165338015942662, val_loss: 0.10623269933704352\n",
      "epochs 26/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.2695661195450359, val_loss: 0.08189703070739698\n",
      "epochs 27/131\n",
      "Current Learning Rate: 0.044158239495655734\n",
      "train_loss: 0.2503816548872877, val_loss: 0.3069482983555645\n",
      "epochs 28/131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:48:23,143] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 82.14915887325036, val_loss: 1.2658484545018938\n",
      "epochs 2/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 3.348756482755696, val_loss: 1.895458000401656\n",
      "epochs 3/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 7.2665004545339835, val_loss: 6.874658743540446\n",
      "epochs 4/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 2.2368354041267327, val_loss: 0.10305563286278306\n",
      "epochs 5/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.4772850062560152, val_loss: 0.35387803668466705\n",
      "epochs 6/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.524306460387177, val_loss: 2.119553150402175\n",
      "epochs 7/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.5631906616467017, val_loss: 0.5761699296741022\n",
      "epochs 8/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 5.930625033599359, val_loss: 6.359745370017158\n",
      "epochs 9/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.8882791976685878, val_loss: 2.3581750873062344\n",
      "epochs 10/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.4944815779173817, val_loss: 0.057120120853748325\n",
      "epochs 11/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.9604280996653769, val_loss: 0.29515812796954477\n",
      "epochs 12/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.75384154446699, val_loss: 1.166509572416544\n",
      "epochs 13/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.6147913265007513, val_loss: 0.06703952133021732\n",
      "epochs 14/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.6097168614742933, val_loss: 0.3061457138844869\n",
      "epochs 15/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.8651789523385189, val_loss: 0.6231337114133768\n",
      "epochs 16/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.5919395184351338, val_loss: 0.08878394831966337\n",
      "epochs 17/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.301727342798754, val_loss: 0.05973589548294614\n",
      "epochs 18/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.5807072782406101, val_loss: 0.13873509665265576\n",
      "epochs 19/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.48339963359413324, val_loss: 0.08187871599673396\n",
      "epochs 20/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.31376726401072963, val_loss: 0.2765240583361851\n",
      "epochs 21/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.20915316328130387, val_loss: 0.13479635877431267\n",
      "epochs 22/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.21569100049910722, val_loss: 0.08909753167972667\n",
      "epochs 23/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.1972346148557133, val_loss: 0.22576515806011027\n",
      "epochs 24/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.2853333266727902, val_loss: 0.12638601051164894\n",
      "epochs 25/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.20554433262872476, val_loss: 1.1010334864258766\n",
      "epochs 26/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 3.106343963769851, val_loss: 0.23107673171519613\n",
      "epochs 27/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.2882623489325245, val_loss: 0.32949626445770264\n",
      "epochs 28/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.19577071795033085, val_loss: 0.06316795601338122\n",
      "epochs 29/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.38679246532006395, val_loss: 0.7797774956044223\n",
      "epochs 30/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.32091370328432983, val_loss: 0.11859558264566455\n",
      "epochs 31/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.33909418733997476, val_loss: 0.15057211297486597\n",
      "epochs 32/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.158907111665165, val_loss: 0.056459981697520964\n",
      "epochs 33/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.12792584531147172, val_loss: 0.05597319732704717\n",
      "epochs 34/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.123437547021442, val_loss: 0.05977701065583258\n",
      "epochs 35/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.13261179194820147, val_loss: 0.09735169469624655\n",
      "epochs 36/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.15291501053919396, val_loss: 0.056885228383988656\n",
      "epochs 37/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.2116170166819184, val_loss: 0.2186744607347969\n",
      "epochs 38/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.11439548118936795, val_loss: 0.16050571241309322\n",
      "epochs 39/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.208914399733422, val_loss: 0.07608900927071874\n",
      "epochs 40/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.24434655539139552, val_loss: 0.06970468813960906\n",
      "epochs 41/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.07531602654813065, val_loss: 0.06529693665328927\n",
      "epochs 42/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.19001725578197726, val_loss: 0.18548016645622234\n",
      "epochs 43/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.09866113208786205, val_loss: 0.05622084517057778\n",
      "epochs 44/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.08911059188001134, val_loss: 0.2912322683389195\n",
      "epochs 45/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.13952427124604583, val_loss: 0.08492910926320797\n",
      "epochs 46/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.08820879668066348, val_loss: 0.0569675709848525\n",
      "epochs 47/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.10553550432194714, val_loss: 0.14691351775480951\n",
      "epochs 48/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.10640009571970613, val_loss: 0.06288104548325969\n",
      "epochs 49/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.3728173443081754, val_loss: 0.5019636262829105\n",
      "epochs 50/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.18050491121701068, val_loss: 0.06578026531719337\n",
      "epochs 51/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.07532615007625686, val_loss: 0.06312820209374574\n",
      "epochs 52/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.1389370293063284, val_loss: 0.07163645973519629\n",
      "epochs 53/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.6177200930951922, val_loss: 0.0912641860161077\n",
      "epochs 54/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.07334197065965445, val_loss: 0.710956554238995\n",
      "epochs 55/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.20608350713909776, val_loss: 0.06256805631063697\n",
      "epochs 56/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.09682974895393406, val_loss: 0.07567998075521448\n",
      "epochs 57/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.14883135209135986, val_loss: 0.07557371562062246\n",
      "epochs 58/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.07038768244631312, val_loss: 0.2306191593266299\n",
      "epochs 59/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.1241053019012152, val_loss: 0.08919394639411217\n",
      "epochs 60/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 76.55640345528998, val_loss: 17.385379248195225\n",
      "epochs 61/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 14.678466362533745, val_loss: 4.350057085355123\n",
      "epochs 62/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 13.552540154368788, val_loss: 0.06461711388935025\n",
      "epochs 63/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 4.651050088582216, val_loss: 2.335827345649401\n",
      "epochs 64/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 7.2324632229628385, val_loss: 3.6363488137722015\n",
      "epochs 65/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 4.747510932109974, val_loss: 4.2882273064719305\n",
      "epochs 66/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 4.190703450529663, val_loss: 0.06482268988717503\n",
      "epochs 67/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 3.2991972631878324, val_loss: 3.9740134676297507\n",
      "epochs 68/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 5.00212342650802, val_loss: 0.13067682040855289\n",
      "epochs 69/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 3.8716040043919175, val_loss: 0.4982148407531592\n",
      "epochs 70/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 11.788552622000376, val_loss: 10.288905044396719\n",
      "epochs 71/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 5.920976178513633, val_loss: 0.47189058632486397\n",
      "epochs 72/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 2.5004430220082954, val_loss: 0.05579133871490032\n",
      "epochs 73/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 2.81150149360851, val_loss: 0.1355493548568726\n",
      "epochs 74/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 2.6702688093538636, val_loss: 1.7073181379172537\n",
      "epochs 75/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 2.600305630652993, val_loss: 7.332080761591594\n",
      "epochs 76/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 2.6402546840685384, val_loss: 0.5255749910655949\n",
      "epochs 77/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.2230808875626988, val_loss: 0.24496343159504855\n",
      "epochs 78/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.1685117447265871, val_loss: 0.2570762891134816\n",
      "epochs 79/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.9797445812986957, val_loss: 0.06033751090257687\n",
      "epochs 80/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 1.8152905429402988, val_loss: 0.5268099089670513\n",
      "epochs 81/197\n",
      "Current Learning Rate: 0.08855944922605707\n",
      "train_loss: 0.8574648289768784, val_loss: 0.15513735998602998\n",
      "epochs 82/197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:49:01,985] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.10089370373774458, val_loss: 0.10962420310938291\n",
      "epochs 2/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.08500204036978108, val_loss: 0.10309268749551848\n",
      "epochs 3/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.060109931544435245, val_loss: 0.11900888772894784\n",
      "epochs 4/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04428828305875262, val_loss: 0.057213084186918825\n",
      "epochs 5/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.05218925242553706, val_loss: 0.10272946164453363\n",
      "epochs 6/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04327548467726619, val_loss: 0.0683707113954976\n",
      "epochs 7/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04657252237235231, val_loss: 0.0914139451243601\n",
      "epochs 8/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04253068261055483, val_loss: 0.07394890075536548\n",
      "epochs 9/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04430896649999475, val_loss: 0.08032964527794523\n",
      "epochs 10/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.0421537916365735, val_loss: 0.07631439520951568\n",
      "epochs 11/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.042672584342114905, val_loss: 0.0768626910511456\n",
      "epochs 12/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04252933753067972, val_loss: 0.07712410468634011\n",
      "epochs 13/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.042267485004332334, val_loss: 0.07637118233833462\n",
      "epochs 14/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.042172110492915464, val_loss: 0.07627139182619026\n",
      "epochs 15/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04222487458200366, val_loss: 0.07617085218144995\n",
      "epochs 16/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.042206248466822284, val_loss: 0.07623297334066592\n",
      "epochs 17/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04196685993250598, val_loss: 0.07599136654365186\n",
      "epochs 18/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.041890128075869545, val_loss: 0.07553976589568062\n",
      "epochs 19/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04203026136383414, val_loss: 0.07558468594733211\n",
      "epochs 20/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.041910192756741134, val_loss: 0.0755303686665785\n",
      "epochs 21/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.041892301187747054, val_loss: 0.07537823564618723\n",
      "epochs 22/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04186641994035906, val_loss: 0.07533618847891274\n",
      "epochs 23/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04174046511589377, val_loss: 0.07505582100323711\n",
      "epochs 24/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.041718861631428204, val_loss: 0.0747394870494544\n",
      "epochs 25/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04184218794452371, val_loss: 0.07460673788834053\n",
      "epochs 26/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.04173643209247126, val_loss: 0.0745501411664817\n",
      "epochs 27/286\n",
      "Current Learning Rate: 0.013221202743208631\n",
      "train_loss: 0.0416945724627348, val_loss: 0.07437879510366151\n",
      "epochs 28/286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:49:25,877] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 29.701419876474475, val_loss: 102.0684724383884\n",
      "epochs 2/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 48.83374382389916, val_loss: 40.71386660469903\n",
      "epochs 3/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 28.637286754118072, val_loss: 20.322111950980293\n",
      "epochs 4/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 21.925101942486233, val_loss: 3.306044105026457\n",
      "epochs 5/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 5.529387543598811, val_loss: 5.770918246772554\n",
      "epochs 6/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 3.663276395863957, val_loss: 4.306446767515606\n",
      "epochs 7/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 2.735299896862772, val_loss: 0.11721709664197988\n",
      "epochs 8/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.3167514387104247, val_loss: 0.16402855717589976\n",
      "epochs 9/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.2686250905195873, val_loss: 0.7789420502053367\n",
      "epochs 10/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.3478643298149109, val_loss: 1.1270839215980635\n",
      "epochs 11/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 2.1144299142890506, val_loss: 0.2510338268637295\n",
      "epochs 12/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.3489236666096582, val_loss: 0.5161681194893188\n",
      "epochs 13/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.8611034899950027, val_loss: 0.9122679041077694\n",
      "epochs 14/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 2.0422353280915155, val_loss: 0.8468634802848101\n",
      "epochs 15/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.9177579714192285, val_loss: 0.20916316067531524\n",
      "epochs 16/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.3814267814159393, val_loss: 0.08053324644343876\n",
      "epochs 17/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 2.2224166327052646, val_loss: 0.8309713767634498\n",
      "epochs 18/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.6542406976222992, val_loss: 0.8289039900733365\n",
      "epochs 19/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.5084633810652628, val_loss: 0.061059213083353825\n",
      "epochs 20/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.373950383729405, val_loss: 0.06283136869185707\n",
      "epochs 21/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.5114243709378772, val_loss: 0.08901613749670408\n",
      "epochs 22/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.3298139323790867, val_loss: 0.13146946208224916\n",
      "epochs 23/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.0505503316720326, val_loss: 0.6460248716175556\n",
      "epochs 24/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 3.1550803217622967, val_loss: 0.4532914108907183\n",
      "epochs 25/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.9974661883380678, val_loss: 0.09601969923824072\n",
      "epochs 26/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.3204236858420901, val_loss: 0.5301765788139569\n",
      "epochs 27/136\n",
      "Current Learning Rate: 0.038331715800234825\n",
      "train_loss: 1.3581873840755887, val_loss: 0.0558085390673821\n",
      "epochs 28/136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:49:53,834] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/4\n",
      "epochs 1/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 232.2129563478477, val_loss: 28.50268472565545\n",
      "epochs 2/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 10.182199745542473, val_loss: 13.965817530949911\n",
      "epochs 3/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 9.193273135909328, val_loss: 0.05606358045932009\n",
      "epochs 4/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 6.690628426218474, val_loss: 0.1698375774988866\n",
      "epochs 5/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.588465573611083, val_loss: 3.658511483007007\n",
      "epochs 6/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 4.684486942158805, val_loss: 0.593368940676252\n",
      "epochs 7/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 3.1524761578551046, val_loss: 12.87155372566647\n",
      "epochs 8/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 6.511892365084754, val_loss: 5.180139382680257\n",
      "epochs 9/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 6.3678595920403795, val_loss: 0.5120891653415229\n",
      "epochs 10/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 3.475367448948048, val_loss: 0.07187197017709776\n",
      "epochs 11/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.6287648379802704, val_loss: 10.68228718969557\n",
      "epochs 12/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 6.204213158951865, val_loss: 2.818211742573314\n",
      "epochs 13/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 3.149505306725149, val_loss: 0.18778156114017797\n",
      "epochs 14/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 1.764108490060877, val_loss: 0.22862092778410037\n",
      "epochs 15/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.955190904714443, val_loss: 0.46490840510361725\n",
      "epochs 16/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.1434315277470484, val_loss: 1.8882652388678656\n",
      "epochs 17/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.176328781026381, val_loss: 0.0643534375833244\n",
      "epochs 18/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 1.19983627591972, val_loss: 0.36041997538672554\n",
      "epochs 19/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 8.50071545055619, val_loss: 1.0368132102820609\n",
      "epochs 20/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.098833405861148, val_loss: 0.6353321988135576\n",
      "epochs 21/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 1.4867303559073695, val_loss: 0.22480287361152781\n",
      "epochs 22/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 1.4133556540365573, val_loss: 0.19926367706890838\n",
      "epochs 23/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 1.3362251274563648, val_loss: 0.07132581672825229\n",
      "epochs 24/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.6076881559910596, val_loss: 2.88258223897881\n",
      "epochs 25/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 3.0423075518122427, val_loss: 3.141745540830824\n",
      "epochs 26/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 2.3727710832048348, val_loss: 0.28874708122263354\n",
      "epochs 27/201\n",
      "Current Learning Rate: 0.08176533434746597\n",
      "train_loss: 1.054597897662057, val_loss: 0.11202710173165542\n",
      "epochs 28/201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-07 20:50:08,889] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "Value: 0.04417854509752057\n",
      "Params:\n",
      "batch_size: 29\n",
      "epochs: 136\n",
      "hidden_size: 401\n",
      "learning_rate: 0.002173913346612086\n",
      "dropout_prob: 0.13670260827829842\n",
      "weight_decay: 5.971344548682018e-05\n",
      "gamma: 0.0167966935710767\n",
      "conv_channels: 80\n",
      "kernel_size: 1\n",
      "num_layers: 5\n"
     ]
    }
   ],
   "source": [
    "study_name = \"CNN-LSTM-Tunner\"\n",
    "storage_url = \"sqlite:///db.sqlite3\"\n",
    "\n",
    "storage = optuna.storages.RDBStorage(url=storage_url)\n",
    "\n",
    "# Check if the study exists\n",
    "study_names = [study.study_name for study in optuna.study.get_all_study_summaries(storage=storage)]\n",
    "if study_name in study_names:\n",
    "    # Delete the study if it exists\n",
    "    print(f\"Deleting study '{study_name}'\")\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_url)\n",
    "else:\n",
    "    print(f\"Study '{study_name}' does not exist in the storage.\")\n",
    "    \n",
    "study = optuna.create_study(direction='minimize', \n",
    "                            storage=storage_url, \n",
    "                            sampler=TPESampler(),\n",
    "                            pruner=optuna.pruners.SuccessiveHalvingPruner(\n",
    "                            min_resource=3,  # Minimum amount of resource allocated to a trial\n",
    "                            reduction_factor=3,  # Reduction factor for pruning\n",
    "                            min_early_stopping_rate=2  # Minimum early-stopping rate\n",
    "                            ),\n",
    "                            study_name=study_name,\n",
    "                            load_if_exists=False)\n",
    "\n",
    "pbar = tqdm(total=20, desc='Optimizing', unit='trial')\n",
    "\n",
    "def callback(study, trial):\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix_str(f\"Best Value: {study.best_value:.4f}\")\n",
    "\n",
    "study.optimize(objective, n_trials=20, callbacks=[callback])\n",
    "pbar.close()\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.12517318265357366\n",
      "epochs 2/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.06719497934035543\n",
      "epochs 3/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.06747081076200293\n",
      "epochs 4/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.05622061065191196\n",
      "epochs 5/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.04823265260482138\n",
      "epochs 6/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.03796529760303976\n",
      "epochs 7/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.03044320971713643\n",
      "epochs 8/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.03286694636227804\n",
      "epochs 9/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.03259271110356268\n",
      "epochs 10/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.029458158706633927\n",
      "epochs 11/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.026877352926897368\n",
      "epochs 12/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02896370605729451\n",
      "epochs 13/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.026314926574640494\n",
      "epochs 14/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.026863333251741197\n",
      "epochs 15/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.030391090896618726\n",
      "epochs 16/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02454436731608843\n",
      "epochs 17/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02331090090188405\n",
      "epochs 18/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02422124435327499\n",
      "epochs 19/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.023718654015317076\n",
      "epochs 20/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.023475968457340193\n",
      "epochs 21/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02421891381823958\n",
      "epochs 22/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02350768133355477\n",
      "epochs 23/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02227710230282456\n",
      "epochs 24/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.022411813622090482\n",
      "epochs 25/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02117071293253469\n",
      "epochs 26/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.021219003824862064\n",
      "epochs 27/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02136807597815749\n",
      "epochs 28/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020457614674577834\n",
      "epochs 29/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02194662687651106\n",
      "epochs 30/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.021242768723621137\n",
      "epochs 31/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020592765852429525\n",
      "epochs 32/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020267716726973757\n",
      "epochs 33/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020528822170389403\n",
      "epochs 34/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020924759134585556\n",
      "epochs 35/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020182385305007402\n",
      "epochs 36/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01994722742906392\n",
      "epochs 37/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02009924833072108\n",
      "epochs 38/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019355307949202444\n",
      "epochs 39/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019255876940506924\n",
      "epochs 40/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019382382276321383\n",
      "epochs 41/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01945636832560585\n",
      "epochs 42/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020121835156546527\n",
      "epochs 43/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019280534226844018\n",
      "epochs 44/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020398801692257013\n",
      "epochs 45/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.022138929334081087\n",
      "epochs 46/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018378448681754957\n",
      "epochs 47/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01908719601083754\n",
      "epochs 48/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01856552939312274\n",
      "epochs 49/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018494139114476706\n",
      "epochs 50/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018721636255375214\n",
      "epochs 51/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017603324700892833\n",
      "epochs 52/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018072460949446797\n",
      "epochs 53/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01761481420274962\n",
      "epochs 54/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018132072125337326\n",
      "epochs 55/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017828084185128792\n",
      "epochs 56/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018862212069237744\n",
      "epochs 57/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02054112196276913\n",
      "epochs 58/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018761328454409656\n",
      "epochs 59/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.022286599947148242\n",
      "epochs 60/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02031354592623689\n",
      "epochs 61/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018927163524494972\n",
      "epochs 62/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.021182988360157685\n",
      "epochs 63/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.026259979660540854\n",
      "epochs 64/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020951424335685767\n",
      "epochs 65/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02103491869719518\n",
      "epochs 66/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01735028096246188\n",
      "epochs 67/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019650280604966812\n",
      "epochs 68/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020180318927116418\n",
      "epochs 69/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019582419240679074\n",
      "epochs 70/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019327677370812226\n",
      "epochs 71/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.022362387551563896\n",
      "epochs 72/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02045447227741264\n",
      "epochs 73/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020035978443316976\n",
      "epochs 74/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.019847144730091613\n",
      "epochs 75/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01819363038090092\n",
      "epochs 76/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017473159772365493\n",
      "epochs 77/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017746170563946687\n",
      "epochs 78/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01708726431092213\n",
      "epochs 79/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016810961215890496\n",
      "epochs 80/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01741119453530431\n",
      "epochs 81/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018895301798625884\n",
      "epochs 82/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018313194404645065\n",
      "epochs 83/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.020554320375258277\n",
      "epochs 84/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01682809913903906\n",
      "epochs 85/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016458006324009383\n",
      "epochs 86/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01680147199820365\n",
      "epochs 87/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017667785065929854\n",
      "epochs 88/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01909363761552851\n",
      "epochs 89/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01947743207791872\n",
      "epochs 90/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0192741302738863\n",
      "epochs 91/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01819519108027668\n",
      "epochs 92/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01931263504041083\n",
      "epochs 93/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01678482893179602\n",
      "epochs 94/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01744260224492151\n",
      "epochs 95/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016769748203740973\n",
      "epochs 96/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017940984583994443\n",
      "epochs 97/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016810740290869337\n",
      "epochs 98/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01674559807374694\n",
      "epochs 99/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017286659110578948\n",
      "epochs 100/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01842201945611224\n",
      "epochs 101/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.02173129128641449\n",
      "epochs 102/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01796507385141147\n",
      "epochs 103/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017215273594720876\n",
      "epochs 104/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01638365831631408\n",
      "epochs 105/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016977983897150908\n",
      "epochs 106/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017284046140654635\n",
      "epochs 107/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017646133190956554\n",
      "epochs 108/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017596455767354066\n",
      "epochs 109/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017225596883454947\n",
      "epochs 110/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017851535923438386\n",
      "epochs 111/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01713561410616859\n",
      "epochs 112/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017432717084442696\n",
      "epochs 113/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01827572724518918\n",
      "epochs 114/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.018345550661992834\n",
      "epochs 115/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016316296727853587\n",
      "epochs 116/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017992889847610733\n",
      "epochs 117/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01784340418116362\n",
      "epochs 118/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01780721588551791\n",
      "epochs 119/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017081013984689315\n",
      "epochs 120/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017051133420965116\n",
      "epochs 121/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016229338895990857\n",
      "epochs 122/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016253979463794234\n",
      "epochs 123/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01699098851531744\n",
      "epochs 124/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01689018198036744\n",
      "epochs 125/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017430344510271593\n",
      "epochs 126/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01698433590246606\n",
      "epochs 127/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.017657796690602683\n",
      "epochs 128/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016061274782663934\n",
      "epochs 129/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01612908480186619\n",
      "epochs 130/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.0181229272808802\n",
      "epochs 131/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016956314708093286\n",
      "epochs 132/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016708478104124812\n",
      "epochs 133/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016166684220445825\n",
      "epochs 134/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016425854081818316\n",
      "epochs 135/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.01762771470304061\n",
      "epochs 136/136\n",
      "Current Learning Rate: 0.002173913346612086\n",
      "train_loss: 0.016887946898889453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneDimCNNLSTMModel(\n",
       "  (conv): Conv1d(57, 80, kernel_size=(1,), stride=(1,), padding=(2,))\n",
       "  (relu1): ReLU()\n",
       "  (dropout_1): Dropout(p=0.13670260827829842, inplace=False)\n",
       "  (lstm): LSTM(80, 401, num_layers=5, batch_first=True, dropout=0.13670260827829842)\n",
       "  (dropout_2): Dropout(p=0.13670260827829842, inplace=False)\n",
       "  (fc): Linear(in_features=401, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelActioner(train_data=train_data,test_data=test_data,device=device)\n",
    "model.train(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.10666667],\n",
       "       [1.02666667],\n",
       "       [1.        ],\n",
       "       [1.26666667]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.reshape(-1,1)\n",
    "print(len(y_test))\n",
    "y_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.10558876643578212\n"
     ]
    }
   ],
   "source": [
    "preds = model.test(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.array(preds)\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(252,)\n",
      "(252,)\n",
      "MAPE Score: %26.87\n",
      "MSE Score: 0.18\n",
      "MAE Score: 0.31\n",
      "R_2 Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "preds_inverse = []\n",
    "y_true_inverse = []\n",
    "\n",
    "idx = 0\n",
    "data_size = len(y_test)//len(company_list)\n",
    "print(data_size)\n",
    "\n",
    "preds_comp_data = []\n",
    "y_true_comp_data = pd.DataFrame()\n",
    "\n",
    "for company in company_list:\n",
    "    scaler = company_dict[company][\"scaler_y\"]\n",
    "    preds_inverse.append(scaler.inverse_transform(preds[idx:idx+data_size]))\n",
    "    y_true_inverse.append(scaler.inverse_transform(y_test[idx:idx+data_size].reshape(-1, 1) ))\n",
    "\n",
    "    for pred in preds_inverse:\n",
    "        preds_comp_data.append({\"Prediction\": pred, \"Company\": company})\n",
    "\n",
    "    idx += data_size\n",
    "\n",
    "\n",
    "\n",
    "# for company in company_list:\n",
    "#     preds_inverse.append(np.array(preds).flatten())\n",
    "#     y_true_inverse.append(np.array(y_test).flatten())\n",
    "comp_preds_inverse = np.array(preds_inverse)\n",
    "comp_true_inverse = np.array(y_true_inverse)\n",
    "\n",
    "preds_inverse = np.array(preds_inverse).flatten()\n",
    "y_true_inverse = np.array(y_true_inverse).flatten()\n",
    "print(y_true_inverse.shape)\n",
    "print(preds_inverse.shape)\n",
    "\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_true_inverse, preds_inverse)\n",
    "mape = mean_absolute_percentage_error(y_true_inverse, preds_inverse)*100\n",
    "mae = mean_absolute_error(y_true_inverse, preds_inverse)\n",
    "r2 = r2_score(y_true_inverse,preds_inverse)\n",
    "\n",
    "print(f\"MAPE Score: %{mape:.2f}\")\n",
    "print(f\"MSE Score: {mse:.2f}\")\n",
    "print(f\"MAE Score: {mae:.2f}\")\n",
    "print(f\"R_2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9491113 , 0.9583211 , 0.9650893 , 0.96887094, 0.9699886 ,\n",
       "       0.9703193 , 0.97093767, 2.4993455 , 2.5532386 , 2.5811653 ,\n",
       "       2.609108  , 2.6206524 , 2.6246848 , 2.623246  , 1.8702964 ,\n",
       "       1.8973935 , 1.9187646 , 1.9329811 , 1.9376363 , 1.937757  ,\n",
       "       1.9388676 , 0.9296104 , 0.9423028 , 0.95085865, 0.95306396,\n",
       "       0.9524521 , 0.9478674 , 0.94055223, 0.9763765 , 0.98598534,\n",
       "       0.99351346, 0.9981543 , 0.9989512 , 0.9988447 , 0.99884194,\n",
       "       5.9994636 , 6.0601487 , 6.104047  , 6.1319346 , 6.134231  ,\n",
       "       6.1354184 , 6.136641  , 1.0138965 , 1.0412184 , 1.0661298 ,\n",
       "       1.0846885 , 1.086496  , 1.0836338 , 1.0837128 , 1.1505904 ,\n",
       "       1.1641707 , 1.1760508 , 1.1951197 , 1.1948907 , 1.1908303 ,\n",
       "       1.1922711 , 1.3580722 , 1.376837  , 1.3773673 , 1.3907557 ,\n",
       "       1.396161  , 1.3968805 , 1.3966826 , 1.5870882 , 1.6149808 ,\n",
       "       1.6361946 , 1.6563485 , 1.660127  , 1.6583315 , 1.6607614 ,\n",
       "       2.2883408 , 2.3127513 , 2.3325222 , 2.3433917 , 2.3457851 ,\n",
       "       2.34356   , 2.3448892 , 5.528812  , 5.5972843 , 5.6586246 ,\n",
       "       5.7142797 , 5.7258496 , 5.724062  , 5.7323456 , 1.0147672 ,\n",
       "       1.0292146 , 1.0466865 , 1.0520601 , 1.0573399 , 1.0589964 ,\n",
       "       1.0615476 , 0.6649695 , 0.66863745, 0.67221504, 0.6747908 ,\n",
       "       0.67410845, 0.6738194 , 0.6737196 , 0.34318227, 0.34587035,\n",
       "       0.34817666, 0.35012457, 0.35054117, 0.35047978, 0.350451  ,\n",
       "       1.2750769 , 1.2901416 , 1.3054883 , 1.3166468 , 1.3195242 ,\n",
       "       1.3198981 , 1.321071  , 1.6872104 , 1.718874  , 1.7407217 ,\n",
       "       1.7544147 , 1.7538072 , 1.754903  , 1.7555734 , 1.0645627 ,\n",
       "       1.0808407 , 1.0943383 , 1.1050189 , 1.108702  , 1.1090887 ,\n",
       "       1.1103522 , 1.1383997 , 1.1612747 , 1.1764699 , 1.185301  ,\n",
       "       1.1884277 , 1.1886936 , 1.1885089 , 1.1204972 , 1.1380943 ,\n",
       "       1.1530114 , 1.1622294 , 1.1640105 , 1.1634798 , 1.1637049 ,\n",
       "       2.5534413 , 2.5957696 , 2.6366558 , 2.6665103 , 2.6789808 ,\n",
       "       2.6574004 , 2.6571386 , 0.36652496, 0.37039772, 0.3730529 ,\n",
       "       0.37515438, 0.37561083, 0.3756809 , 0.37582895, 1.3016956 ,\n",
       "       1.3336097 , 1.3674006 , 1.3919092 , 1.3937943 , 1.3940661 ,\n",
       "       1.3945475 , 1.2022281 , 1.2111396 , 1.2224478 , 1.2301127 ,\n",
       "       1.2319593 , 1.2320927 , 1.231564  , 0.9923399 , 1.0449235 ,\n",
       "       1.0736887 , 1.0867121 , 1.093029  , 1.0953329 , 1.110036  ,\n",
       "       2.1737595 , 2.1944823 , 2.2097178 , 2.2217102 , 2.2245963 ,\n",
       "       2.2247763 , 2.225369  , 2.6715991 , 2.7014406 , 2.72456   ,\n",
       "       2.7423291 , 2.745791  , 2.7459033 , 2.7477193 , 0.2948723 ,\n",
       "       0.3001681 , 0.30443725, 0.30790347, 0.30909765, 0.30933702,\n",
       "       0.3094875 , 0.9154948 , 0.9256278 , 0.93713444, 0.94633734,\n",
       "       0.94924533, 0.94880927, 0.94931084, 0.45768166, 0.46728003,\n",
       "       0.4749748 , 0.48091272, 0.48294336, 0.48335105, 0.4836677 ,\n",
       "       1.6276371 , 1.6410124 , 1.659388  , 1.671697  , 1.6705317 ,\n",
       "       1.6681592 , 1.6671596 , 1.544133  , 1.5606544 , 1.5766162 ,\n",
       "       1.5843651 , 1.5882711 , 1.5862166 , 1.5863636 , 2.3510756 ,\n",
       "       2.3979619 , 2.4458344 , 2.4880786 , 2.50122   , 2.5018024 ,\n",
       "       2.5027294 , 1.0356114 , 1.0431601 , 1.047512  , 1.0506182 ,\n",
       "       1.0496976 , 1.0502163 , 1.0498215 , 0.39969254, 0.40924144,\n",
       "       0.41628188, 0.41919926, 0.41974232, 0.42056796, 0.42088905,\n",
       "       0.90978473, 0.9249964 , 0.9376319 , 0.94669443, 0.9487351 ,\n",
       "       0.94884956, 0.9489401 ], dtype=float32)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9491113 ],\n",
       "        [0.9583211 ],\n",
       "        [0.9650893 ],\n",
       "        [0.96887094],\n",
       "        [0.9699886 ],\n",
       "        [0.9703193 ],\n",
       "        [0.97093767]],\n",
       "\n",
       "       [[2.4993455 ],\n",
       "        [2.5532386 ],\n",
       "        [2.5811653 ],\n",
       "        [2.609108  ],\n",
       "        [2.6206524 ],\n",
       "        [2.6246848 ],\n",
       "        [2.623246  ]],\n",
       "\n",
       "       [[1.8702964 ],\n",
       "        [1.8973935 ],\n",
       "        [1.9187646 ],\n",
       "        [1.9329811 ],\n",
       "        [1.9376363 ],\n",
       "        [1.937757  ],\n",
       "        [1.9388676 ]],\n",
       "\n",
       "       [[0.9296104 ],\n",
       "        [0.9423028 ],\n",
       "        [0.95085865],\n",
       "        [0.95306396],\n",
       "        [0.9524521 ],\n",
       "        [0.9478674 ],\n",
       "        [0.94055223]],\n",
       "\n",
       "       [[0.9763765 ],\n",
       "        [0.98598534],\n",
       "        [0.99351346],\n",
       "        [0.9981543 ],\n",
       "        [0.9989512 ],\n",
       "        [0.9988447 ],\n",
       "        [0.99884194]],\n",
       "\n",
       "       [[5.9994636 ],\n",
       "        [6.0601487 ],\n",
       "        [6.104047  ],\n",
       "        [6.1319346 ],\n",
       "        [6.134231  ],\n",
       "        [6.1354184 ],\n",
       "        [6.136641  ]],\n",
       "\n",
       "       [[1.0138965 ],\n",
       "        [1.0412184 ],\n",
       "        [1.0661298 ],\n",
       "        [1.0846885 ],\n",
       "        [1.086496  ],\n",
       "        [1.0836338 ],\n",
       "        [1.0837128 ]],\n",
       "\n",
       "       [[1.1505904 ],\n",
       "        [1.1641707 ],\n",
       "        [1.1760508 ],\n",
       "        [1.1951197 ],\n",
       "        [1.1948907 ],\n",
       "        [1.1908303 ],\n",
       "        [1.1922711 ]],\n",
       "\n",
       "       [[1.3580722 ],\n",
       "        [1.376837  ],\n",
       "        [1.3773673 ],\n",
       "        [1.3907557 ],\n",
       "        [1.396161  ],\n",
       "        [1.3968805 ],\n",
       "        [1.3966826 ]],\n",
       "\n",
       "       [[1.5870882 ],\n",
       "        [1.6149808 ],\n",
       "        [1.6361946 ],\n",
       "        [1.6563485 ],\n",
       "        [1.660127  ],\n",
       "        [1.6583315 ],\n",
       "        [1.6607614 ]],\n",
       "\n",
       "       [[2.2883408 ],\n",
       "        [2.3127513 ],\n",
       "        [2.3325222 ],\n",
       "        [2.3433917 ],\n",
       "        [2.3457851 ],\n",
       "        [2.34356   ],\n",
       "        [2.3448892 ]],\n",
       "\n",
       "       [[5.528812  ],\n",
       "        [5.5972843 ],\n",
       "        [5.6586246 ],\n",
       "        [5.7142797 ],\n",
       "        [5.7258496 ],\n",
       "        [5.724062  ],\n",
       "        [5.7323456 ]],\n",
       "\n",
       "       [[1.0147672 ],\n",
       "        [1.0292146 ],\n",
       "        [1.0466865 ],\n",
       "        [1.0520601 ],\n",
       "        [1.0573399 ],\n",
       "        [1.0589964 ],\n",
       "        [1.0615476 ]],\n",
       "\n",
       "       [[0.6649695 ],\n",
       "        [0.66863745],\n",
       "        [0.67221504],\n",
       "        [0.6747908 ],\n",
       "        [0.67410845],\n",
       "        [0.6738194 ],\n",
       "        [0.6737196 ]],\n",
       "\n",
       "       [[0.34318227],\n",
       "        [0.34587035],\n",
       "        [0.34817666],\n",
       "        [0.35012457],\n",
       "        [0.35054117],\n",
       "        [0.35047978],\n",
       "        [0.350451  ]],\n",
       "\n",
       "       [[1.2750769 ],\n",
       "        [1.2901416 ],\n",
       "        [1.3054883 ],\n",
       "        [1.3166468 ],\n",
       "        [1.3195242 ],\n",
       "        [1.3198981 ],\n",
       "        [1.321071  ]],\n",
       "\n",
       "       [[1.6872104 ],\n",
       "        [1.718874  ],\n",
       "        [1.7407217 ],\n",
       "        [1.7544147 ],\n",
       "        [1.7538072 ],\n",
       "        [1.754903  ],\n",
       "        [1.7555734 ]],\n",
       "\n",
       "       [[1.0645627 ],\n",
       "        [1.0808407 ],\n",
       "        [1.0943383 ],\n",
       "        [1.1050189 ],\n",
       "        [1.108702  ],\n",
       "        [1.1090887 ],\n",
       "        [1.1103522 ]],\n",
       "\n",
       "       [[1.1383997 ],\n",
       "        [1.1612747 ],\n",
       "        [1.1764699 ],\n",
       "        [1.185301  ],\n",
       "        [1.1884277 ],\n",
       "        [1.1886936 ],\n",
       "        [1.1885089 ]],\n",
       "\n",
       "       [[1.1204972 ],\n",
       "        [1.1380943 ],\n",
       "        [1.1530114 ],\n",
       "        [1.1622294 ],\n",
       "        [1.1640105 ],\n",
       "        [1.1634798 ],\n",
       "        [1.1637049 ]],\n",
       "\n",
       "       [[2.5534413 ],\n",
       "        [2.5957696 ],\n",
       "        [2.6366558 ],\n",
       "        [2.6665103 ],\n",
       "        [2.6789808 ],\n",
       "        [2.6574004 ],\n",
       "        [2.6571386 ]],\n",
       "\n",
       "       [[0.36652496],\n",
       "        [0.37039772],\n",
       "        [0.3730529 ],\n",
       "        [0.37515438],\n",
       "        [0.37561083],\n",
       "        [0.3756809 ],\n",
       "        [0.37582895]],\n",
       "\n",
       "       [[1.3016956 ],\n",
       "        [1.3336097 ],\n",
       "        [1.3674006 ],\n",
       "        [1.3919092 ],\n",
       "        [1.3937943 ],\n",
       "        [1.3940661 ],\n",
       "        [1.3945475 ]],\n",
       "\n",
       "       [[1.2022281 ],\n",
       "        [1.2111396 ],\n",
       "        [1.2224478 ],\n",
       "        [1.2301127 ],\n",
       "        [1.2319593 ],\n",
       "        [1.2320927 ],\n",
       "        [1.231564  ]],\n",
       "\n",
       "       [[0.9923399 ],\n",
       "        [1.0449235 ],\n",
       "        [1.0736887 ],\n",
       "        [1.0867121 ],\n",
       "        [1.093029  ],\n",
       "        [1.0953329 ],\n",
       "        [1.110036  ]],\n",
       "\n",
       "       [[2.1737595 ],\n",
       "        [2.1944823 ],\n",
       "        [2.2097178 ],\n",
       "        [2.2217102 ],\n",
       "        [2.2245963 ],\n",
       "        [2.2247763 ],\n",
       "        [2.225369  ]],\n",
       "\n",
       "       [[2.6715991 ],\n",
       "        [2.7014406 ],\n",
       "        [2.72456   ],\n",
       "        [2.7423291 ],\n",
       "        [2.745791  ],\n",
       "        [2.7459033 ],\n",
       "        [2.7477193 ]],\n",
       "\n",
       "       [[0.2948723 ],\n",
       "        [0.3001681 ],\n",
       "        [0.30443725],\n",
       "        [0.30790347],\n",
       "        [0.30909765],\n",
       "        [0.30933702],\n",
       "        [0.3094875 ]],\n",
       "\n",
       "       [[0.9154948 ],\n",
       "        [0.9256278 ],\n",
       "        [0.93713444],\n",
       "        [0.94633734],\n",
       "        [0.94924533],\n",
       "        [0.94880927],\n",
       "        [0.94931084]],\n",
       "\n",
       "       [[0.45768166],\n",
       "        [0.46728003],\n",
       "        [0.4749748 ],\n",
       "        [0.48091272],\n",
       "        [0.48294336],\n",
       "        [0.48335105],\n",
       "        [0.4836677 ]],\n",
       "\n",
       "       [[1.6276371 ],\n",
       "        [1.6410124 ],\n",
       "        [1.659388  ],\n",
       "        [1.671697  ],\n",
       "        [1.6705317 ],\n",
       "        [1.6681592 ],\n",
       "        [1.6671596 ]],\n",
       "\n",
       "       [[1.544133  ],\n",
       "        [1.5606544 ],\n",
       "        [1.5766162 ],\n",
       "        [1.5843651 ],\n",
       "        [1.5882711 ],\n",
       "        [1.5862166 ],\n",
       "        [1.5863636 ]],\n",
       "\n",
       "       [[2.3510756 ],\n",
       "        [2.3979619 ],\n",
       "        [2.4458344 ],\n",
       "        [2.4880786 ],\n",
       "        [2.50122   ],\n",
       "        [2.5018024 ],\n",
       "        [2.5027294 ]],\n",
       "\n",
       "       [[1.0356114 ],\n",
       "        [1.0431601 ],\n",
       "        [1.047512  ],\n",
       "        [1.0506182 ],\n",
       "        [1.0496976 ],\n",
       "        [1.0502163 ],\n",
       "        [1.0498215 ]],\n",
       "\n",
       "       [[0.39969254],\n",
       "        [0.40924144],\n",
       "        [0.41628188],\n",
       "        [0.41919926],\n",
       "        [0.41974232],\n",
       "        [0.42056796],\n",
       "        [0.42088905]],\n",
       "\n",
       "       [[0.90978473],\n",
       "        [0.9249964 ],\n",
       "        [0.9376319 ],\n",
       "        [0.94669443],\n",
       "        [0.9487351 ],\n",
       "        [0.94884956],\n",
       "        [0.9489401 ]]], dtype=float32)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_preds_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06  , 1.03  , 1.02  , 1.12  , 1.13  , 1.16  , 1.03  , 1.71  ,\n",
       "       1.3   , 2.16  , 3.31  , 1.87  , 1.45  , 2.53  , 1.84  , 1.88  ,\n",
       "       1.74  , 2.01  , 2.07  , 2.08  , 2.03  , 0.75  , 1.13  , 0.54  ,\n",
       "       0.72  , 0.65  , 0.61  , 0.7   , 1.03  , 1.06  , 0.99  , 0.94  ,\n",
       "       1.01  , 1.07  , 1.01  , 6.66  , 7.52  , 6.08  , 6.61  , 6.41  ,\n",
       "       7.15  , 8.34  , 0.9   , 0.76  , 0.05  , 0.44  , 0.57  , 0.58  ,\n",
       "       0.72  , 1.05  , 1.06  , 1.54  , 1.11  , 1.13  , 1.29  , 1.21  ,\n",
       "       1.87  , 1.95  , 1.88  , 2.11  , 1.94  , 1.96  , 1.85  , 1.74  ,\n",
       "       1.45  , 1.77  , 1.62  , 1.76  , 1.9   , 1.52  , 3.16  , 2.99  ,\n",
       "       1.87  , 2.9   , 3.07  , 3.17  , 2.25  , 5.98  , 6.28  , 6.04  ,\n",
       "       5.71  , 5.81  , 4.79  , 4.69  , 1.13  , 1.15  , 0.78  , 1.39  ,\n",
       "       1.33  , 1.5   , 1.43  , 0.66  , 0.66  , 0.44  , 0.56  , 0.65  ,\n",
       "       0.7   , 0.64  , 0.44  , 0.45  , 0.48  , 0.4   , 0.4   , 0.48  ,\n",
       "       0.48  , 1.66  , 1.78  , 1.55  , 1.78  , 1.89  , 2.15  , 1.96  ,\n",
       "       2.04  , 1.69  , 1.63  , 2.07  , 2.07  , 2.15  , 2.    , 1.07  ,\n",
       "       1.02  , 1.21  , 1.39  , 1.27  , 1.42  , 1.32  , 1.1   , 0.78  ,\n",
       "       1.09  , 1.52  , 1.18  , 0.77  , 1.19  , 1.3   , 1.35  , 1.31  ,\n",
       "       1.55  , 1.54  , 1.68  , 1.67  , 3.26  , 3.53  , 3.76  , 3.35  ,\n",
       "       3.61  , 3.47  , 3.6   , 0.3933, 0.3833, 0.42  , 0.4067, 0.4067,\n",
       "       0.4233, 0.43  , 1.68  , 1.58  , 1.67  , 1.48  , 1.75  , 1.69  ,\n",
       "       1.8   , 1.35  , 1.67  , 1.11  , 1.43  , 1.52  , 1.23  , 1.41  ,\n",
       "       1.15  , 1.33  , 1.24  , 1.33  , 1.42  , 1.33  , 1.18  , 2.72  ,\n",
       "       2.82  , 2.75  , 2.61  , 2.88  , 2.94  , 2.97  , 3.01  , 3.15  ,\n",
       "       2.44  , 3.    , 3.14  , 3.22  , 2.33  , 0.32  , 0.37  , 0.37  ,\n",
       "       0.37  , 0.37  , 0.39  , 0.39  , 1.0333, 1.12  , 1.1933, 1.2067,\n",
       "       1.2   , 1.3333, 1.26  , 0.6   , 0.65  , 0.65  , 0.69  , 0.66  ,\n",
       "       0.74  , 0.63  , 2.05  , 1.87  , 1.68  , 1.24  , 1.45  , 1.51  ,\n",
       "       1.98  , 1.77  , 2.3   , 1.41  , 2.09  , 2.03  , 2.13  , 2.03  ,\n",
       "       1.81  , 2.54  , 2.13  , 2.83  , 2.02  , 1.43  , 3.32  , 1.08  ,\n",
       "       1.16  , 1.09  , 1.03  , 1.3   , 1.07  , 0.93  , 0.4311, 0.4978,\n",
       "       0.4178, 0.44  , 0.5467, 0.5667, 0.4733, 0.89  , 1.04  , 1.08  ,\n",
       "       1.04  , 0.99  , 1.17  , 0.97  ])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9491113 , 0.9583211 , 0.9650893 , 0.96887094, 0.9699886 ,\n",
       "       0.9703193 , 0.97093767, 2.4993455 , 2.5532386 , 2.5811653 ,\n",
       "       2.609108  , 2.6206524 , 2.6246848 , 2.623246  , 1.8702964 ,\n",
       "       1.8973935 , 1.9187646 , 1.9329811 , 1.9376363 , 1.937757  ,\n",
       "       1.9388676 , 0.9296104 , 0.9423028 , 0.95085865, 0.95306396,\n",
       "       0.9524521 , 0.9478674 , 0.94055223, 0.9763765 , 0.98598534,\n",
       "       0.99351346, 0.9981543 , 0.9989512 , 0.9988447 , 0.99884194,\n",
       "       5.9994636 , 6.0601487 , 6.104047  , 6.1319346 , 6.134231  ,\n",
       "       6.1354184 , 6.136641  , 1.0138965 , 1.0412184 , 1.0661298 ,\n",
       "       1.0846885 , 1.086496  , 1.0836338 , 1.0837128 , 1.1505904 ,\n",
       "       1.1641707 , 1.1760508 , 1.1951197 , 1.1948907 , 1.1908303 ,\n",
       "       1.1922711 , 1.3580722 , 1.376837  , 1.3773673 , 1.3907557 ,\n",
       "       1.396161  , 1.3968805 , 1.3966826 , 1.5870882 , 1.6149808 ,\n",
       "       1.6361946 , 1.6563485 , 1.660127  , 1.6583315 , 1.6607614 ,\n",
       "       2.2883408 , 2.3127513 , 2.3325222 , 2.3433917 , 2.3457851 ,\n",
       "       2.34356   , 2.3448892 , 5.528812  , 5.5972843 , 5.6586246 ,\n",
       "       5.7142797 , 5.7258496 , 5.724062  , 5.7323456 , 1.0147672 ,\n",
       "       1.0292146 , 1.0466865 , 1.0520601 , 1.0573399 , 1.0589964 ,\n",
       "       1.0615476 , 0.6649695 , 0.66863745, 0.67221504, 0.6747908 ,\n",
       "       0.67410845, 0.6738194 , 0.6737196 , 0.34318227, 0.34587035,\n",
       "       0.34817666, 0.35012457, 0.35054117, 0.35047978, 0.350451  ,\n",
       "       1.2750769 , 1.2901416 , 1.3054883 , 1.3166468 , 1.3195242 ,\n",
       "       1.3198981 , 1.321071  , 1.6872104 , 1.718874  , 1.7407217 ,\n",
       "       1.7544147 , 1.7538072 , 1.754903  , 1.7555734 , 1.0645627 ,\n",
       "       1.0808407 , 1.0943383 , 1.1050189 , 1.108702  , 1.1090887 ,\n",
       "       1.1103522 , 1.1383997 , 1.1612747 , 1.1764699 , 1.185301  ,\n",
       "       1.1884277 , 1.1886936 , 1.1885089 , 1.1204972 , 1.1380943 ,\n",
       "       1.1530114 , 1.1622294 , 1.1640105 , 1.1634798 , 1.1637049 ,\n",
       "       2.5534413 , 2.5957696 , 2.6366558 , 2.6665103 , 2.6789808 ,\n",
       "       2.6574004 , 2.6571386 , 0.36652496, 0.37039772, 0.3730529 ,\n",
       "       0.37515438, 0.37561083, 0.3756809 , 0.37582895, 1.3016956 ,\n",
       "       1.3336097 , 1.3674006 , 1.3919092 , 1.3937943 , 1.3940661 ,\n",
       "       1.3945475 , 1.2022281 , 1.2111396 , 1.2224478 , 1.2301127 ,\n",
       "       1.2319593 , 1.2320927 , 1.231564  , 0.9923399 , 1.0449235 ,\n",
       "       1.0736887 , 1.0867121 , 1.093029  , 1.0953329 , 1.110036  ,\n",
       "       2.1737595 , 2.1944823 , 2.2097178 , 2.2217102 , 2.2245963 ,\n",
       "       2.2247763 , 2.225369  , 2.6715991 , 2.7014406 , 2.72456   ,\n",
       "       2.7423291 , 2.745791  , 2.7459033 , 2.7477193 , 0.2948723 ,\n",
       "       0.3001681 , 0.30443725, 0.30790347, 0.30909765, 0.30933702,\n",
       "       0.3094875 , 0.9154948 , 0.9256278 , 0.93713444, 0.94633734,\n",
       "       0.94924533, 0.94880927, 0.94931084, 0.45768166, 0.46728003,\n",
       "       0.4749748 , 0.48091272, 0.48294336, 0.48335105, 0.4836677 ,\n",
       "       1.6276371 , 1.6410124 , 1.659388  , 1.671697  , 1.6705317 ,\n",
       "       1.6681592 , 1.6671596 , 1.544133  , 1.5606544 , 1.5766162 ,\n",
       "       1.5843651 , 1.5882711 , 1.5862166 , 1.5863636 , 2.3510756 ,\n",
       "       2.3979619 , 2.4458344 , 2.4880786 , 2.50122   , 2.5018024 ,\n",
       "       2.5027294 , 1.0356114 , 1.0431601 , 1.047512  , 1.0506182 ,\n",
       "       1.0496976 , 1.0502163 , 1.0498215 , 0.39969254, 0.40924144,\n",
       "       0.41628188, 0.41919926, 0.41974232, 0.42056796, 0.42088905,\n",
       "       0.90978473, 0.9249964 , 0.9376319 , 0.94669443, 0.9487351 ,\n",
       "       0.94884956, 0.9489401 ], dtype=float32)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_inverse = np.array(preds_inverse).flatten()\n",
    "preds_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.06  ],\n",
       "        [1.03  ],\n",
       "        [1.02  ],\n",
       "        [1.12  ],\n",
       "        [1.13  ],\n",
       "        [1.16  ],\n",
       "        [1.03  ]],\n",
       "\n",
       "       [[1.71  ],\n",
       "        [1.3   ],\n",
       "        [2.16  ],\n",
       "        [3.31  ],\n",
       "        [1.87  ],\n",
       "        [1.45  ],\n",
       "        [2.53  ]],\n",
       "\n",
       "       [[1.84  ],\n",
       "        [1.88  ],\n",
       "        [1.74  ],\n",
       "        [2.01  ],\n",
       "        [2.07  ],\n",
       "        [2.08  ],\n",
       "        [2.03  ]],\n",
       "\n",
       "       [[0.75  ],\n",
       "        [1.13  ],\n",
       "        [0.54  ],\n",
       "        [0.72  ],\n",
       "        [0.65  ],\n",
       "        [0.61  ],\n",
       "        [0.7   ]],\n",
       "\n",
       "       [[1.03  ],\n",
       "        [1.06  ],\n",
       "        [0.99  ],\n",
       "        [0.94  ],\n",
       "        [1.01  ],\n",
       "        [1.07  ],\n",
       "        [1.01  ]],\n",
       "\n",
       "       [[6.66  ],\n",
       "        [7.52  ],\n",
       "        [6.08  ],\n",
       "        [6.61  ],\n",
       "        [6.41  ],\n",
       "        [7.15  ],\n",
       "        [8.34  ]],\n",
       "\n",
       "       [[0.9   ],\n",
       "        [0.76  ],\n",
       "        [0.05  ],\n",
       "        [0.44  ],\n",
       "        [0.57  ],\n",
       "        [0.58  ],\n",
       "        [0.72  ]],\n",
       "\n",
       "       [[1.05  ],\n",
       "        [1.06  ],\n",
       "        [1.54  ],\n",
       "        [1.11  ],\n",
       "        [1.13  ],\n",
       "        [1.29  ],\n",
       "        [1.21  ]],\n",
       "\n",
       "       [[1.87  ],\n",
       "        [1.95  ],\n",
       "        [1.88  ],\n",
       "        [2.11  ],\n",
       "        [1.94  ],\n",
       "        [1.96  ],\n",
       "        [1.85  ]],\n",
       "\n",
       "       [[1.74  ],\n",
       "        [1.45  ],\n",
       "        [1.77  ],\n",
       "        [1.62  ],\n",
       "        [1.76  ],\n",
       "        [1.9   ],\n",
       "        [1.52  ]],\n",
       "\n",
       "       [[3.16  ],\n",
       "        [2.99  ],\n",
       "        [1.87  ],\n",
       "        [2.9   ],\n",
       "        [3.07  ],\n",
       "        [3.17  ],\n",
       "        [2.25  ]],\n",
       "\n",
       "       [[5.98  ],\n",
       "        [6.28  ],\n",
       "        [6.04  ],\n",
       "        [5.71  ],\n",
       "        [5.81  ],\n",
       "        [4.79  ],\n",
       "        [4.69  ]],\n",
       "\n",
       "       [[1.13  ],\n",
       "        [1.15  ],\n",
       "        [0.78  ],\n",
       "        [1.39  ],\n",
       "        [1.33  ],\n",
       "        [1.5   ],\n",
       "        [1.43  ]],\n",
       "\n",
       "       [[0.66  ],\n",
       "        [0.66  ],\n",
       "        [0.44  ],\n",
       "        [0.56  ],\n",
       "        [0.65  ],\n",
       "        [0.7   ],\n",
       "        [0.64  ]],\n",
       "\n",
       "       [[0.44  ],\n",
       "        [0.45  ],\n",
       "        [0.48  ],\n",
       "        [0.4   ],\n",
       "        [0.4   ],\n",
       "        [0.48  ],\n",
       "        [0.48  ]],\n",
       "\n",
       "       [[1.66  ],\n",
       "        [1.78  ],\n",
       "        [1.55  ],\n",
       "        [1.78  ],\n",
       "        [1.89  ],\n",
       "        [2.15  ],\n",
       "        [1.96  ]],\n",
       "\n",
       "       [[2.04  ],\n",
       "        [1.69  ],\n",
       "        [1.63  ],\n",
       "        [2.07  ],\n",
       "        [2.07  ],\n",
       "        [2.15  ],\n",
       "        [2.    ]],\n",
       "\n",
       "       [[1.07  ],\n",
       "        [1.02  ],\n",
       "        [1.21  ],\n",
       "        [1.39  ],\n",
       "        [1.27  ],\n",
       "        [1.42  ],\n",
       "        [1.32  ]],\n",
       "\n",
       "       [[1.1   ],\n",
       "        [0.78  ],\n",
       "        [1.09  ],\n",
       "        [1.52  ],\n",
       "        [1.18  ],\n",
       "        [0.77  ],\n",
       "        [1.19  ]],\n",
       "\n",
       "       [[1.3   ],\n",
       "        [1.35  ],\n",
       "        [1.31  ],\n",
       "        [1.55  ],\n",
       "        [1.54  ],\n",
       "        [1.68  ],\n",
       "        [1.67  ]],\n",
       "\n",
       "       [[3.26  ],\n",
       "        [3.53  ],\n",
       "        [3.76  ],\n",
       "        [3.35  ],\n",
       "        [3.61  ],\n",
       "        [3.47  ],\n",
       "        [3.6   ]],\n",
       "\n",
       "       [[0.3933],\n",
       "        [0.3833],\n",
       "        [0.42  ],\n",
       "        [0.4067],\n",
       "        [0.4067],\n",
       "        [0.4233],\n",
       "        [0.43  ]],\n",
       "\n",
       "       [[1.68  ],\n",
       "        [1.58  ],\n",
       "        [1.67  ],\n",
       "        [1.48  ],\n",
       "        [1.75  ],\n",
       "        [1.69  ],\n",
       "        [1.8   ]],\n",
       "\n",
       "       [[1.35  ],\n",
       "        [1.67  ],\n",
       "        [1.11  ],\n",
       "        [1.43  ],\n",
       "        [1.52  ],\n",
       "        [1.23  ],\n",
       "        [1.41  ]],\n",
       "\n",
       "       [[1.15  ],\n",
       "        [1.33  ],\n",
       "        [1.24  ],\n",
       "        [1.33  ],\n",
       "        [1.42  ],\n",
       "        [1.33  ],\n",
       "        [1.18  ]],\n",
       "\n",
       "       [[2.72  ],\n",
       "        [2.82  ],\n",
       "        [2.75  ],\n",
       "        [2.61  ],\n",
       "        [2.88  ],\n",
       "        [2.94  ],\n",
       "        [2.97  ]],\n",
       "\n",
       "       [[3.01  ],\n",
       "        [3.15  ],\n",
       "        [2.44  ],\n",
       "        [3.    ],\n",
       "        [3.14  ],\n",
       "        [3.22  ],\n",
       "        [2.33  ]],\n",
       "\n",
       "       [[0.32  ],\n",
       "        [0.37  ],\n",
       "        [0.37  ],\n",
       "        [0.37  ],\n",
       "        [0.37  ],\n",
       "        [0.39  ],\n",
       "        [0.39  ]],\n",
       "\n",
       "       [[1.0333],\n",
       "        [1.12  ],\n",
       "        [1.1933],\n",
       "        [1.2067],\n",
       "        [1.2   ],\n",
       "        [1.3333],\n",
       "        [1.26  ]],\n",
       "\n",
       "       [[0.6   ],\n",
       "        [0.65  ],\n",
       "        [0.65  ],\n",
       "        [0.69  ],\n",
       "        [0.66  ],\n",
       "        [0.74  ],\n",
       "        [0.63  ]],\n",
       "\n",
       "       [[2.05  ],\n",
       "        [1.87  ],\n",
       "        [1.68  ],\n",
       "        [1.24  ],\n",
       "        [1.45  ],\n",
       "        [1.51  ],\n",
       "        [1.98  ]],\n",
       "\n",
       "       [[1.77  ],\n",
       "        [2.3   ],\n",
       "        [1.41  ],\n",
       "        [2.09  ],\n",
       "        [2.03  ],\n",
       "        [2.13  ],\n",
       "        [2.03  ]],\n",
       "\n",
       "       [[1.81  ],\n",
       "        [2.54  ],\n",
       "        [2.13  ],\n",
       "        [2.83  ],\n",
       "        [2.02  ],\n",
       "        [1.43  ],\n",
       "        [3.32  ]],\n",
       "\n",
       "       [[1.08  ],\n",
       "        [1.16  ],\n",
       "        [1.09  ],\n",
       "        [1.03  ],\n",
       "        [1.3   ],\n",
       "        [1.07  ],\n",
       "        [0.93  ]],\n",
       "\n",
       "       [[0.4311],\n",
       "        [0.4978],\n",
       "        [0.4178],\n",
       "        [0.44  ],\n",
       "        [0.5467],\n",
       "        [0.5667],\n",
       "        [0.4733]],\n",
       "\n",
       "       [[0.89  ],\n",
       "        [1.04  ],\n",
       "        [1.08  ],\n",
       "        [1.04  ],\n",
       "        [0.99  ],\n",
       "        [1.17  ],\n",
       "        [0.97  ]]])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_true_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8109635 ],\n",
       "       [0.835523  ],\n",
       "       [0.85357153],\n",
       "       [0.8636559 ],\n",
       "       [0.8666363 ],\n",
       "       [0.86751807],\n",
       "       [0.8691671 ],\n",
       "       [0.79357255],\n",
       "       [0.81720984],\n",
       "       [0.8294585 ],\n",
       "       [0.841714  ],\n",
       "       [0.8467773 ],\n",
       "       [0.8485459 ],\n",
       "       [0.8479148 ],\n",
       "       [0.81171834],\n",
       "       [0.83392906],\n",
       "       [0.8514464 ],\n",
       "       [0.86309934],\n",
       "       [0.866915  ],\n",
       "       [0.86701393],\n",
       "       [0.8679242 ],\n",
       "       [0.7897916 ],\n",
       "       [0.8193089 ],\n",
       "       [0.8392062 ],\n",
       "       [0.84433484],\n",
       "       [0.84291196],\n",
       "       [0.83224976],\n",
       "       [0.81523776],\n",
       "       [0.8182007 ],\n",
       "       [0.8323314 ],\n",
       "       [0.84340215],\n",
       "       [0.8502269 ],\n",
       "       [0.8513988 ],\n",
       "       [0.8512422 ],\n",
       "       [0.85123813],\n",
       "       [0.81898284],\n",
       "       [0.83466375],\n",
       "       [0.846007  ],\n",
       "       [0.8532131 ],\n",
       "       [0.8538065 ],\n",
       "       [0.85411334],\n",
       "       [0.85442924],\n",
       "       [0.792963  ],\n",
       "       [0.8088479 ],\n",
       "       [0.82333124],\n",
       "       [0.8341212 ],\n",
       "       [0.83517206],\n",
       "       [0.833508  ],\n",
       "       [0.8335539 ],\n",
       "       [0.77508867],\n",
       "       [0.7884027 ],\n",
       "       [0.8000498 ],\n",
       "       [0.8187449 ],\n",
       "       [0.8185203 ],\n",
       "       [0.81453955],\n",
       "       [0.81595206],\n",
       "       [0.82102597],\n",
       "       [0.8361589 ],\n",
       "       [0.8365865 ],\n",
       "       [0.8473836 ],\n",
       "       [0.85174274],\n",
       "       [0.85232306],\n",
       "       [0.85216343],\n",
       "       [0.7781205 ],\n",
       "       [0.8007974 ],\n",
       "       [0.8180444 ],\n",
       "       [0.8344296 ],\n",
       "       [0.83750165],\n",
       "       [0.8360419 ],\n",
       "       [0.83801734],\n",
       "       [0.8157449 ],\n",
       "       [0.82930624],\n",
       "       [0.84029007],\n",
       "       [0.84632874],\n",
       "       [0.8476584 ],\n",
       "       [0.8464222 ],\n",
       "       [0.8471606 ],\n",
       "       [0.81756246],\n",
       "       [0.82635224],\n",
       "       [0.8342265 ],\n",
       "       [0.84137094],\n",
       "       [0.84285617],\n",
       "       [0.8426267 ],\n",
       "       [0.84369004],\n",
       "       [0.7990293 ],\n",
       "       [0.81040525],\n",
       "       [0.8241626 ],\n",
       "       [0.8283938 ],\n",
       "       [0.8325511 ],\n",
       "       [0.8338555 ],\n",
       "       [0.8358643 ],\n",
       "       [0.79677963],\n",
       "       [0.8082421 ],\n",
       "       [0.819422  ],\n",
       "       [0.82747126],\n",
       "       [0.82533896],\n",
       "       [0.82443583],\n",
       "       [0.8241236 ],\n",
       "       [0.8062225 ],\n",
       "       [0.8203702 ],\n",
       "       [0.8325087 ],\n",
       "       [0.8427609 ],\n",
       "       [0.84495354],\n",
       "       [0.84463036],\n",
       "       [0.84447885],\n",
       "       [0.79496515],\n",
       "       [0.8086978 ],\n",
       "       [0.8226876 ],\n",
       "       [0.8328594 ],\n",
       "       [0.83548236],\n",
       "       [0.8358232 ],\n",
       "       [0.83689237],\n",
       "       [0.79069835],\n",
       "       [0.81061256],\n",
       "       [0.82435334],\n",
       "       [0.83296525],\n",
       "       [0.8325832 ],\n",
       "       [0.83327234],\n",
       "       [0.833694  ],\n",
       "       [0.78820896],\n",
       "       [0.8049904 ],\n",
       "       [0.8189055 ],\n",
       "       [0.82991636],\n",
       "       [0.8337133 ],\n",
       "       [0.83411205],\n",
       "       [0.8354145 ],\n",
       "       [0.78806996],\n",
       "       [0.8081356 ],\n",
       "       [0.8214648 ],\n",
       "       [0.82921135],\n",
       "       [0.8319541 ],\n",
       "       [0.8321874 ],\n",
       "       [0.8320253 ],\n",
       "       [0.7821808 ],\n",
       "       [0.80240726],\n",
       "       [0.8195534 ],\n",
       "       [0.8301488 ],\n",
       "       [0.832196  ],\n",
       "       [0.831586  ],\n",
       "       [0.8318447 ],\n",
       "       [0.7882507 ],\n",
       "       [0.8102968 ],\n",
       "       [0.8315916 ],\n",
       "       [0.8471408 ],\n",
       "       [0.8536359 ],\n",
       "       [0.842396  ],\n",
       "       [0.84225976],\n",
       "       [0.7772618 ],\n",
       "       [0.79570353],\n",
       "       [0.8083471 ],\n",
       "       [0.81835425],\n",
       "       [0.8205278 ],\n",
       "       [0.82086146],\n",
       "       [0.82156646],\n",
       "       [0.7178359 ],\n",
       "       [0.754102  ],\n",
       "       [0.79250073],\n",
       "       [0.82035136],\n",
       "       [0.82249355],\n",
       "       [0.8228024 ],\n",
       "       [0.82334936],\n",
       "       [0.7918191 ],\n",
       "       [0.80119956],\n",
       "       [0.81310296],\n",
       "       [0.8211713 ],\n",
       "       [0.8231151 ],\n",
       "       [0.82325554],\n",
       "       [0.82269907],\n",
       "       [0.7269174 ],\n",
       "       [0.77515924],\n",
       "       [0.8015493 ],\n",
       "       [0.8134974 ],\n",
       "       [0.81929266],\n",
       "       [0.82140636],\n",
       "       [0.8348955 ],\n",
       "       [0.7998121 ],\n",
       "       [0.8160018 ],\n",
       "       [0.8279046 ],\n",
       "       [0.8372736 ],\n",
       "       [0.8395283 ],\n",
       "       [0.839669  ],\n",
       "       [0.840132  ],\n",
       "       [0.79681545],\n",
       "       [0.8116621 ],\n",
       "       [0.8231642 ],\n",
       "       [0.83200455],\n",
       "       [0.8337269 ],\n",
       "       [0.8337828 ],\n",
       "       [0.8346863 ],\n",
       "       [0.7899107 ],\n",
       "       [0.80699384],\n",
       "       [0.82076526],\n",
       "       [0.8319466 ],\n",
       "       [0.83579886],\n",
       "       [0.836571  ],\n",
       "       [0.8370564 ],\n",
       "       [0.79618424],\n",
       "       [0.80824745],\n",
       "       [0.8219458 ],\n",
       "       [0.8329016 ],\n",
       "       [0.83636355],\n",
       "       [0.8358444 ],\n",
       "       [0.8364415 ],\n",
       "       [0.7801944 ],\n",
       "       [0.80304766],\n",
       "       [0.8213686 ],\n",
       "       [0.83550644],\n",
       "       [0.8403413 ],\n",
       "       [0.84131205],\n",
       "       [0.84206593],\n",
       "       [0.795593  ],\n",
       "       [0.8091035 ],\n",
       "       [0.8276646 ],\n",
       "       [0.840098  ],\n",
       "       [0.83892095],\n",
       "       [0.8365245 ],\n",
       "       [0.83551466],\n",
       "       [0.7869178 ],\n",
       "       [0.80250406],\n",
       "       [0.81756234],\n",
       "       [0.82487273],\n",
       "       [0.8285576 ],\n",
       "       [0.8266194 ],\n",
       "       [0.826758  ],\n",
       "       [0.79344904],\n",
       "       [0.8048569 ],\n",
       "       [0.8165047 ],\n",
       "       [0.82678306],\n",
       "       [0.8299805 ],\n",
       "       [0.83012223],\n",
       "       [0.8303478 ],\n",
       "       [0.7990748 ],\n",
       "       [0.8170476 ],\n",
       "       [0.8274095 ],\n",
       "       [0.83480513],\n",
       "       [0.83261347],\n",
       "       [0.83384836],\n",
       "       [0.83290815],\n",
       "       [0.7798784 ],\n",
       "       [0.8093139 ],\n",
       "       [0.8310169 ],\n",
       "       [0.84001005],\n",
       "       [0.8416841 ],\n",
       "       [0.8442292 ],\n",
       "       [0.845219  ],\n",
       "       [0.8062201 ],\n",
       "       [0.82257676],\n",
       "       [0.8361634 ],\n",
       "       [0.84590805],\n",
       "       [0.84810233],\n",
       "       [0.84822536],\n",
       "       [0.84832275]], dtype=float32)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.flatten()\n",
    "x_indices = np.arange(len(y_true_inverse))\n",
    "preds = np.array(preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)//len(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_indices = [[\"2022-Q2\", \"2022-Q3\" ,\"2022-Q4\", '2023-Q1', '2023-Q2', '2023-Q3']] * len(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3'],\n",
       " ['2022-Q2', '2022-Q3', '2022-Q4', '2023-Q1', '2023-Q2', '2023-Q3']]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "All Companies True: EPS - PENDS",
         "type": "scatter",
         "x": [
          "2022-Q2 AFL",
          "2022-Q3 AFL",
          "2022-Q4 AFL",
          "2023-Q1 AFL",
          "2023-Q2 AFL",
          "2023-Q3 AFL",
          "2022-Q2 AON",
          "2022-Q3 AON",
          "2022-Q4 AON",
          "2023-Q1 AON",
          "2023-Q2 AON",
          "2023-Q3 AON",
          "2022-Q2 AXP",
          "2022-Q3 AXP",
          "2022-Q4 AXP",
          "2023-Q1 AXP",
          "2023-Q2 AXP",
          "2023-Q3 AXP",
          "2022-Q2 BEN",
          "2022-Q3 BEN",
          "2022-Q4 BEN",
          "2023-Q1 BEN",
          "2023-Q2 BEN",
          "2023-Q3 BEN",
          "2022-Q2 BK",
          "2022-Q3 BK",
          "2022-Q4 BK",
          "2023-Q1 BK",
          "2023-Q2 BK",
          "2023-Q3 BK",
          "2022-Q2 BLK",
          "2022-Q3 BLK",
          "2022-Q4 BLK",
          "2023-Q1 BLK",
          "2023-Q2 BLK",
          "2023-Q3 BLK",
          "2022-Q2 BX",
          "2022-Q3 BX",
          "2022-Q4 BX",
          "2023-Q1 BX",
          "2023-Q2 BX",
          "2023-Q3 BX",
          "2022-Q2 CBOE",
          "2022-Q3 CBOE",
          "2022-Q4 CBOE",
          "2023-Q1 CBOE",
          "2023-Q2 CBOE",
          "2023-Q3 CBOE",
          "2022-Q2 CMA",
          "2022-Q3 CMA",
          "2022-Q4 CMA",
          "2023-Q1 CMA",
          "2023-Q2 CMA",
          "2023-Q3 CMA",
          "2022-Q2 CME",
          "2022-Q3 CME",
          "2022-Q4 CME",
          "2023-Q1 CME",
          "2023-Q2 CME",
          "2023-Q3 CME",
          "2022-Q2 COF",
          "2022-Q3 COF",
          "2022-Q4 COF",
          "2023-Q1 COF",
          "2023-Q2 COF",
          "2023-Q3 COF",
          "2022-Q2 GS",
          "2022-Q3 GS",
          "2022-Q4 GS",
          "2023-Q1 GS",
          "2023-Q2 GS",
          "2023-Q3 GS",
          "2022-Q2 HIG",
          "2022-Q3 HIG",
          "2022-Q4 HIG",
          "2023-Q1 HIG",
          "2023-Q2 HIG",
          "2023-Q3 HIG",
          "2022-Q2 IVZ",
          "2022-Q3 IVZ",
          "2022-Q4 IVZ",
          "2023-Q1 IVZ",
          "2023-Q2 IVZ",
          "2023-Q3 IVZ",
          "2022-Q2 KEY",
          "2022-Q3 KEY",
          "2022-Q4 KEY",
          "2023-Q1 KEY",
          "2023-Q2 KEY",
          "2023-Q3 KEY",
          "2022-Q2 MA",
          "2022-Q3 MA",
          "2022-Q4 MA",
          "2023-Q1 MA",
          "2023-Q2 MA",
          "2023-Q3 MA",
          "2022-Q2 MCO",
          "2022-Q3 MCO",
          "2022-Q4 MCO",
          "2023-Q1 MCO",
          "2023-Q2 MCO",
          "2023-Q3 MCO",
          "2022-Q2 MKTX",
          "2022-Q3 MKTX",
          "2022-Q4 MKTX",
          "2023-Q1 MKTX",
          "2023-Q2 MKTX",
          "2023-Q3 MKTX",
          "2022-Q2 MMC",
          "2022-Q3 MMC",
          "2022-Q4 MMC",
          "2023-Q1 MMC",
          "2023-Q2 MMC",
          "2023-Q3 MMC",
          "2022-Q2 MSCI",
          "2022-Q3 MSCI",
          "2022-Q4 MSCI",
          "2023-Q1 MSCI",
          "2023-Q2 MSCI",
          "2023-Q3 MSCI",
          "2022-Q2 MTB",
          "2022-Q3 MTB",
          "2022-Q4 MTB",
          "2023-Q1 MTB",
          "2023-Q2 MTB",
          "2023-Q3 MTB",
          "2022-Q2 NDAQ",
          "2022-Q3 NDAQ",
          "2022-Q4 NDAQ",
          "2023-Q1 NDAQ",
          "2023-Q2 NDAQ",
          "2023-Q3 NDAQ",
          "2022-Q2 NTRS",
          "2022-Q3 NTRS",
          "2022-Q4 NTRS",
          "2023-Q1 NTRS",
          "2023-Q2 NTRS",
          "2023-Q3 NTRS",
          "2022-Q2 PFG",
          "2022-Q3 PFG",
          "2022-Q4 PFG",
          "2023-Q1 PFG",
          "2023-Q2 PFG",
          "2023-Q3 PFG",
          "2022-Q2 PGR",
          "2022-Q3 PGR",
          "2022-Q4 PGR",
          "2023-Q1 PGR",
          "2023-Q2 PGR",
          "2023-Q3 PGR",
          "2022-Q2 PNC",
          "2022-Q3 PNC",
          "2022-Q4 PNC",
          "2023-Q1 PNC",
          "2023-Q2 PNC",
          "2023-Q3 PNC",
          "2022-Q2 PRU",
          "2022-Q3 PRU",
          "2022-Q4 PRU",
          "2023-Q1 PRU",
          "2023-Q2 PRU",
          "2023-Q3 PRU",
          "2022-Q2 RF",
          "2022-Q3 RF",
          "2022-Q4 RF",
          "2023-Q1 RF",
          "2023-Q2 RF",
          "2023-Q3 RF",
          "2022-Q2 RJF",
          "2022-Q3 RJF",
          "2022-Q4 RJF",
          "2023-Q1 RJF",
          "2023-Q2 RJF",
          "2023-Q3 RJF",
          "2022-Q2 SCHW",
          "2022-Q3 SCHW",
          "2022-Q4 SCHW",
          "2023-Q1 SCHW",
          "2023-Q2 SCHW",
          "2023-Q3 SCHW",
          "2022-Q2 STT",
          "2022-Q3 STT",
          "2022-Q4 STT",
          "2023-Q1 STT",
          "2023-Q2 STT",
          "2023-Q3 STT",
          "2022-Q2 TROW",
          "2022-Q3 TROW",
          "2022-Q4 TROW",
          "2023-Q1 TROW",
          "2023-Q2 TROW",
          "2023-Q3 TROW",
          "2022-Q2 TRV",
          "2022-Q3 TRV",
          "2022-Q4 TRV",
          "2023-Q1 TRV",
          "2023-Q2 TRV",
          "2023-Q3 TRV",
          "2022-Q2 WFC",
          "2022-Q3 WFC",
          "2022-Q4 WFC",
          "2023-Q1 WFC",
          "2023-Q2 WFC",
          "2023-Q3 WFC",
          "2022-Q2 WRB",
          "2022-Q3 WRB",
          "2022-Q4 WRB",
          "2023-Q1 WRB",
          "2023-Q2 WRB",
          "2023-Q3 WRB",
          "2022-Q2 ZION",
          "2022-Q3 ZION",
          "2022-Q4 ZION",
          "2023-Q1 ZION",
          "2023-Q2 ZION",
          "2023-Q3 ZION"
         ],
         "y": [
          1.06,
          1.03,
          1.02,
          1.12,
          1.13,
          1.16,
          1.03,
          1.7099999999999997,
          1.3,
          2.16,
          3.31,
          1.87,
          1.45,
          2.53,
          1.84,
          1.88,
          1.74,
          2.01,
          2.07,
          2.08,
          2.03,
          0.7500000000000001,
          1.13,
          0.54,
          0.72,
          0.65,
          0.61,
          0.7000000000000001,
          1.03,
          1.06,
          0.99,
          0.94,
          1.01,
          1.07,
          1.01,
          6.66,
          7.519999999999999,
          6.08,
          6.61,
          6.41,
          7.15,
          8.34,
          0.9000000000000002,
          0.76,
          0.049999999999999906,
          0.44,
          0.5699999999999998,
          0.5799999999999998,
          0.72,
          1.05,
          1.06,
          1.54,
          1.11,
          1.13,
          1.2899999999999998,
          1.21,
          1.87,
          1.95,
          1.88,
          2.11,
          1.9399999999999997,
          1.9599999999999997,
          1.85,
          1.74,
          1.45,
          1.77,
          1.62,
          1.7600000000000002,
          1.9,
          1.52,
          3.16,
          2.99,
          1.87,
          2.9,
          3.0700000000000007,
          3.17,
          2.2500000000000004,
          5.980000000000001,
          6.28,
          6.04,
          5.71,
          5.809999999999999,
          4.79,
          4.6899999999999995,
          1.13,
          1.15,
          0.7800000000000001,
          1.39,
          1.33,
          1.5,
          1.43,
          0.66,
          0.66,
          0.44,
          0.56,
          0.65,
          0.7,
          0.64,
          0.44,
          0.45,
          0.48000000000000004,
          0.4,
          0.4,
          0.48000000000000004,
          0.48000000000000004,
          1.6600000000000001,
          1.7800000000000002,
          1.55,
          1.7800000000000002,
          1.89,
          2.15,
          1.9600000000000004,
          2.04,
          1.69,
          1.6300000000000001,
          2.07,
          2.07,
          2.15,
          2,
          1.07,
          1.02,
          1.21,
          1.39,
          1.27,
          1.42,
          1.32,
          1.1000000000000003,
          0.7799999999999999,
          1.0900000000000003,
          1.52,
          1.18,
          0.77,
          1.1900000000000002,
          1.3,
          1.3499999999999999,
          1.3099999999999998,
          1.55,
          1.54,
          1.68,
          1.67,
          3.26,
          3.53,
          3.76,
          3.35,
          3.61,
          3.4700000000000006,
          3.6,
          0.3933,
          0.3833,
          0.42,
          0.4067,
          0.4067,
          0.42329999999999995,
          0.43,
          1.68,
          1.58,
          1.6700000000000002,
          1.48,
          1.75,
          1.69,
          1.7999999999999996,
          1.35,
          1.67,
          1.11,
          1.4300000000000002,
          1.52,
          1.23,
          1.41,
          1.15,
          1.33,
          1.24,
          1.33,
          1.4199999999999997,
          1.33,
          1.18,
          2.72,
          2.8200000000000003,
          2.75,
          2.6099999999999994,
          2.88,
          2.94,
          2.9700000000000006,
          3.01,
          3.1500000000000004,
          2.4400000000000004,
          3.0000000000000004,
          3.14,
          3.22,
          2.33,
          0.32,
          0.37,
          0.37,
          0.37,
          0.37,
          0.39,
          0.39,
          1.0333,
          1.12,
          1.1933,
          1.2067,
          1.2,
          1.3333,
          1.2599999999999998,
          0.6,
          0.65,
          0.65,
          0.69,
          0.66,
          0.74,
          0.63,
          2.05,
          1.8700000000000003,
          1.68,
          1.24,
          1.4500000000000002,
          1.51,
          1.9799999999999998,
          1.77,
          2.2999999999999994,
          1.41,
          2.09,
          2.03,
          2.1299999999999994,
          2.03,
          1.8100000000000003,
          2.54,
          2.13,
          2.83,
          2.02,
          1.4300000000000002,
          3.3199999999999994,
          1.08,
          1.16,
          1.09,
          1.0300000000000002,
          1.3,
          1.07,
          0.93,
          0.4310999999999999,
          0.49779999999999996,
          0.4178,
          0.44,
          0.5466999999999999,
          0.5666999999999999,
          0.4733,
          0.89,
          1.04,
          1.08,
          1.04,
          0.99,
          1.17,
          0.97
         ]
        },
        {
         "mode": "lines+markers",
         "name": "All Companies Preds: EPS - PENDS",
         "type": "scatter",
         "x": [
          "2022-Q2 AFL",
          "2022-Q3 AFL",
          "2022-Q4 AFL",
          "2023-Q1 AFL",
          "2023-Q2 AFL",
          "2023-Q3 AFL",
          "2022-Q2 AON",
          "2022-Q3 AON",
          "2022-Q4 AON",
          "2023-Q1 AON",
          "2023-Q2 AON",
          "2023-Q3 AON",
          "2022-Q2 AXP",
          "2022-Q3 AXP",
          "2022-Q4 AXP",
          "2023-Q1 AXP",
          "2023-Q2 AXP",
          "2023-Q3 AXP",
          "2022-Q2 BEN",
          "2022-Q3 BEN",
          "2022-Q4 BEN",
          "2023-Q1 BEN",
          "2023-Q2 BEN",
          "2023-Q3 BEN",
          "2022-Q2 BK",
          "2022-Q3 BK",
          "2022-Q4 BK",
          "2023-Q1 BK",
          "2023-Q2 BK",
          "2023-Q3 BK",
          "2022-Q2 BLK",
          "2022-Q3 BLK",
          "2022-Q4 BLK",
          "2023-Q1 BLK",
          "2023-Q2 BLK",
          "2023-Q3 BLK",
          "2022-Q2 BX",
          "2022-Q3 BX",
          "2022-Q4 BX",
          "2023-Q1 BX",
          "2023-Q2 BX",
          "2023-Q3 BX",
          "2022-Q2 CBOE",
          "2022-Q3 CBOE",
          "2022-Q4 CBOE",
          "2023-Q1 CBOE",
          "2023-Q2 CBOE",
          "2023-Q3 CBOE",
          "2022-Q2 CMA",
          "2022-Q3 CMA",
          "2022-Q4 CMA",
          "2023-Q1 CMA",
          "2023-Q2 CMA",
          "2023-Q3 CMA",
          "2022-Q2 CME",
          "2022-Q3 CME",
          "2022-Q4 CME",
          "2023-Q1 CME",
          "2023-Q2 CME",
          "2023-Q3 CME",
          "2022-Q2 COF",
          "2022-Q3 COF",
          "2022-Q4 COF",
          "2023-Q1 COF",
          "2023-Q2 COF",
          "2023-Q3 COF",
          "2022-Q2 GS",
          "2022-Q3 GS",
          "2022-Q4 GS",
          "2023-Q1 GS",
          "2023-Q2 GS",
          "2023-Q3 GS",
          "2022-Q2 HIG",
          "2022-Q3 HIG",
          "2022-Q4 HIG",
          "2023-Q1 HIG",
          "2023-Q2 HIG",
          "2023-Q3 HIG",
          "2022-Q2 IVZ",
          "2022-Q3 IVZ",
          "2022-Q4 IVZ",
          "2023-Q1 IVZ",
          "2023-Q2 IVZ",
          "2023-Q3 IVZ",
          "2022-Q2 KEY",
          "2022-Q3 KEY",
          "2022-Q4 KEY",
          "2023-Q1 KEY",
          "2023-Q2 KEY",
          "2023-Q3 KEY",
          "2022-Q2 MA",
          "2022-Q3 MA",
          "2022-Q4 MA",
          "2023-Q1 MA",
          "2023-Q2 MA",
          "2023-Q3 MA",
          "2022-Q2 MCO",
          "2022-Q3 MCO",
          "2022-Q4 MCO",
          "2023-Q1 MCO",
          "2023-Q2 MCO",
          "2023-Q3 MCO",
          "2022-Q2 MKTX",
          "2022-Q3 MKTX",
          "2022-Q4 MKTX",
          "2023-Q1 MKTX",
          "2023-Q2 MKTX",
          "2023-Q3 MKTX",
          "2022-Q2 MMC",
          "2022-Q3 MMC",
          "2022-Q4 MMC",
          "2023-Q1 MMC",
          "2023-Q2 MMC",
          "2023-Q3 MMC",
          "2022-Q2 MSCI",
          "2022-Q3 MSCI",
          "2022-Q4 MSCI",
          "2023-Q1 MSCI",
          "2023-Q2 MSCI",
          "2023-Q3 MSCI",
          "2022-Q2 MTB",
          "2022-Q3 MTB",
          "2022-Q4 MTB",
          "2023-Q1 MTB",
          "2023-Q2 MTB",
          "2023-Q3 MTB",
          "2022-Q2 NDAQ",
          "2022-Q3 NDAQ",
          "2022-Q4 NDAQ",
          "2023-Q1 NDAQ",
          "2023-Q2 NDAQ",
          "2023-Q3 NDAQ",
          "2022-Q2 NTRS",
          "2022-Q3 NTRS",
          "2022-Q4 NTRS",
          "2023-Q1 NTRS",
          "2023-Q2 NTRS",
          "2023-Q3 NTRS",
          "2022-Q2 PFG",
          "2022-Q3 PFG",
          "2022-Q4 PFG",
          "2023-Q1 PFG",
          "2023-Q2 PFG",
          "2023-Q3 PFG",
          "2022-Q2 PGR",
          "2022-Q3 PGR",
          "2022-Q4 PGR",
          "2023-Q1 PGR",
          "2023-Q2 PGR",
          "2023-Q3 PGR",
          "2022-Q2 PNC",
          "2022-Q3 PNC",
          "2022-Q4 PNC",
          "2023-Q1 PNC",
          "2023-Q2 PNC",
          "2023-Q3 PNC",
          "2022-Q2 PRU",
          "2022-Q3 PRU",
          "2022-Q4 PRU",
          "2023-Q1 PRU",
          "2023-Q2 PRU",
          "2023-Q3 PRU",
          "2022-Q2 RF",
          "2022-Q3 RF",
          "2022-Q4 RF",
          "2023-Q1 RF",
          "2023-Q2 RF",
          "2023-Q3 RF",
          "2022-Q2 RJF",
          "2022-Q3 RJF",
          "2022-Q4 RJF",
          "2023-Q1 RJF",
          "2023-Q2 RJF",
          "2023-Q3 RJF",
          "2022-Q2 SCHW",
          "2022-Q3 SCHW",
          "2022-Q4 SCHW",
          "2023-Q1 SCHW",
          "2023-Q2 SCHW",
          "2023-Q3 SCHW",
          "2022-Q2 STT",
          "2022-Q3 STT",
          "2022-Q4 STT",
          "2023-Q1 STT",
          "2023-Q2 STT",
          "2023-Q3 STT",
          "2022-Q2 TROW",
          "2022-Q3 TROW",
          "2022-Q4 TROW",
          "2023-Q1 TROW",
          "2023-Q2 TROW",
          "2023-Q3 TROW",
          "2022-Q2 TRV",
          "2022-Q3 TRV",
          "2022-Q4 TRV",
          "2023-Q1 TRV",
          "2023-Q2 TRV",
          "2023-Q3 TRV",
          "2022-Q2 WFC",
          "2022-Q3 WFC",
          "2022-Q4 WFC",
          "2023-Q1 WFC",
          "2023-Q2 WFC",
          "2023-Q3 WFC",
          "2022-Q2 WRB",
          "2022-Q3 WRB",
          "2022-Q4 WRB",
          "2023-Q1 WRB",
          "2023-Q2 WRB",
          "2023-Q3 WRB",
          "2022-Q2 ZION",
          "2022-Q3 ZION",
          "2022-Q4 ZION",
          "2023-Q1 ZION",
          "2023-Q2 ZION",
          "2023-Q3 ZION"
         ],
         "y": [
          0.94911128282547,
          0.9583210945129395,
          0.9650893211364746,
          0.9688709378242493,
          0.9699885845184326,
          0.9703192710876465,
          0.9709376692771912,
          2.499345541000366,
          2.5532386302948,
          2.581165313720703,
          2.6091079711914062,
          2.620652437210083,
          2.6246848106384277,
          2.62324595451355,
          1.8702963590621948,
          1.8973934650421143,
          1.918764591217041,
          1.9329811334609985,
          1.9376362562179565,
          1.9377570152282715,
          1.9388675689697266,
          0.9296103715896606,
          0.9423028230667114,
          0.9508586525917053,
          0.95306396484375,
          0.9524521231651306,
          0.9478673934936523,
          0.9405522346496582,
          0.976376473903656,
          0.9859853386878967,
          0.9935134649276733,
          0.9981542825698853,
          0.9989511966705322,
          0.9988446831703186,
          0.9988419413566589,
          5.9994635581970215,
          6.0601487159729,
          6.104046821594238,
          6.131934642791748,
          6.134231090545654,
          6.13541841506958,
          6.136641025543213,
          1.0138964653015137,
          1.0412184000015259,
          1.0661298036575317,
          1.0846885442733765,
          1.0864959955215454,
          1.0836337804794312,
          1.0837128162384033,
          1.150590419769287,
          1.164170742034912,
          1.1760507822036743,
          1.1951197385787964,
          1.1948907375335693,
          1.1908303499221802,
          1.192271113395691,
          1.3580721616744995,
          1.3768370151519775,
          1.3773672580718994,
          1.3907556533813477,
          1.3961609601974487,
          1.3968805074691772,
          1.396682620048523,
          1.5870882272720337,
          1.6149808168411255,
          1.6361945867538452,
          1.656348466873169,
          1.66012704372406,
          1.6583315134048462,
          1.6607613563537598,
          2.2883408069610596,
          2.312751293182373,
          2.33252215385437,
          2.3433916568756104,
          2.345785140991211,
          2.343559980392456,
          2.3448891639709473,
          5.528811931610107,
          5.597284317016602,
          5.658624649047852,
          5.714279651641846,
          5.725849628448486,
          5.724061965942383,
          5.7323455810546875,
          1.0147671699523926,
          1.02921462059021,
          1.0466865301132202,
          1.0520601272583008,
          1.0573399066925049,
          1.0589964389801025,
          1.0615476369857788,
          0.6649695038795471,
          0.6686374545097351,
          0.6722150444984436,
          0.6747907996177673,
          0.6741084456443787,
          0.6738194227218628,
          0.673719584941864,
          0.3431822657585144,
          0.34587034583091736,
          0.34817665815353394,
          0.3501245677471161,
          0.3505411744117737,
          0.35047978162765503,
          0.3504509925842285,
          1.2750768661499023,
          1.2901415824890137,
          1.3054883480072021,
          1.3166468143463135,
          1.3195241689682007,
          1.3198981285095215,
          1.3210710287094116,
          1.6872104406356812,
          1.7188739776611328,
          1.7407217025756836,
          1.754414677619934,
          1.7538071870803833,
          1.754902958869934,
          1.7555733919143677,
          1.0645626783370972,
          1.0808407068252563,
          1.094338297843933,
          1.1050188541412354,
          1.1087019443511963,
          1.109088659286499,
          1.1103521585464478,
          1.1383997201919556,
          1.1612746715545654,
          1.1764699220657349,
          1.1853009462356567,
          1.1884276866912842,
          1.188693642616272,
          1.1885088682174683,
          1.120497226715088,
          1.1380943059921265,
          1.153011441230774,
          1.1622294187545776,
          1.1640105247497559,
          1.1634798049926758,
          1.1637048721313477,
          2.553441286087036,
          2.5957696437835693,
          2.636655807495117,
          2.6665103435516357,
          2.678980827331543,
          2.657400369644165,
          2.6571385860443115,
          0.36652496457099915,
          0.3703977167606354,
          0.3730528950691223,
          0.37515437602996826,
          0.3756108283996582,
          0.3756808936595917,
          0.37582895159721375,
          1.3016955852508545,
          1.333609700202942,
          1.3674006462097168,
          1.3919092416763306,
          1.393794298171997,
          1.3940660953521729,
          1.394547462463379,
          1.20222806930542,
          1.2111395597457886,
          1.2224477529525757,
          1.2301126718521118,
          1.231959342956543,
          1.2320927381515503,
          1.2315640449523926,
          0.9923399090766907,
          1.0449235439300537,
          1.0736887454986572,
          1.086712121963501,
          1.0930290222167969,
          1.0953328609466553,
          1.110036015510559,
          2.1737594604492188,
          2.1944823265075684,
          2.2097177505493164,
          2.221710205078125,
          2.2245962619781494,
          2.224776268005371,
          2.2253689765930176,
          2.6715991497039795,
          2.7014405727386475,
          2.724560022354126,
          2.7423291206359863,
          2.745790958404541,
          2.745903253555298,
          2.7477192878723145,
          0.29487231373786926,
          0.30016809701919556,
          0.3044372498989105,
          0.3079034686088562,
          0.30909764766693115,
          0.3093370199203491,
          0.3094874918460846,
          0.9154947996139526,
          0.9256278276443481,
          0.9371344447135925,
          0.9463373422622681,
          0.9492453336715698,
          0.9488092660903931,
          0.949310839176178,
          0.45768165588378906,
          0.4672800302505493,
          0.4749748110771179,
          0.4809127151966095,
          0.4829433560371399,
          0.48335105180740356,
          0.4836677014827728,
          1.6276371479034424,
          1.64101243019104,
          1.6593879461288452,
          1.6716970205307007,
          1.6705317497253418,
          1.6681592464447021,
          1.6671595573425293,
          1.544132947921753,
          1.5606544017791748,
          1.5766161680221558,
          1.5843651294708252,
          1.588271141052246,
          1.5862165689468384,
          1.5863635540008545,
          2.3510756492614746,
          2.3979618549346924,
          2.4458343982696533,
          2.4880785942077637,
          2.5012199878692627,
          2.501802444458008,
          2.5027294158935547,
          1.0356113910675049,
          1.043160080909729,
          1.0475120544433594,
          1.0506181716918945,
          1.0496976375579834,
          1.0502163171768188,
          1.0498214960098267,
          0.3996925354003906,
          0.4092414379119873,
          0.41628187894821167,
          0.41919925808906555,
          0.41974231600761414,
          0.42056795954704285,
          0.4208890497684479,
          0.909784734249115,
          0.9249963760375977,
          0.9376319050788879,
          0.9466944336891174,
          0.9487351179122925,
          0.9488495588302612,
          0.948940098285675
         ]
        }
       ],
       "layout": {
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Multiple Companies: EPS - PENDS"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "rangemode": "tozero",
         "side": "left",
         "title": {
          "text": "EPS"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trace_true = go.Scatter(x=x_indices, y=y_true_inverse, mode=\"lines+markers\", name=f\"All Stocks True: EPS - PENDS\")\n",
    "# trace_preds = go.Scatter(x=x_indices, y=preds_inverse, mode=\"lines+markers\", name=f\"All Stocks Preds: EPS - PENDS\")\n",
    "\n",
    "\n",
    "stock_symbol = \"Multiple Companies\"\n",
    "\n",
    "# Initialize the figure with layout\n",
    "layout = go.Layout(\n",
    "    title=f\"{stock_symbol}: EPS - PENDS\",\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='EPS', side='left', rangemode='tozero'),\n",
    "    height=600,\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "\n",
    "# Variables to keep track of the extended x-axis and y-axis data\n",
    "extended_x_indices = []\n",
    "all_true_y = []\n",
    "all_preds_y = []\n",
    "\n",
    "# Adjust x_indices and concatenate y data for each company\n",
    "for i, company in enumerate(company_list):\n",
    "    # Extend x_indices with company identifier to differentiate time points between companies\n",
    "    extended_x_indices += [f\"{x} {company}\" for x in x_indices[i]]\n",
    "    all_true_y += list(comp_true_inverse[i].flatten())\n",
    "    all_preds_y += list(comp_preds_inverse[i].flatten())\n",
    "\n",
    "# Add traces for the concatenated true and predicted EPS values\n",
    "trace_true = go.Scatter(x=extended_x_indices, y=all_true_y, mode=\"lines+markers\", name=\"All Companies True: EPS - PENDS\")\n",
    "trace_preds = go.Scatter(x=extended_x_indices, y=all_preds_y, mode=\"lines+markers\", name=\"All Companies Preds: EPS - PENDS\")\n",
    "\n",
    "# Add traces to the figure\n",
    "fig.add_trace(trace_true)\n",
    "fig.add_trace(trace_preds)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: EPS, dtype: float64)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data[pivoted_data[\"OFTIC\"] == \"RCL\"][\"EPS\"].tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "All Stocks: EPS - PENDS",
         "type": "scatter",
         "x": [
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ]
         ],
         "y": [
          1.06,
          1.03,
          1.02,
          1.12,
          1.13,
          1.16,
          1.03,
          1.7099999999999997,
          1.3,
          2.16,
          3.31,
          1.87,
          1.45,
          2.53,
          1.84,
          1.88,
          1.74,
          2.01,
          2.07,
          2.08,
          2.03,
          0.7500000000000001,
          1.13,
          0.54,
          0.72,
          0.65,
          0.61,
          0.7000000000000001,
          1.03,
          1.06,
          0.99,
          0.94,
          1.01,
          1.07,
          1.01,
          6.66,
          7.519999999999999,
          6.08,
          6.61,
          6.41,
          7.15,
          8.34,
          0.9000000000000002,
          0.76,
          0.049999999999999906,
          0.44,
          0.5699999999999998,
          0.5799999999999998,
          0.72,
          1.05,
          1.06,
          1.54,
          1.11,
          1.13,
          1.2899999999999998,
          1.21,
          1.87,
          1.95,
          1.88,
          2.11,
          1.9399999999999997,
          1.9599999999999997,
          1.85,
          1.74,
          1.45,
          1.77,
          1.62,
          1.7600000000000002,
          1.9,
          1.52,
          3.16,
          2.99,
          1.87,
          2.9,
          3.0700000000000007,
          3.17,
          2.2500000000000004,
          5.980000000000001,
          6.28,
          6.04,
          5.71,
          5.809999999999999,
          4.79,
          4.6899999999999995,
          1.13,
          1.15,
          0.7800000000000001,
          1.39,
          1.33,
          1.5,
          1.43,
          0.66,
          0.66,
          0.44,
          0.56,
          0.65,
          0.7,
          0.64,
          0.44,
          0.45,
          0.48000000000000004,
          0.4,
          0.4,
          0.48000000000000004,
          0.48000000000000004,
          1.6600000000000001,
          1.7800000000000002,
          1.55,
          1.7800000000000002,
          1.89,
          2.15,
          1.9600000000000004,
          2.04,
          1.69,
          1.6300000000000001,
          2.07,
          2.07,
          2.15,
          2,
          1.07,
          1.02,
          1.21,
          1.39,
          1.27,
          1.42,
          1.32,
          1.1000000000000003,
          0.7799999999999999,
          1.0900000000000003,
          1.52,
          1.18,
          0.77,
          1.1900000000000002,
          1.3,
          1.3499999999999999,
          1.3099999999999998,
          1.55,
          1.54,
          1.68,
          1.67,
          3.26,
          3.53,
          3.76,
          3.35,
          3.61,
          3.4700000000000006,
          3.6,
          0.3933,
          0.3833,
          0.42,
          0.4067,
          0.4067,
          0.42329999999999995,
          0.43,
          1.68,
          1.58,
          1.6700000000000002,
          1.48,
          1.75,
          1.69,
          1.7999999999999996,
          1.35,
          1.67,
          1.11,
          1.4300000000000002,
          1.52,
          1.23,
          1.41,
          1.15,
          1.33,
          1.24,
          1.33,
          1.4199999999999997,
          1.33,
          1.18,
          2.72,
          2.8200000000000003,
          2.75,
          2.6099999999999994,
          2.88,
          2.94,
          2.9700000000000006,
          3.01,
          3.1500000000000004,
          2.4400000000000004,
          3.0000000000000004,
          3.14,
          3.22,
          2.33,
          0.32,
          0.37,
          0.37,
          0.37,
          0.37,
          0.39,
          0.39,
          1.0333,
          1.12,
          1.1933,
          1.2067,
          1.2,
          1.3333,
          1.2599999999999998,
          0.6,
          0.65,
          0.65,
          0.69,
          0.66,
          0.74,
          0.63,
          2.05,
          1.8700000000000003,
          1.68,
          1.24,
          1.4500000000000002,
          1.51,
          1.9799999999999998,
          1.77,
          2.2999999999999994,
          1.41,
          2.09,
          2.03,
          2.1299999999999994,
          2.03,
          1.8100000000000003,
          2.54,
          2.13,
          2.83,
          2.02,
          1.4300000000000002,
          3.3199999999999994,
          1.08,
          1.16,
          1.09,
          1.0300000000000002,
          1.3,
          1.07,
          0.93,
          0.4310999999999999,
          0.49779999999999996,
          0.4178,
          0.44,
          0.5466999999999999,
          0.5666999999999999,
          0.4733,
          0.89,
          1.04,
          1.08,
          1.04,
          0.99,
          1.17,
          0.97
         ]
        },
        {
         "mode": "lines+markers",
         "name": "All Stocks: EPS - PENDS",
         "type": "scatter",
         "x": [
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ],
          [
           "2022-Q2",
           "2022-Q3",
           "2022-Q4",
           "2023-Q1",
           "2023-Q2",
           "2023-Q3"
          ]
         ],
         "y": [
          0.94911128282547,
          0.9583210945129395,
          0.9650893211364746,
          0.9688709378242493,
          0.9699885845184326,
          0.9703192710876465,
          0.9709376692771912,
          2.499345541000366,
          2.5532386302948,
          2.581165313720703,
          2.6091079711914062,
          2.620652437210083,
          2.6246848106384277,
          2.62324595451355,
          1.8702963590621948,
          1.8973934650421143,
          1.918764591217041,
          1.9329811334609985,
          1.9376362562179565,
          1.9377570152282715,
          1.9388675689697266,
          0.9296103715896606,
          0.9423028230667114,
          0.9508586525917053,
          0.95306396484375,
          0.9524521231651306,
          0.9478673934936523,
          0.9405522346496582,
          0.976376473903656,
          0.9859853386878967,
          0.9935134649276733,
          0.9981542825698853,
          0.9989511966705322,
          0.9988446831703186,
          0.9988419413566589,
          5.9994635581970215,
          6.0601487159729,
          6.104046821594238,
          6.131934642791748,
          6.134231090545654,
          6.13541841506958,
          6.136641025543213,
          1.0138964653015137,
          1.0412184000015259,
          1.0661298036575317,
          1.0846885442733765,
          1.0864959955215454,
          1.0836337804794312,
          1.0837128162384033,
          1.150590419769287,
          1.164170742034912,
          1.1760507822036743,
          1.1951197385787964,
          1.1948907375335693,
          1.1908303499221802,
          1.192271113395691,
          1.3580721616744995,
          1.3768370151519775,
          1.3773672580718994,
          1.3907556533813477,
          1.3961609601974487,
          1.3968805074691772,
          1.396682620048523,
          1.5870882272720337,
          1.6149808168411255,
          1.6361945867538452,
          1.656348466873169,
          1.66012704372406,
          1.6583315134048462,
          1.6607613563537598,
          2.2883408069610596,
          2.312751293182373,
          2.33252215385437,
          2.3433916568756104,
          2.345785140991211,
          2.343559980392456,
          2.3448891639709473,
          5.528811931610107,
          5.597284317016602,
          5.658624649047852,
          5.714279651641846,
          5.725849628448486,
          5.724061965942383,
          5.7323455810546875,
          1.0147671699523926,
          1.02921462059021,
          1.0466865301132202,
          1.0520601272583008,
          1.0573399066925049,
          1.0589964389801025,
          1.0615476369857788,
          0.6649695038795471,
          0.6686374545097351,
          0.6722150444984436,
          0.6747907996177673,
          0.6741084456443787,
          0.6738194227218628,
          0.673719584941864,
          0.3431822657585144,
          0.34587034583091736,
          0.34817665815353394,
          0.3501245677471161,
          0.3505411744117737,
          0.35047978162765503,
          0.3504509925842285,
          1.2750768661499023,
          1.2901415824890137,
          1.3054883480072021,
          1.3166468143463135,
          1.3195241689682007,
          1.3198981285095215,
          1.3210710287094116,
          1.6872104406356812,
          1.7188739776611328,
          1.7407217025756836,
          1.754414677619934,
          1.7538071870803833,
          1.754902958869934,
          1.7555733919143677,
          1.0645626783370972,
          1.0808407068252563,
          1.094338297843933,
          1.1050188541412354,
          1.1087019443511963,
          1.109088659286499,
          1.1103521585464478,
          1.1383997201919556,
          1.1612746715545654,
          1.1764699220657349,
          1.1853009462356567,
          1.1884276866912842,
          1.188693642616272,
          1.1885088682174683,
          1.120497226715088,
          1.1380943059921265,
          1.153011441230774,
          1.1622294187545776,
          1.1640105247497559,
          1.1634798049926758,
          1.1637048721313477,
          2.553441286087036,
          2.5957696437835693,
          2.636655807495117,
          2.6665103435516357,
          2.678980827331543,
          2.657400369644165,
          2.6571385860443115,
          0.36652496457099915,
          0.3703977167606354,
          0.3730528950691223,
          0.37515437602996826,
          0.3756108283996582,
          0.3756808936595917,
          0.37582895159721375,
          1.3016955852508545,
          1.333609700202942,
          1.3674006462097168,
          1.3919092416763306,
          1.393794298171997,
          1.3940660953521729,
          1.394547462463379,
          1.20222806930542,
          1.2111395597457886,
          1.2224477529525757,
          1.2301126718521118,
          1.231959342956543,
          1.2320927381515503,
          1.2315640449523926,
          0.9923399090766907,
          1.0449235439300537,
          1.0736887454986572,
          1.086712121963501,
          1.0930290222167969,
          1.0953328609466553,
          1.110036015510559,
          2.1737594604492188,
          2.1944823265075684,
          2.2097177505493164,
          2.221710205078125,
          2.2245962619781494,
          2.224776268005371,
          2.2253689765930176,
          2.6715991497039795,
          2.7014405727386475,
          2.724560022354126,
          2.7423291206359863,
          2.745790958404541,
          2.745903253555298,
          2.7477192878723145,
          0.29487231373786926,
          0.30016809701919556,
          0.3044372498989105,
          0.3079034686088562,
          0.30909764766693115,
          0.3093370199203491,
          0.3094874918460846,
          0.9154947996139526,
          0.9256278276443481,
          0.9371344447135925,
          0.9463373422622681,
          0.9492453336715698,
          0.9488092660903931,
          0.949310839176178,
          0.45768165588378906,
          0.4672800302505493,
          0.4749748110771179,
          0.4809127151966095,
          0.4829433560371399,
          0.48335105180740356,
          0.4836677014827728,
          1.6276371479034424,
          1.64101243019104,
          1.6593879461288452,
          1.6716970205307007,
          1.6705317497253418,
          1.6681592464447021,
          1.6671595573425293,
          1.544132947921753,
          1.5606544017791748,
          1.5766161680221558,
          1.5843651294708252,
          1.588271141052246,
          1.5862165689468384,
          1.5863635540008545,
          2.3510756492614746,
          2.3979618549346924,
          2.4458343982696533,
          2.4880785942077637,
          2.5012199878692627,
          2.501802444458008,
          2.5027294158935547,
          1.0356113910675049,
          1.043160080909729,
          1.0475120544433594,
          1.0506181716918945,
          1.0496976375579834,
          1.0502163171768188,
          1.0498214960098267,
          0.3996925354003906,
          0.4092414379119873,
          0.41628187894821167,
          0.41919925808906555,
          0.41974231600761414,
          0.42056795954704285,
          0.4208890497684479,
          0.909784734249115,
          0.9249963760375977,
          0.9376319050788879,
          0.9466944336891174,
          0.9487351179122925,
          0.9488495588302612,
          0.948940098285675
         ]
        }
       ],
       "layout": {
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Multiple Companies: EPS - PENDS"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "rangemode": "tozero",
         "side": "left",
         "title": {
          "text": "EPS"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace_true = go.Scatter(x=x_indices, y=y_true_inverse, mode=\"lines+markers\", name=f\"All Stocks: EPS - PENDS\")\n",
    "trace_preds = go.Scatter(x=x_indices, y=preds_inverse, mode=\"lines+markers\", name=f\"All Stocks: EPS - PENDS\")\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = f\"{stock_symbol}: EPS - PENDS\",\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='EPS', side='left', rangemode='tozero'),\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace_true, trace_preds], layout=layout)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
