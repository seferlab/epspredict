{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import talib\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import seaborn as sns\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.trial import TrialState\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau \n",
    "import shap\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n",
      "2.1.2+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8902\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"gpu\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "print('CUDA version:', torch.version.cuda)\n",
    "print('cuDNN version:', torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>17.869831</td>\n",
       "      <td>1.698451</td>\n",
       "      <td>2.6718</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>777.000000</td>\n",
       "      <td>0.209015</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062957</td>\n",
       "      <td>2.970550</td>\n",
       "      <td>4.378340</td>\n",
       "      <td>-5.517243</td>\n",
       "      <td>-6.345335</td>\n",
       "      <td>7.308964</td>\n",
       "      <td>-1.141483</td>\n",
       "      <td>-1.381423</td>\n",
       "      <td>-1.624192</td>\n",
       "      <td>5.051769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>17.834875</td>\n",
       "      <td>3.131120</td>\n",
       "      <td>2.9872</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>1645.000000</td>\n",
       "      <td>0.258659</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120922</td>\n",
       "      <td>-13.305144</td>\n",
       "      <td>-5.027217</td>\n",
       "      <td>-22.349038</td>\n",
       "      <td>-23.061887</td>\n",
       "      <td>-10.695185</td>\n",
       "      <td>-21.535988</td>\n",
       "      <td>-8.171207</td>\n",
       "      <td>-25.425447</td>\n",
       "      <td>0.418158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>21.771714</td>\n",
       "      <td>3.087308</td>\n",
       "      <td>3.4500</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>1321.000000</td>\n",
       "      <td>0.249095</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087046</td>\n",
       "      <td>11.933447</td>\n",
       "      <td>9.541476</td>\n",
       "      <td>18.150742</td>\n",
       "      <td>10.076208</td>\n",
       "      <td>9.328708</td>\n",
       "      <td>15.503083</td>\n",
       "      <td>7.838985</td>\n",
       "      <td>14.100815</td>\n",
       "      <td>7.019633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>26.426070</td>\n",
       "      <td>2.380069</td>\n",
       "      <td>3.9154</td>\n",
       "      <td>0.528200</td>\n",
       "      <td>1457.000000</td>\n",
       "      <td>0.153558</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191850</td>\n",
       "      <td>15.556124</td>\n",
       "      <td>4.893814</td>\n",
       "      <td>3.789965</td>\n",
       "      <td>21.538450</td>\n",
       "      <td>8.417417</td>\n",
       "      <td>10.874069</td>\n",
       "      <td>18.506872</td>\n",
       "      <td>10.358213</td>\n",
       "      <td>-2.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>27.652509</td>\n",
       "      <td>3.109565</td>\n",
       "      <td>4.2582</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>2056.000000</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484614</td>\n",
       "      <td>-2.905304</td>\n",
       "      <td>2.024644</td>\n",
       "      <td>-7.498254</td>\n",
       "      <td>-7.341768</td>\n",
       "      <td>1.063541</td>\n",
       "      <td>-4.676644</td>\n",
       "      <td>-4.741380</td>\n",
       "      <td>-4.544226</td>\n",
       "      <td>5.565071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.600140</td>\n",
       "      <td>37.3900</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>45.779944</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355782</td>\n",
       "      <td>-15.534891</td>\n",
       "      <td>-5.840908</td>\n",
       "      <td>-24.280433</td>\n",
       "      <td>-13.633068</td>\n",
       "      <td>-9.080399</td>\n",
       "      <td>-17.844385</td>\n",
       "      <td>-17.722026</td>\n",
       "      <td>-12.791300</td>\n",
       "      <td>0.512814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>52.800000</td>\n",
       "      <td>3.001754</td>\n",
       "      <td>41.5751</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>41.655270</td>\n",
       "      <td>0.300673</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178828</td>\n",
       "      <td>14.988381</td>\n",
       "      <td>10.496262</td>\n",
       "      <td>15.292074</td>\n",
       "      <td>7.934506</td>\n",
       "      <td>6.057101</td>\n",
       "      <td>16.488114</td>\n",
       "      <td>19.393354</td>\n",
       "      <td>9.857481</td>\n",
       "      <td>9.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>49.574468</td>\n",
       "      <td>4.422004</td>\n",
       "      <td>42.9480</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>44.643278</td>\n",
       "      <td>0.516266</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>4.699164</td>\n",
       "      <td>3.493137</td>\n",
       "      <td>-3.644893</td>\n",
       "      <td>7.351231</td>\n",
       "      <td>0.970027</td>\n",
       "      <td>3.185392</td>\n",
       "      <td>5.459461</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>2.509890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>51.193548</td>\n",
       "      <td>3.780823</td>\n",
       "      <td>40.7500</td>\n",
       "      <td>1.583422</td>\n",
       "      <td>44.768947</td>\n",
       "      <td>0.615809</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>1.258389</td>\n",
       "      <td>5.768897</td>\n",
       "      <td>-7.078949</td>\n",
       "      <td>1.449274</td>\n",
       "      <td>-2.709415</td>\n",
       "      <td>0.271247</td>\n",
       "      <td>3.190669</td>\n",
       "      <td>-0.512819</td>\n",
       "      <td>8.569506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>47.115385</td>\n",
       "      <td>9.213489</td>\n",
       "      <td>41.1200</td>\n",
       "      <td>1.832289</td>\n",
       "      <td>46.407565</td>\n",
       "      <td>0.484964</td>\n",
       "      <td>0.34</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063856</td>\n",
       "      <td>3.910523</td>\n",
       "      <td>2.539892</td>\n",
       "      <td>1.418919</td>\n",
       "      <td>9.928574</td>\n",
       "      <td>13.014539</td>\n",
       "      <td>4.946547</td>\n",
       "      <td>13.833353</td>\n",
       "      <td>5.532642</td>\n",
       "      <td>-0.185349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9660 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date OFTIC       PENDS       MEAN     STDEV      BPS       CPS  \\\n",
       "0      2011-06-30  AAPL  2011-06-30  17.869831  1.698451   2.6718  0.422900   \n",
       "1      2011-09-30  AAPL  2011-09-30  17.834875  3.131120   2.9872  0.397600   \n",
       "2      2011-12-31  AAPL  2011-12-31  21.771714  3.087308   3.4500  0.665700   \n",
       "3      2012-03-31  AAPL  2012-03-31  26.426070  2.380069   3.9154  0.528200   \n",
       "4      2012-06-30  AAPL  2012-06-30  27.652509  3.109565   4.2582  0.384300   \n",
       "...           ...   ...         ...        ...       ...      ...       ...   \n",
       "14055  2018-12-31  ZION  2018-12-31  53.000000  4.600140  37.3900  1.760000   \n",
       "14056  2019-03-31  ZION  2019-03-31  52.800000  3.001754  41.5751 -0.210000   \n",
       "14057  2019-06-30  ZION  2019-06-30  49.574468  4.422004  42.9480  1.380000   \n",
       "14058  2019-09-30  ZION  2019-09-30  51.193548  3.780823  40.7500  1.583422   \n",
       "14059  2019-12-31  ZION  2019-12-31  47.115385  9.213489  41.1200  1.832289   \n",
       "\n",
       "               CPX       CSH   DPS  ...  \\\n",
       "0       777.000000  0.209015  0.00  ...   \n",
       "1      1645.000000  0.258659  0.00  ...   \n",
       "2      1321.000000  0.249095  0.00  ...   \n",
       "3      1457.000000  0.153558  0.00  ...   \n",
       "4      2056.000000  0.204900  0.00  ...   \n",
       "...            ...       ...   ...  ...   \n",
       "14055    45.779944  0.504142  0.30  ...   \n",
       "14056    41.655270  0.300673  0.30  ...   \n",
       "14057    44.643278  0.516266  0.30  ...   \n",
       "14058    44.768947  0.615809  0.34  ...   \n",
       "14059    46.407565  0.484964  0.34  ...   \n",
       "\n",
       "       commodity_trade_Close_quarterly_return  \\\n",
       "0                                    0.062957   \n",
       "1                                    0.120922   \n",
       "2                                   -0.087046   \n",
       "3                                   -0.191850   \n",
       "4                                    0.484614   \n",
       "...                                       ...   \n",
       "14055                                0.355782   \n",
       "14056                               -0.178828   \n",
       "14057                                0.081598   \n",
       "14058                               -0.036101   \n",
       "14059                               -0.063856   \n",
       "\n",
       "       C_Discretionary_Close_quarterly_return  \\\n",
       "0                                    2.970550   \n",
       "1                                  -13.305144   \n",
       "2                                   11.933447   \n",
       "3                                   15.556124   \n",
       "4                                   -2.905304   \n",
       "...                                       ...   \n",
       "14055                              -15.534891   \n",
       "14056                               14.988381   \n",
       "14057                                4.699164   \n",
       "14058                                1.258389   \n",
       "14059                                3.910523   \n",
       "\n",
       "       C_Staples_Close_quarterly_return  Energy_Close_quarterly_return  \\\n",
       "0                              4.378340                      -5.517243   \n",
       "1                             -5.027217                     -22.349038   \n",
       "2                              9.541476                      18.150742   \n",
       "3                              4.893814                       3.789965   \n",
       "4                              2.024644                      -7.498254   \n",
       "...                                 ...                            ...   \n",
       "14055                         -5.840908                     -24.280433   \n",
       "14056                         10.496262                      15.292074   \n",
       "14057                          3.493137                      -3.644893   \n",
       "14058                          5.768897                      -7.078949   \n",
       "14059                          2.539892                       1.418919   \n",
       "\n",
       "       Financials_Close_quarterly_return  Health_care_Close_quarterly_return  \\\n",
       "0                              -6.345335                            7.308964   \n",
       "1                             -23.061887                          -10.695185   \n",
       "2                              10.076208                            9.328708   \n",
       "3                              21.538450                            8.417417   \n",
       "4                              -7.341768                            1.063541   \n",
       "...                                  ...                                 ...   \n",
       "14055                         -13.633068                           -9.080399   \n",
       "14056                           7.934506                            6.057101   \n",
       "14057                           7.351231                            0.970027   \n",
       "14058                           1.449274                           -2.709415   \n",
       "14059                           9.928574                           13.014539   \n",
       "\n",
       "       industrials_Close_quarterly_return  information_Close_quarterly_return  \\\n",
       "0                               -1.141483                           -1.381423   \n",
       "1                              -21.535988                           -8.171207   \n",
       "2                               15.503083                            7.838985   \n",
       "3                               10.874069                           18.506872   \n",
       "4                               -4.676644                           -4.741380   \n",
       "...                                   ...                                 ...   \n",
       "14055                          -17.844385                          -17.722026   \n",
       "14056                           16.488114                           19.393354   \n",
       "14057                            3.185392                            5.459461   \n",
       "14058                            0.271247                            3.190669   \n",
       "14059                            4.946547                           13.833353   \n",
       "\n",
       "       materials_Close_quarterly_return  utilities_Close_quarterly_return  \n",
       "0                             -1.624192                          5.051769  \n",
       "1                            -25.425447                          0.418158  \n",
       "2                             14.100815                          7.019633  \n",
       "3                             10.358213                         -2.612559  \n",
       "4                             -4.544226                          5.565071  \n",
       "...                                 ...                               ...  \n",
       "14055                        -12.791300                          0.512814  \n",
       "14056                          9.857481                          9.920635  \n",
       "14057                          5.405405                          2.509890  \n",
       "14058                         -0.512819                          8.569506  \n",
       "14059                          5.532642                         -0.185349  \n",
       "\n",
       "[9660 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data = pd.read_csv(\"../Data/longmerged_deneme_51.csv\")\n",
    "pivoted_data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "#pivoted_data.set_index(\"PENDS\", inplace=True)\n",
    "len(pivoted_data[\"OFTIC\"].unique())\n",
    "# pivoted_data[pivoted_data[\"OFTIC\"] == \"AAPL\"]\n",
    "pivoted_data = pivoted_data[pivoted_data['PENDS'] > '2011-03-31']\n",
    "pivoted_data = pivoted_data[pivoted_data['PENDS'] < '2020-01-01']\n",
    "\n",
    "pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoted_data = pivoted_data[~pivoted_data[\"OFTIC\"].isin([\"DFS\", \"ICE\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'OFTIC', 'PENDS', 'MEAN', 'STDEV', 'BPS', 'CPS', 'CPX', 'CSH', 'DPS', 'EBG', 'EBI', 'EBS', 'EBT', 'ENT', 'EPS', 'FFO', 'GPS', 'GRM', 'NAV', 'NDT', 'NET', 'OPR', 'PRE', 'ROA', 'ROE', 'SAL', 'Real_Estate_Index_Price', 'VIX_Close', 'Gold_Close', 'Three_Month_Yield', 'Brent_Close', 'Hrc_close', 'commodity_trade_Close', 'C_Discretionary_Close', 'C_Staples_Close', 'Energy_Close', 'Financials_Close', 'Health_care_Close', 'industrials_Close', 'information_Close', 'materials_Close', 'utilities_Close', 'Sector', 'numeric_sector', 'Real_Estate_Index_Price_quarterly_return', 'VIX_Close_quarterly_return', 'Gold_Close_quarterly_return', 'Three_Month_Yield_quarterly_return', 'Brent_Close_quarterly_return', 'Hrc_close_quarterly_return', 'commodity_trade_Close_quarterly_return', 'C_Discretionary_Close_quarterly_return', 'C_Staples_Close_quarterly_return', 'Energy_Close_quarterly_return', 'Financials_Close_quarterly_return', 'Health_care_Close_quarterly_return', 'industrials_Close_quarterly_return', 'information_Close_quarterly_return', 'materials_Close_quarterly_return', 'utilities_Close_quarterly_return']\n"
     ]
    }
   ],
   "source": [
    "print(list(pivoted_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sector\n",
       "Communication Services     5\n",
       "Consumer Discretionary    32\n",
       "Consumer Staples          20\n",
       "Energy                    14\n",
       "Financials                38\n",
       "Health Care               38\n",
       "Industrials               36\n",
       "Information Technology    34\n",
       "Materials                 19\n",
       "Real Estate               20\n",
       "Utilities                 20\n",
       "Name: OFTIC, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_counts = pivoted_data.groupby('Sector')['OFTIC'].nunique()\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'OFTIC', 'PENDS', 'MEAN', 'STDEV', 'BPS', 'CPS', 'CPX', 'CSH',\n",
       "       'DPS', 'EBG', 'EBI', 'EBS', 'EBT', 'ENT', 'EPS', 'FFO', 'GPS', 'GRM',\n",
       "       'NAV', 'NDT', 'NET', 'OPR', 'PRE', 'ROA', 'ROE', 'SAL',\n",
       "       'Real_Estate_Index_Price', 'VIX_Close', 'Gold_Close',\n",
       "       'Three_Month_Yield', 'Brent_Close', 'Hrc_close',\n",
       "       'commodity_trade_Close', 'C_Discretionary_Close', 'C_Staples_Close',\n",
       "       'Energy_Close', 'Financials_Close', 'Health_care_Close',\n",
       "       'industrials_Close', 'information_Close', 'materials_Close',\n",
       "       'utilities_Close', 'Sector', 'numeric_sector',\n",
       "       'Real_Estate_Index_Price_quarterly_return',\n",
       "       'VIX_Close_quarterly_return', 'Gold_Close_quarterly_return',\n",
       "       'Three_Month_Yield_quarterly_return', 'Brent_Close_quarterly_return',\n",
       "       'Hrc_close_quarterly_return', 'commodity_trade_Close_quarterly_return',\n",
       "       'C_Discretionary_Close_quarterly_return',\n",
       "       'C_Staples_Close_quarterly_return', 'Energy_Close_quarterly_return',\n",
       "       'Financials_Close_quarterly_return',\n",
       "       'Health_care_Close_quarterly_return',\n",
       "       'industrials_Close_quarterly_return',\n",
       "       'information_Close_quarterly_return',\n",
       "       'materials_Close_quarterly_return', 'utilities_Close_quarterly_return'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoted_data.drop(columns=['Real_Estate_Index_Price','Three_Month_Yield', 'Brent_Close', 'Hrc_close',\n",
    "#        'commodity_trade_Close', 'C_Discretionary_Close', 'C_Staples_Close',\n",
    "#        'Energy_Close', 'Financials_Close', 'Health_care_Close',\n",
    "#        'industrials_Close', 'information_Close', 'materials_Close',\n",
    "#        'utilities_Close', 'numeric_sector',\n",
    "#        'Real_Estate_Index_Price_quarterly_return',\n",
    "#        'VIX_Close_quarterly_return', 'Gold_Close_quarterly_return',\n",
    "#        'Three_Month_Yield_quarterly_return', 'Brent_Close_quarterly_return',\n",
    "#        'Hrc_close_quarterly_return', 'commodity_trade_Close_quarterly_return',\n",
    "#        'C_Discretionary_Close_quarterly_return',\n",
    "#        'C_Staples_Close_quarterly_return', 'Energy_Close_quarterly_return',\n",
    "#        'Financials_Close_quarterly_return',\n",
    "#        'Health_care_Close_quarterly_return',\n",
    "#        'industrials_Close_quarterly_return',\n",
    "#        'information_Close_quarterly_return',\n",
    "#        'materials_Close_quarterly_return', 'utilities_Close_quarterly_return'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-06-30</td>\n",
       "      <td>27.546538</td>\n",
       "      <td>3.568238</td>\n",
       "      <td>12.8250</td>\n",
       "      <td>1.587282</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.235599</td>\n",
       "      <td>0.150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062957</td>\n",
       "      <td>2.970550</td>\n",
       "      <td>4.378340</td>\n",
       "      <td>-5.517243</td>\n",
       "      <td>-6.345335</td>\n",
       "      <td>7.308964</td>\n",
       "      <td>-1.141483</td>\n",
       "      <td>-1.381423</td>\n",
       "      <td>-1.624192</td>\n",
       "      <td>5.051769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-09-30</td>\n",
       "      <td>26.911765</td>\n",
       "      <td>2.682322</td>\n",
       "      <td>13.6250</td>\n",
       "      <td>1.763514</td>\n",
       "      <td>0.324273</td>\n",
       "      <td>0.310576</td>\n",
       "      <td>0.150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120922</td>\n",
       "      <td>-13.305144</td>\n",
       "      <td>-5.027217</td>\n",
       "      <td>-22.349038</td>\n",
       "      <td>-23.061887</td>\n",
       "      <td>-10.695185</td>\n",
       "      <td>-21.535988</td>\n",
       "      <td>-8.171207</td>\n",
       "      <td>-25.425447</td>\n",
       "      <td>0.418158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>1.982881</td>\n",
       "      <td>14.4800</td>\n",
       "      <td>1.621515</td>\n",
       "      <td>0.320066</td>\n",
       "      <td>0.322786</td>\n",
       "      <td>0.165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087046</td>\n",
       "      <td>11.933447</td>\n",
       "      <td>9.541476</td>\n",
       "      <td>18.150742</td>\n",
       "      <td>10.076208</td>\n",
       "      <td>9.328708</td>\n",
       "      <td>15.503083</td>\n",
       "      <td>7.838985</td>\n",
       "      <td>14.100815</td>\n",
       "      <td>7.019633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-03-31</td>\n",
       "      <td>28.115385</td>\n",
       "      <td>2.550767</td>\n",
       "      <td>14.5950</td>\n",
       "      <td>1.660416</td>\n",
       "      <td>0.201505</td>\n",
       "      <td>0.197983</td>\n",
       "      <td>0.165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191850</td>\n",
       "      <td>15.556124</td>\n",
       "      <td>4.893814</td>\n",
       "      <td>3.789965</td>\n",
       "      <td>21.538450</td>\n",
       "      <td>8.417417</td>\n",
       "      <td>10.874069</td>\n",
       "      <td>18.506872</td>\n",
       "      <td>10.358213</td>\n",
       "      <td>-2.612559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>2.592189</td>\n",
       "      <td>15.1450</td>\n",
       "      <td>1.823892</td>\n",
       "      <td>0.289115</td>\n",
       "      <td>0.257621</td>\n",
       "      <td>0.165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484614</td>\n",
       "      <td>-2.905304</td>\n",
       "      <td>2.024644</td>\n",
       "      <td>-7.498254</td>\n",
       "      <td>-7.341768</td>\n",
       "      <td>1.063541</td>\n",
       "      <td>-4.676644</td>\n",
       "      <td>-4.741380</td>\n",
       "      <td>-4.544226</td>\n",
       "      <td>5.565071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.600140</td>\n",
       "      <td>37.3900</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>45.779944</td>\n",
       "      <td>0.504142</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355782</td>\n",
       "      <td>-15.534891</td>\n",
       "      <td>-5.840908</td>\n",
       "      <td>-24.280433</td>\n",
       "      <td>-13.633068</td>\n",
       "      <td>-9.080399</td>\n",
       "      <td>-17.844385</td>\n",
       "      <td>-17.722026</td>\n",
       "      <td>-12.791300</td>\n",
       "      <td>0.512814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-03-31</td>\n",
       "      <td>52.800000</td>\n",
       "      <td>3.001754</td>\n",
       "      <td>41.5751</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>41.655270</td>\n",
       "      <td>0.300673</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178828</td>\n",
       "      <td>14.988381</td>\n",
       "      <td>10.496262</td>\n",
       "      <td>15.292074</td>\n",
       "      <td>7.934506</td>\n",
       "      <td>6.057101</td>\n",
       "      <td>16.488114</td>\n",
       "      <td>19.393354</td>\n",
       "      <td>9.857481</td>\n",
       "      <td>9.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>49.574468</td>\n",
       "      <td>4.422004</td>\n",
       "      <td>42.9480</td>\n",
       "      <td>1.380000</td>\n",
       "      <td>44.643278</td>\n",
       "      <td>0.516266</td>\n",
       "      <td>0.300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>4.699164</td>\n",
       "      <td>3.493137</td>\n",
       "      <td>-3.644893</td>\n",
       "      <td>7.351231</td>\n",
       "      <td>0.970027</td>\n",
       "      <td>3.185392</td>\n",
       "      <td>5.459461</td>\n",
       "      <td>5.405405</td>\n",
       "      <td>2.509890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>51.193548</td>\n",
       "      <td>3.780823</td>\n",
       "      <td>40.7500</td>\n",
       "      <td>1.583422</td>\n",
       "      <td>44.768947</td>\n",
       "      <td>0.615809</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036101</td>\n",
       "      <td>1.258389</td>\n",
       "      <td>5.768897</td>\n",
       "      <td>-7.078949</td>\n",
       "      <td>1.449274</td>\n",
       "      <td>-2.709415</td>\n",
       "      <td>0.271247</td>\n",
       "      <td>3.190669</td>\n",
       "      <td>-0.512819</td>\n",
       "      <td>8.569506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>47.115385</td>\n",
       "      <td>9.213489</td>\n",
       "      <td>41.1200</td>\n",
       "      <td>1.832289</td>\n",
       "      <td>46.407565</td>\n",
       "      <td>0.484964</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063856</td>\n",
       "      <td>3.910523</td>\n",
       "      <td>2.539892</td>\n",
       "      <td>1.418919</td>\n",
       "      <td>9.928574</td>\n",
       "      <td>13.014539</td>\n",
       "      <td>4.946547</td>\n",
       "      <td>13.833353</td>\n",
       "      <td>5.532642</td>\n",
       "      <td>-0.185349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1330 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date OFTIC       PENDS       MEAN     STDEV      BPS       CPS  \\\n",
       "51     2011-06-30   AFL  2011-06-30  27.546538  3.568238  12.8250  1.587282   \n",
       "52     2011-09-30   AFL  2011-09-30  26.911765  2.682322  13.6250  1.763514   \n",
       "53     2011-12-31   AFL  2011-12-31  27.750000  1.982881  14.4800  1.621515   \n",
       "54     2012-03-31   AFL  2012-03-31  28.115385  2.550767  14.5950  1.660416   \n",
       "55     2012-06-30   AFL  2012-06-30  28.200000  2.592189  15.1450  1.823892   \n",
       "...           ...   ...         ...        ...       ...      ...       ...   \n",
       "14055  2018-12-31  ZION  2018-12-31  53.000000  4.600140  37.3900  1.760000   \n",
       "14056  2019-03-31  ZION  2019-03-31  52.800000  3.001754  41.5751 -0.210000   \n",
       "14057  2019-06-30  ZION  2019-06-30  49.574468  4.422004  42.9480  1.380000   \n",
       "14058  2019-09-30  ZION  2019-09-30  51.193548  3.780823  40.7500  1.583422   \n",
       "14059  2019-12-31  ZION  2019-12-31  47.115385  9.213489  41.1200  1.832289   \n",
       "\n",
       "             CPX       CSH    DPS  ...  \\\n",
       "51      0.259769  0.235599  0.150  ...   \n",
       "52      0.324273  0.310576  0.150  ...   \n",
       "53      0.320066  0.322786  0.165  ...   \n",
       "54      0.201505  0.197983  0.165  ...   \n",
       "55      0.289115  0.257621  0.165  ...   \n",
       "...          ...       ...    ...  ...   \n",
       "14055  45.779944  0.504142  0.300  ...   \n",
       "14056  41.655270  0.300673  0.300  ...   \n",
       "14057  44.643278  0.516266  0.300  ...   \n",
       "14058  44.768947  0.615809  0.340  ...   \n",
       "14059  46.407565  0.484964  0.340  ...   \n",
       "\n",
       "       commodity_trade_Close_quarterly_return  \\\n",
       "51                                   0.062957   \n",
       "52                                   0.120922   \n",
       "53                                  -0.087046   \n",
       "54                                  -0.191850   \n",
       "55                                   0.484614   \n",
       "...                                       ...   \n",
       "14055                                0.355782   \n",
       "14056                               -0.178828   \n",
       "14057                                0.081598   \n",
       "14058                               -0.036101   \n",
       "14059                               -0.063856   \n",
       "\n",
       "       C_Discretionary_Close_quarterly_return  \\\n",
       "51                                   2.970550   \n",
       "52                                 -13.305144   \n",
       "53                                  11.933447   \n",
       "54                                  15.556124   \n",
       "55                                  -2.905304   \n",
       "...                                       ...   \n",
       "14055                              -15.534891   \n",
       "14056                               14.988381   \n",
       "14057                                4.699164   \n",
       "14058                                1.258389   \n",
       "14059                                3.910523   \n",
       "\n",
       "       C_Staples_Close_quarterly_return  Energy_Close_quarterly_return  \\\n",
       "51                             4.378340                      -5.517243   \n",
       "52                            -5.027217                     -22.349038   \n",
       "53                             9.541476                      18.150742   \n",
       "54                             4.893814                       3.789965   \n",
       "55                             2.024644                      -7.498254   \n",
       "...                                 ...                            ...   \n",
       "14055                         -5.840908                     -24.280433   \n",
       "14056                         10.496262                      15.292074   \n",
       "14057                          3.493137                      -3.644893   \n",
       "14058                          5.768897                      -7.078949   \n",
       "14059                          2.539892                       1.418919   \n",
       "\n",
       "       Financials_Close_quarterly_return  Health_care_Close_quarterly_return  \\\n",
       "51                             -6.345335                            7.308964   \n",
       "52                            -23.061887                          -10.695185   \n",
       "53                             10.076208                            9.328708   \n",
       "54                             21.538450                            8.417417   \n",
       "55                             -7.341768                            1.063541   \n",
       "...                                  ...                                 ...   \n",
       "14055                         -13.633068                           -9.080399   \n",
       "14056                           7.934506                            6.057101   \n",
       "14057                           7.351231                            0.970027   \n",
       "14058                           1.449274                           -2.709415   \n",
       "14059                           9.928574                           13.014539   \n",
       "\n",
       "       industrials_Close_quarterly_return  information_Close_quarterly_return  \\\n",
       "51                              -1.141483                           -1.381423   \n",
       "52                             -21.535988                           -8.171207   \n",
       "53                              15.503083                            7.838985   \n",
       "54                              10.874069                           18.506872   \n",
       "55                              -4.676644                           -4.741380   \n",
       "...                                   ...                                 ...   \n",
       "14055                          -17.844385                          -17.722026   \n",
       "14056                           16.488114                           19.393354   \n",
       "14057                            3.185392                            5.459461   \n",
       "14058                            0.271247                            3.190669   \n",
       "14059                            4.946547                           13.833353   \n",
       "\n",
       "       materials_Close_quarterly_return  utilities_Close_quarterly_return  \n",
       "51                            -1.624192                          5.051769  \n",
       "52                           -25.425447                          0.418158  \n",
       "53                            14.100815                          7.019633  \n",
       "54                            10.358213                         -2.612559  \n",
       "55                            -4.544226                          5.565071  \n",
       "...                                 ...                               ...  \n",
       "14055                        -12.791300                          0.512814  \n",
       "14056                          9.857481                          9.920635  \n",
       "14057                          5.405405                          2.509890  \n",
       "14058                         -0.512819                          8.569506  \n",
       "14059                          5.532642                         -0.185349  \n",
       "\n",
       "[1330 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data = pivoted_data[pivoted_data[\"Sector\"] == \"Financials\"]\n",
    "pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: PENDS, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_pends_by_ticker = pivoted_data.groupby('OFTIC')['PENDS'].nunique()\n",
    "unique_pends_not_50 = unique_pends_by_ticker[unique_pends_by_ticker != 35]\n",
    "unique_pends_not_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1330 entries, 51 to 14059\n",
      "Data columns (total 61 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Date                                      1330 non-null   object \n",
      " 1   OFTIC                                     1330 non-null   object \n",
      " 2   PENDS                                     1330 non-null   object \n",
      " 3   MEAN                                      1330 non-null   float64\n",
      " 4   STDEV                                     1330 non-null   float64\n",
      " 5   BPS                                       1330 non-null   float64\n",
      " 6   CPS                                       1330 non-null   float64\n",
      " 7   CPX                                       1330 non-null   float64\n",
      " 8   CSH                                       1330 non-null   float64\n",
      " 9   DPS                                       1330 non-null   float64\n",
      " 10  EBG                                       1330 non-null   float64\n",
      " 11  EBI                                       1330 non-null   float64\n",
      " 12  EBS                                       1330 non-null   float64\n",
      " 13  EBT                                       1330 non-null   float64\n",
      " 14  ENT                                       1330 non-null   float64\n",
      " 15  EPS                                       1330 non-null   float64\n",
      " 16  FFO                                       1330 non-null   float64\n",
      " 17  GPS                                       1330 non-null   float64\n",
      " 18  GRM                                       1330 non-null   float64\n",
      " 19  NAV                                       1330 non-null   float64\n",
      " 20  NDT                                       1330 non-null   float64\n",
      " 21  NET                                       1330 non-null   float64\n",
      " 22  OPR                                       1330 non-null   float64\n",
      " 23  PRE                                       1330 non-null   float64\n",
      " 24  ROA                                       1330 non-null   float64\n",
      " 25  ROE                                       1330 non-null   float64\n",
      " 26  SAL                                       1330 non-null   float64\n",
      " 27  Real_Estate_Index_Price                   1330 non-null   float64\n",
      " 28  VIX_Close                                 1330 non-null   float64\n",
      " 29  Gold_Close                                1330 non-null   float64\n",
      " 30  Three_Month_Yield                         1330 non-null   float64\n",
      " 31  Brent_Close                               1330 non-null   float64\n",
      " 32  Hrc_close                                 1330 non-null   float64\n",
      " 33  commodity_trade_Close                     1330 non-null   float64\n",
      " 34  C_Discretionary_Close                     1330 non-null   float64\n",
      " 35  C_Staples_Close                           1330 non-null   float64\n",
      " 36  Energy_Close                              1330 non-null   float64\n",
      " 37  Financials_Close                          1330 non-null   float64\n",
      " 38  Health_care_Close                         1330 non-null   float64\n",
      " 39  industrials_Close                         1330 non-null   float64\n",
      " 40  information_Close                         1330 non-null   float64\n",
      " 41  materials_Close                           1330 non-null   float64\n",
      " 42  utilities_Close                           1330 non-null   float64\n",
      " 43  Sector                                    1330 non-null   object \n",
      " 44  numeric_sector                            1330 non-null   int64  \n",
      " 45  Real_Estate_Index_Price_quarterly_return  1330 non-null   float64\n",
      " 46  VIX_Close_quarterly_return                1330 non-null   float64\n",
      " 47  Gold_Close_quarterly_return               1330 non-null   float64\n",
      " 48  Three_Month_Yield_quarterly_return        1330 non-null   float64\n",
      " 49  Brent_Close_quarterly_return              1330 non-null   float64\n",
      " 50  Hrc_close_quarterly_return                1330 non-null   float64\n",
      " 51  commodity_trade_Close_quarterly_return    1330 non-null   float64\n",
      " 52  C_Discretionary_Close_quarterly_return    1330 non-null   float64\n",
      " 53  C_Staples_Close_quarterly_return          1330 non-null   float64\n",
      " 54  Energy_Close_quarterly_return             1330 non-null   float64\n",
      " 55  Financials_Close_quarterly_return         1330 non-null   float64\n",
      " 56  Health_care_Close_quarterly_return        1330 non-null   float64\n",
      " 57  industrials_Close_quarterly_return        1330 non-null   float64\n",
      " 58  information_Close_quarterly_return        1330 non-null   float64\n",
      " 59  materials_Close_quarterly_return          1330 non-null   float64\n",
      " 60  utilities_Close_quarterly_return          1330 non-null   float64\n",
      "dtypes: float64(56), int64(1), object(4)\n",
      "memory usage: 644.2+ KB\n"
     ]
    }
   ],
   "source": [
    "pivoted_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_data = pivoted_data.copy()\n",
    "\n",
    "# pivoted_data['Hrc_close'] = pd.to_numeric(pivoted_data['Hrc_close'].astype(str).str.split(\",\").str[0], errors='coerce')\n",
    "pivoted_data['Gold_Close'] = pd.to_numeric(pivoted_data['Gold_Close'].astype(str).str.replace(\",\", \"\"), errors='coerce').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AFL', 'AON', 'AXP', 'BEN', 'BK', 'BLK', 'BX', 'CBOE', 'CMA',\n",
       "       'CME', 'COF', 'DFS', 'GS', 'HIG', 'ICE', 'IVZ', 'KEY', 'MA', 'MCO',\n",
       "       'MKTX', 'MMC', 'MSCI', 'MTB', 'NDAQ', 'NTRS', 'PFG', 'PGR', 'PNC',\n",
       "       'PRU', 'RF', 'RJF', 'SCHW', 'STT', 'TROW', 'TRV', 'WFC', 'WRB',\n",
       "       'ZION'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_list = pivoted_data[\"OFTIC\"].unique()\n",
    "company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoted_data = pivoted_data[[\"EPS\",\"ROE\",\"SAL\",\"NET\",\"EBI\",\"EBT\",\"GPS\",\"DPS\",\"BPS\",\"NAV\",\"OFTIC\",\"PENDS\",\"MEAN\",\"STDEV\", \"BPS\", \"CPS\", \"PRE\", \"NDT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 19, 38]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisors = [i for i in range(1, len(company_list) + 1) if len(company_list) % i == 0]\n",
    "divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, OFTIC, PENDS, MEAN, STDEV, BPS, CPS, CPX, CSH, DPS, EBG, EBI, EBS, EBT, ENT, EPS, FFO, GPS, GRM, NAV, NDT, NET, OPR, PRE, ROA, ROE, SAL, Real_Estate_Index_Price, VIX_Close, Gold_Close, Three_Month_Yield, Brent_Close, Hrc_close, commodity_trade_Close, C_Discretionary_Close, C_Staples_Close, Energy_Close, Financials_Close, Health_care_Close, industrials_Close, information_Close, materials_Close, utilities_Close, Sector, numeric_sector, Real_Estate_Index_Price_quarterly_return, VIX_Close_quarterly_return, Gold_Close_quarterly_return, Three_Month_Yield_quarterly_return, Brent_Close_quarterly_return, Hrc_close_quarterly_return, commodity_trade_Close_quarterly_return, C_Discretionary_Close_quarterly_return, C_Staples_Close_quarterly_return, Energy_Close_quarterly_return, Financials_Close_quarterly_return, Health_care_Close_quarterly_return, industrials_Close_quarterly_return, information_Close_quarterly_return, materials_Close_quarterly_return, utilities_Close_quarterly_return]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 61 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data[pivoted_data[\"OFTIC\"] == \"ZTS\"].sort_values(by=\"PENDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoted_data.reset_index(inplace=True, drop=True)\n",
    "# pivoted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "PGR: EPS - PENDS",
         "type": "scatter",
         "x": [
          "2011-06-30",
          "2011-09-30",
          "2011-12-31",
          "2012-03-31",
          "2012-06-30",
          "2012-09-30",
          "2012-12-31",
          "2013-03-31",
          "2013-06-30",
          "2013-09-30",
          "2013-12-31",
          "2014-03-31",
          "2014-06-30",
          "2014-09-30",
          "2014-12-31",
          "2015-03-31",
          "2015-06-30",
          "2015-09-30",
          "2015-12-31",
          "2016-03-31",
          "2016-06-30",
          "2016-09-30",
          "2016-12-31",
          "2017-03-31",
          "2017-06-30",
          "2017-09-30",
          "2017-12-31",
          "2018-03-31",
          "2018-06-30",
          "2018-09-30",
          "2018-12-31",
          "2019-03-31",
          "2019-06-30",
          "2019-09-30",
          "2019-12-31"
         ],
         "y": [
          0.35,
          0.29,
          0.39,
          0.34,
          0.2,
          0.27,
          0.34,
          0.42,
          0.39,
          0.3599999999999999,
          0.41,
          0.41,
          0.45,
          0.46,
          0.6,
          0.46,
          0.5300000000000001,
          0.49,
          0.5300000000000001,
          0.42,
          0.29,
          0.3599999999999999,
          0.63,
          0.67,
          0.5900000000000001,
          0.41,
          0.82,
          1.29,
          1.15,
          1.33,
          1.24,
          1.33,
          1.42,
          1.33,
          1.18
         ]
        }
       ],
       "layout": {
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "PGR: EPS - PENDS"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "rangemode": "tozero",
         "side": "left",
         "title": {
          "text": "EPS"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_symbol = \"PGR\"\n",
    "stock = pivoted_data[pivoted_data[\"OFTIC\"] == stock_symbol].sort_values(by=\"PENDS\")\n",
    "\n",
    "stock_features_dict = {}\n",
    "for column in stock.columns:\n",
    "    stock_features_dict[column] = stock[column]\n",
    "\n",
    "trace = go.Scatter(x=stock_features_dict[\"PENDS\"], y=stock_features_dict[\"EPS\"], mode=\"lines+markers\", name=f\"{stock_symbol}: EPS - PENDS\")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = f\"{stock_symbol}: EPS - PENDS\",\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='EPS', side='left', rangemode='tozero'),\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>EPS</th>\n",
       "      <th>PENDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12444</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2011-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12445</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2011-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12446</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.73</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2012-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2012-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2012-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2012-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12451</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2013-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>TROW</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2013-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2013-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12454</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2013-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2014-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2014-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2014-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12458</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2015-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12460</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2015-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12461</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2015-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2016-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12464</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2016-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12465</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2016-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2016-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12467</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2017-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12468</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2017-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12469</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.56</td>\n",
       "      <td>2017-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2017-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12471</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2018-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.77</td>\n",
       "      <td>2018-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12473</th>\n",
       "      <td>TROW</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2018-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12474</th>\n",
       "      <td>TROW</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12475</th>\n",
       "      <td>TROW</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2019-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12476</th>\n",
       "      <td>TROW</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2019-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12477</th>\n",
       "      <td>TROW</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2019-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12478</th>\n",
       "      <td>TROW</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      OFTIC   EPS       PENDS\n",
       "12444  TROW  0.76  2011-06-30\n",
       "12445  TROW  0.71  2011-09-30\n",
       "12446  TROW  0.73  2011-12-31\n",
       "12447  TROW  0.75  2012-03-31\n",
       "12448  TROW  0.79  2012-06-30\n",
       "12449  TROW  0.87  2012-09-30\n",
       "12450  TROW  0.88  2012-12-31\n",
       "12451  TROW  0.91  2013-03-31\n",
       "12452  TROW  0.92  2013-06-30\n",
       "12453  TROW  1.00  2013-09-30\n",
       "12454  TROW  1.06  2013-12-31\n",
       "12455  TROW  1.05  2014-03-31\n",
       "12456  TROW  1.13  2014-06-30\n",
       "12457  TROW  1.12  2014-09-30\n",
       "12458  TROW  1.18  2014-12-31\n",
       "12459  TROW  1.13  2015-03-31\n",
       "12460  TROW  1.24  2015-06-30\n",
       "12461  TROW  1.06  2015-09-30\n",
       "12462  TROW  1.17  2015-12-31\n",
       "12463  TROW  1.15  2016-03-31\n",
       "12464  TROW  1.42  2016-06-30\n",
       "12465  TROW  1.28  2016-09-30\n",
       "12466  TROW  1.50  2016-12-31\n",
       "12467  TROW  1.42  2017-03-31\n",
       "12468  TROW  1.50  2017-06-30\n",
       "12469  TROW  1.56  2017-09-30\n",
       "12470  TROW  1.65  2017-12-31\n",
       "12471  TROW  1.77  2018-03-31\n",
       "12472  TROW  1.77  2018-06-30\n",
       "12473  TROW  2.30  2018-09-30\n",
       "12474  TROW  1.41  2018-12-31\n",
       "12475  TROW  2.09  2019-03-31\n",
       "12476  TROW  2.03  2019-06-30\n",
       "12477  TROW  2.13  2019-09-30\n",
       "12478  TROW  2.03  2019-12-31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data[pivoted_data[\"OFTIC\"]==\"TROW\"][[\"OFTIC\",\"EPS\",\"PENDS\"]].sort_values(by=[\"OFTIC\", \"PENDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPS</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.780</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.830</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.740</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.825</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.805</td>\n",
       "      <td>AFL</td>\n",
       "      <td>2012-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14055</th>\n",
       "      <td>1.080</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14056</th>\n",
       "      <td>1.040</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14057</th>\n",
       "      <td>0.990</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-06-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>1.170</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14059</th>\n",
       "      <td>0.970</td>\n",
       "      <td>ZION</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1330 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         EPS OFTIC       PENDS\n",
       "51     0.780   AFL  2011-06-30\n",
       "52     0.830   AFL  2011-09-30\n",
       "53     0.740   AFL  2011-12-31\n",
       "54     0.825   AFL  2012-03-31\n",
       "55     0.805   AFL  2012-06-30\n",
       "...      ...   ...         ...\n",
       "14055  1.080  ZION  2018-12-31\n",
       "14056  1.040  ZION  2019-03-31\n",
       "14057  0.990  ZION  2019-06-30\n",
       "14058  1.170  ZION  2019-09-30\n",
       "14059  0.970  ZION  2019-12-31\n",
       "\n",
       "[1330 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data[[\"EPS\",\"OFTIC\",\"PENDS\"]].sort_values(by=[\"OFTIC\", \"PENDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51       0.780\n",
       "52       0.830\n",
       "53       0.740\n",
       "54       0.825\n",
       "55       0.805\n",
       "         ...  \n",
       "14055    1.080\n",
       "14056    1.040\n",
       "14057    0.990\n",
       "14058    1.170\n",
       "14059    0.970\n",
       "Name: EPS, Length: 1330, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = pivoted_data.drop(columns=[\"EPS\"])\n",
    "y_ = pivoted_data['EPS']\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82c68f2b5e3475e833f647d2b4bdfbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "company_dict = {}\n",
    "\n",
    "for company in tqdm(pivoted_data[\"OFTIC\"].unique()):\n",
    "    comp_data = pivoted_data[pivoted_data[\"OFTIC\"] == company].sort_values(by='PENDS')\n",
    "    X = comp_data.drop(columns=[\"EPS\", \"Sector\",\"PENDS\",\"OFTIC\",\"Date\"])\n",
    "    y = comp_data['EPS']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    X_train_df = X_train\n",
    "    X_test_df = X_test\n",
    "    y_train_df = y_train\n",
    "    y_test_df = y_test\n",
    "\n",
    "    for column in X_train.columns:\n",
    "        if \"embedding\" in column:\n",
    "            continue\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train[[column]].values)\n",
    "        X_train_df[column] = X_train_scaled\n",
    "            \n",
    "        X_test_scaled = scaler.transform(X_test[[column]].values)\n",
    "        X_test_df[column] = X_test_scaled\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "\n",
    "    y_train_scaled = pd.DataFrame(scaler_y.fit_transform(y_train.values.reshape(-1, 1)), columns=['EPS'], index=y_train.index)\n",
    "    y_train_df = y_train_scaled\n",
    "            \n",
    "    y_test_scaled = pd.DataFrame(scaler_y.transform(y_test.values.reshape(-1, 1)), columns=['EPS'], index=y_test.index)\n",
    "    y_test_df = y_test_scaled\n",
    "\n",
    "    X_train_df[\"lagged_EPS\"] = y_train_df\n",
    "    X_test_df[\"lagged_EPS\"] = y_test_df\n",
    "\n",
    "    company_dict[company] = {\"X_train\": X_train_df, \"X_test\": X_test_df, \"y_train\": y_train_df, \"y_test\": y_test_df, \"scaler_y\": scaler_y}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>EBG</th>\n",
       "      <th>EBI</th>\n",
       "      <th>EBS</th>\n",
       "      <th>...</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "      <th>lagged_EPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>1.071070</td>\n",
       "      <td>0.302241</td>\n",
       "      <td>1.138858</td>\n",
       "      <td>0.968741</td>\n",
       "      <td>0.732330</td>\n",
       "      <td>0.913012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862455</td>\n",
       "      <td>0.714346</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735004</td>\n",
       "      <td>0.251749</td>\n",
       "      <td>0.864275</td>\n",
       "      <td>0.437380</td>\n",
       "      <td>0.515463</td>\n",
       "      <td>0.484770</td>\n",
       "      <td>0.538344</td>\n",
       "      <td>0.693463</td>\n",
       "      <td>0.445998</td>\n",
       "      <td>0.871560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>1.232931</td>\n",
       "      <td>0.099889</td>\n",
       "      <td>1.312640</td>\n",
       "      <td>1.136925</td>\n",
       "      <td>0.738867</td>\n",
       "      <td>1.218710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.139008</td>\n",
       "      <td>0.993442</td>\n",
       "      <td>1.087978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712070</td>\n",
       "      <td>0.566433</td>\n",
       "      <td>0.545328</td>\n",
       "      <td>0.600558</td>\n",
       "      <td>0.953648</td>\n",
       "      <td>0.836612</td>\n",
       "      <td>0.622477</td>\n",
       "      <td>0.637155</td>\n",
       "      <td>0.375589</td>\n",
       "      <td>1.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>1.045968</td>\n",
       "      <td>0.365504</td>\n",
       "      <td>0.998169</td>\n",
       "      <td>0.957238</td>\n",
       "      <td>1.365191</td>\n",
       "      <td>1.079371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.093243</td>\n",
       "      <td>0.835560</td>\n",
       "      <td>0.985656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077257</td>\n",
       "      <td>0.076821</td>\n",
       "      <td>-0.047689</td>\n",
       "      <td>0.211407</td>\n",
       "      <td>0.072120</td>\n",
       "      <td>0.099668</td>\n",
       "      <td>-0.358002</td>\n",
       "      <td>0.319639</td>\n",
       "      <td>0.337380</td>\n",
       "      <td>0.954128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>1.355125</td>\n",
       "      <td>0.274253</td>\n",
       "      <td>1.360514</td>\n",
       "      <td>1.014460</td>\n",
       "      <td>0.950904</td>\n",
       "      <td>0.416721</td>\n",
       "      <td>0.300325</td>\n",
       "      <td>0.387881</td>\n",
       "      <td>1.248482</td>\n",
       "      <td>0.482935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.980329</td>\n",
       "      <td>0.838735</td>\n",
       "      <td>0.929415</td>\n",
       "      <td>0.694981</td>\n",
       "      <td>0.650111</td>\n",
       "      <td>1.026594</td>\n",
       "      <td>1.033229</td>\n",
       "      <td>0.892645</td>\n",
       "      <td>0.778369</td>\n",
       "      <td>1.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>1.320314</td>\n",
       "      <td>0.452580</td>\n",
       "      <td>1.610092</td>\n",
       "      <td>1.077870</td>\n",
       "      <td>1.792527</td>\n",
       "      <td>0.979763</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>0.962517</td>\n",
       "      <td>0.799854</td>\n",
       "      <td>0.980050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623823</td>\n",
       "      <td>0.512132</td>\n",
       "      <td>0.461833</td>\n",
       "      <td>0.681903</td>\n",
       "      <td>0.455872</td>\n",
       "      <td>0.667441</td>\n",
       "      <td>0.510931</td>\n",
       "      <td>0.780009</td>\n",
       "      <td>0.430992</td>\n",
       "      <td>1.119266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>1.278996</td>\n",
       "      <td>0.207665</td>\n",
       "      <td>1.770002</td>\n",
       "      <td>1.149039</td>\n",
       "      <td>0.986787</td>\n",
       "      <td>1.276714</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>1.245461</td>\n",
       "      <td>1.185465</td>\n",
       "      <td>1.283922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504605</td>\n",
       "      <td>0.618266</td>\n",
       "      <td>0.377041</td>\n",
       "      <td>0.549573</td>\n",
       "      <td>0.315381</td>\n",
       "      <td>0.588763</td>\n",
       "      <td>0.425888</td>\n",
       "      <td>0.630280</td>\n",
       "      <td>0.715035</td>\n",
       "      <td>1.036697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>1.409959</td>\n",
       "      <td>0.050862</td>\n",
       "      <td>1.677857</td>\n",
       "      <td>0.968847</td>\n",
       "      <td>1.056922</td>\n",
       "      <td>0.950983</td>\n",
       "      <td>0.067114</td>\n",
       "      <td>0.984237</td>\n",
       "      <td>1.025625</td>\n",
       "      <td>0.997427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.596497</td>\n",
       "      <td>0.467675</td>\n",
       "      <td>0.586866</td>\n",
       "      <td>0.739691</td>\n",
       "      <td>0.915764</td>\n",
       "      <td>0.714989</td>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.783228</td>\n",
       "      <td>0.304654</td>\n",
       "      <td>0.899083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MEAN     STDEV       BPS       CPS       CPX       CSH       DPS  \\\n",
       "9973  1.071070  0.302241  1.138858  0.968741  0.732330  0.913012  0.000000   \n",
       "9974  1.232931  0.099889  1.312640  1.136925  0.738867  1.218710  0.000000   \n",
       "9975  1.045968  0.365504  0.998169  0.957238  1.365191  1.079371  0.000000   \n",
       "9976  1.355125  0.274253  1.360514  1.014460  0.950904  0.416721  0.300325   \n",
       "9977  1.320314  0.452580  1.610092  1.077870  1.792527  0.979763  0.067114   \n",
       "9978  1.278996  0.207665  1.770002  1.149039  0.986787  1.276714  0.067114   \n",
       "9979  1.409959  0.050862  1.677857  0.968847  1.056922  0.950983  0.067114   \n",
       "\n",
       "           EBG       EBI       EBS  ...  \\\n",
       "9973  0.862455  0.714346  0.804487  ...   \n",
       "9974  1.139008  0.993442  1.087978  ...   \n",
       "9975  1.093243  0.835560  0.985656  ...   \n",
       "9976  0.387881  1.248482  0.482935  ...   \n",
       "9977  0.962517  0.799854  0.980050  ...   \n",
       "9978  1.245461  1.185465  1.283922  ...   \n",
       "9979  0.984237  1.025625  0.997427  ...   \n",
       "\n",
       "      C_Discretionary_Close_quarterly_return  \\\n",
       "9973                                0.735004   \n",
       "9974                                0.712070   \n",
       "9975                               -0.077257   \n",
       "9976                                0.980329   \n",
       "9977                                0.623823   \n",
       "9978                                0.504605   \n",
       "9979                                0.596497   \n",
       "\n",
       "      C_Staples_Close_quarterly_return  Energy_Close_quarterly_return  \\\n",
       "9973                          0.251749                       0.864275   \n",
       "9974                          0.566433                       0.545328   \n",
       "9975                          0.076821                      -0.047689   \n",
       "9976                          0.838735                       0.929415   \n",
       "9977                          0.512132                       0.461833   \n",
       "9978                          0.618266                       0.377041   \n",
       "9979                          0.467675                       0.586866   \n",
       "\n",
       "      Financials_Close_quarterly_return  Health_care_Close_quarterly_return  \\\n",
       "9973                           0.437380                            0.515463   \n",
       "9974                           0.600558                            0.953648   \n",
       "9975                           0.211407                            0.072120   \n",
       "9976                           0.694981                            0.650111   \n",
       "9977                           0.681903                            0.455872   \n",
       "9978                           0.549573                            0.315381   \n",
       "9979                           0.739691                            0.915764   \n",
       "\n",
       "      industrials_Close_quarterly_return  information_Close_quarterly_return  \\\n",
       "9973                            0.484770                            0.538344   \n",
       "9974                            0.836612                            0.622477   \n",
       "9975                            0.099668                           -0.358002   \n",
       "9976                            1.026594                            1.033229   \n",
       "9977                            0.667441                            0.510931   \n",
       "9978                            0.588763                            0.425888   \n",
       "9979                            0.714989                            0.824818   \n",
       "\n",
       "      materials_Close_quarterly_return  utilities_Close_quarterly_return  \\\n",
       "9973                          0.693463                          0.445998   \n",
       "9974                          0.637155                          0.375589   \n",
       "9975                          0.319639                          0.337380   \n",
       "9976                          0.892645                          0.778369   \n",
       "9977                          0.780009                          0.430992   \n",
       "9978                          0.630280                          0.715035   \n",
       "9979                          0.783228                          0.304654   \n",
       "\n",
       "      lagged_EPS  \n",
       "9973    0.871560  \n",
       "9974    1.036697  \n",
       "9975    0.954128  \n",
       "9976    1.036697  \n",
       "9977    1.119266  \n",
       "9978    1.036697  \n",
       "9979    0.899083  \n",
       "\n",
       "[7 rows x 57 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"PGR\"][\"X_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>EBG</th>\n",
       "      <th>EBI</th>\n",
       "      <th>EBS</th>\n",
       "      <th>...</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "      <th>lagged_EPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12444</th>\n",
       "      <td>0.094055</td>\n",
       "      <td>0.093334</td>\n",
       "      <td>0.032658</td>\n",
       "      <td>0.593023</td>\n",
       "      <td>0.208539</td>\n",
       "      <td>0.251689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267056</td>\n",
       "      <td>0.113715</td>\n",
       "      <td>0.084615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563929</td>\n",
       "      <td>0.553415</td>\n",
       "      <td>0.415602</td>\n",
       "      <td>0.374808</td>\n",
       "      <td>0.697910</td>\n",
       "      <td>0.550621</td>\n",
       "      <td>0.254508</td>\n",
       "      <td>0.602163</td>\n",
       "      <td>0.550142</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12445</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.682171</td>\n",
       "      <td>0.198291</td>\n",
       "      <td>0.473431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435979</td>\n",
       "      <td>0.005513</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.332943</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12446</th>\n",
       "      <td>0.087462</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.049428</td>\n",
       "      <td>0.205426</td>\n",
       "      <td>0.465784</td>\n",
       "      <td>0.433615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.874480</td>\n",
       "      <td>0.794207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.743001</td>\n",
       "      <td>0.775029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.642385</td>\n",
       "      <td>0.018868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>0.153808</td>\n",
       "      <td>0.012053</td>\n",
       "      <td>0.115627</td>\n",
       "      <td>0.577519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120564</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.106131</td>\n",
       "      <td>0.077877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.577455</td>\n",
       "      <td>0.645411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.740234</td>\n",
       "      <td>0.875023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905314</td>\n",
       "      <td>0.190879</td>\n",
       "      <td>0.037736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>0.147095</td>\n",
       "      <td>0.054947</td>\n",
       "      <td>0.051194</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.139073</td>\n",
       "      <td>0.358470</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.285772</td>\n",
       "      <td>0.118539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360339</td>\n",
       "      <td>0.443646</td>\n",
       "      <td>0.366688</td>\n",
       "      <td>0.352466</td>\n",
       "      <td>0.459443</td>\n",
       "      <td>0.455177</td>\n",
       "      <td>0.128563</td>\n",
       "      <td>0.528287</td>\n",
       "      <td>0.574203</td>\n",
       "      <td>0.075472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>0.197957</td>\n",
       "      <td>0.162969</td>\n",
       "      <td>0.195066</td>\n",
       "      <td>0.740310</td>\n",
       "      <td>0.163355</td>\n",
       "      <td>0.555681</td>\n",
       "      <td>0.013575</td>\n",
       "      <td>0.465401</td>\n",
       "      <td>0.225017</td>\n",
       "      <td>0.014902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699222</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.814483</td>\n",
       "      <td>0.662573</td>\n",
       "      <td>0.630793</td>\n",
       "      <td>0.646533</td>\n",
       "      <td>0.580276</td>\n",
       "      <td>0.751507</td>\n",
       "      <td>0.237308</td>\n",
       "      <td>0.150943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450</th>\n",
       "      <td>0.237328</td>\n",
       "      <td>0.127765</td>\n",
       "      <td>0.179178</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.209713</td>\n",
       "      <td>0.477393</td>\n",
       "      <td>0.466063</td>\n",
       "      <td>0.405140</td>\n",
       "      <td>0.232943</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509137</td>\n",
       "      <td>0.228173</td>\n",
       "      <td>0.484243</td>\n",
       "      <td>0.632134</td>\n",
       "      <td>0.395993</td>\n",
       "      <td>0.682694</td>\n",
       "      <td>0.065555</td>\n",
       "      <td>0.694129</td>\n",
       "      <td>0.123988</td>\n",
       "      <td>0.160377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12451</th>\n",
       "      <td>0.310290</td>\n",
       "      <td>0.276269</td>\n",
       "      <td>0.256852</td>\n",
       "      <td>0.864341</td>\n",
       "      <td>0.245033</td>\n",
       "      <td>0.164767</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>0.131730</td>\n",
       "      <td>0.272226</td>\n",
       "      <td>0.096496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867086</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.824606</td>\n",
       "      <td>0.766053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.856411</td>\n",
       "      <td>0.490786</td>\n",
       "      <td>0.753780</td>\n",
       "      <td>0.874442</td>\n",
       "      <td>0.188679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>0.312487</td>\n",
       "      <td>0.412637</td>\n",
       "      <td>0.322168</td>\n",
       "      <td>0.585271</td>\n",
       "      <td>0.154525</td>\n",
       "      <td>0.366330</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>0.300360</td>\n",
       "      <td>0.362509</td>\n",
       "      <td>0.171945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683277</td>\n",
       "      <td>0.337496</td>\n",
       "      <td>0.520387</td>\n",
       "      <td>0.669756</td>\n",
       "      <td>0.556774</td>\n",
       "      <td>0.638333</td>\n",
       "      <td>0.345915</td>\n",
       "      <td>0.589659</td>\n",
       "      <td>0.137113</td>\n",
       "      <td>0.198113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>0.346015</td>\n",
       "      <td>0.078710</td>\n",
       "      <td>0.395428</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.163355</td>\n",
       "      <td>0.699661</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>0.644527</td>\n",
       "      <td>0.455892</td>\n",
       "      <td>0.425983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720867</td>\n",
       "      <td>0.364506</td>\n",
       "      <td>0.697205</td>\n",
       "      <td>0.570106</td>\n",
       "      <td>0.657827</td>\n",
       "      <td>0.820146</td>\n",
       "      <td>0.483967</td>\n",
       "      <td>0.884706</td>\n",
       "      <td>0.280954</td>\n",
       "      <td>0.273585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12454</th>\n",
       "      <td>0.445362</td>\n",
       "      <td>0.164102</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.430464</td>\n",
       "      <td>0.626933</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>0.563358</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>0.523722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815318</td>\n",
       "      <td>0.721849</td>\n",
       "      <td>0.718605</td>\n",
       "      <td>0.736675</td>\n",
       "      <td>0.784887</td>\n",
       "      <td>0.921757</td>\n",
       "      <td>0.739156</td>\n",
       "      <td>0.896793</td>\n",
       "      <td>0.388603</td>\n",
       "      <td>0.330189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>0.486848</td>\n",
       "      <td>0.122963</td>\n",
       "      <td>0.550298</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.177351</td>\n",
       "      <td>0.385959</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.352571</td>\n",
       "      <td>0.535837</td>\n",
       "      <td>0.242103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351609</td>\n",
       "      <td>0.357904</td>\n",
       "      <td>0.567174</td>\n",
       "      <td>0.566311</td>\n",
       "      <td>0.628894</td>\n",
       "      <td>0.585056</td>\n",
       "      <td>0.370265</td>\n",
       "      <td>0.701276</td>\n",
       "      <td>0.744189</td>\n",
       "      <td>0.320755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>0.504697</td>\n",
       "      <td>0.140808</td>\n",
       "      <td>0.634856</td>\n",
       "      <td>0.538760</td>\n",
       "      <td>0.183041</td>\n",
       "      <td>0.368290</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.331638</td>\n",
       "      <td>0.618539</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.569146</td>\n",
       "      <td>0.518181</td>\n",
       "      <td>0.857910</td>\n",
       "      <td>0.557225</td>\n",
       "      <td>0.571591</td>\n",
       "      <td>0.670695</td>\n",
       "      <td>0.512528</td>\n",
       "      <td>0.769539</td>\n",
       "      <td>0.629910</td>\n",
       "      <td>0.396226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>0.436745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628916</td>\n",
       "      <td>0.895349</td>\n",
       "      <td>0.189452</td>\n",
       "      <td>0.488252</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.445993</td>\n",
       "      <td>0.672295</td>\n",
       "      <td>0.561538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458408</td>\n",
       "      <td>0.400438</td>\n",
       "      <td>0.317990</td>\n",
       "      <td>0.559476</td>\n",
       "      <td>0.612164</td>\n",
       "      <td>0.535993</td>\n",
       "      <td>0.457789</td>\n",
       "      <td>0.640706</td>\n",
       "      <td>0.083523</td>\n",
       "      <td>0.386792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12458</th>\n",
       "      <td>0.457114</td>\n",
       "      <td>0.360914</td>\n",
       "      <td>0.689351</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.289211</td>\n",
       "      <td>0.578705</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.509878</td>\n",
       "      <td>0.642316</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744675</td>\n",
       "      <td>0.698663</td>\n",
       "      <td>0.239577</td>\n",
       "      <td>0.668038</td>\n",
       "      <td>0.685892</td>\n",
       "      <td>0.755673</td>\n",
       "      <td>0.442509</td>\n",
       "      <td>0.591727</td>\n",
       "      <td>0.884659</td>\n",
       "      <td>0.443396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459</th>\n",
       "      <td>0.411755</td>\n",
       "      <td>0.254840</td>\n",
       "      <td>0.553069</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.289486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634735</td>\n",
       "      <td>0.345769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614677</td>\n",
       "      <td>0.373267</td>\n",
       "      <td>0.502548</td>\n",
       "      <td>0.460867</td>\n",
       "      <td>0.648891</td>\n",
       "      <td>0.542789</td>\n",
       "      <td>0.314448</td>\n",
       "      <td>0.653670</td>\n",
       "      <td>0.036382</td>\n",
       "      <td>0.396226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12460</th>\n",
       "      <td>0.376414</td>\n",
       "      <td>0.376522</td>\n",
       "      <td>0.537800</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.467830</td>\n",
       "      <td>0.338546</td>\n",
       "      <td>0.095023</td>\n",
       "      <td>0.319994</td>\n",
       "      <td>0.738112</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512965</td>\n",
       "      <td>0.240142</td>\n",
       "      <td>0.474809</td>\n",
       "      <td>0.542188</td>\n",
       "      <td>0.518372</td>\n",
       "      <td>0.498658</td>\n",
       "      <td>0.302671</td>\n",
       "      <td>0.623027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12461</th>\n",
       "      <td>0.316145</td>\n",
       "      <td>0.166019</td>\n",
       "      <td>0.497621</td>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.628082</td>\n",
       "      <td>0.702593</td>\n",
       "      <td>0.095023</td>\n",
       "      <td>0.609546</td>\n",
       "      <td>0.567540</td>\n",
       "      <td>0.553846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360428</td>\n",
       "      <td>0.309052</td>\n",
       "      <td>0.093218</td>\n",
       "      <td>0.358897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373183</td>\n",
       "      <td>0.134261</td>\n",
       "      <td>0.200419</td>\n",
       "      <td>0.520242</td>\n",
       "      <td>0.330189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>0.252885</td>\n",
       "      <td>0.140150</td>\n",
       "      <td>0.468467</td>\n",
       "      <td>0.399225</td>\n",
       "      <td>0.562309</td>\n",
       "      <td>0.231221</td>\n",
       "      <td>0.095023</td>\n",
       "      <td>0.196447</td>\n",
       "      <td>0.556168</td>\n",
       "      <td>0.561538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642971</td>\n",
       "      <td>0.675355</td>\n",
       "      <td>0.516327</td>\n",
       "      <td>0.632847</td>\n",
       "      <td>0.753213</td>\n",
       "      <td>0.750282</td>\n",
       "      <td>0.622293</td>\n",
       "      <td>0.865070</td>\n",
       "      <td>0.312259</td>\n",
       "      <td>0.433962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>0.310918</td>\n",
       "      <td>0.137207</td>\n",
       "      <td>0.537491</td>\n",
       "      <td>0.290698</td>\n",
       "      <td>0.556414</td>\n",
       "      <td>0.087991</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.105211</td>\n",
       "      <td>0.404204</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502674</td>\n",
       "      <td>0.586610</td>\n",
       "      <td>0.616098</td>\n",
       "      <td>0.391941</td>\n",
       "      <td>0.193544</td>\n",
       "      <td>0.706730</td>\n",
       "      <td>0.440192</td>\n",
       "      <td>0.724246</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.415094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12464</th>\n",
       "      <td>0.254980</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.571711</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.816726</td>\n",
       "      <td>0.462325</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.484558</td>\n",
       "      <td>0.537905</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415448</td>\n",
       "      <td>0.532922</td>\n",
       "      <td>0.805169</td>\n",
       "      <td>0.552953</td>\n",
       "      <td>0.640224</td>\n",
       "      <td>0.607723</td>\n",
       "      <td>0.222635</td>\n",
       "      <td>0.729638</td>\n",
       "      <td>0.582574</td>\n",
       "      <td>0.669811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12465</th>\n",
       "      <td>0.206408</td>\n",
       "      <td>0.163385</td>\n",
       "      <td>0.601368</td>\n",
       "      <td>0.980620</td>\n",
       "      <td>0.825716</td>\n",
       "      <td>0.753099</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.758609</td>\n",
       "      <td>0.627498</td>\n",
       "      <td>0.661538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548890</td>\n",
       "      <td>0.185169</td>\n",
       "      <td>0.637586</td>\n",
       "      <td>0.605185</td>\n",
       "      <td>0.440133</td>\n",
       "      <td>0.695681</td>\n",
       "      <td>0.687438</td>\n",
       "      <td>0.720234</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>0.537736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>0.233682</td>\n",
       "      <td>0.269639</td>\n",
       "      <td>0.664637</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>0.675957</td>\n",
       "      <td>0.269933</td>\n",
       "      <td>0.104072</td>\n",
       "      <td>0.266065</td>\n",
       "      <td>0.929359</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519877</td>\n",
       "      <td>0.217753</td>\n",
       "      <td>0.716534</td>\n",
       "      <td>0.975962</td>\n",
       "      <td>0.250981</td>\n",
       "      <td>0.759025</td>\n",
       "      <td>0.351791</td>\n",
       "      <td>0.746572</td>\n",
       "      <td>0.273155</td>\n",
       "      <td>0.745283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12467</th>\n",
       "      <td>0.229971</td>\n",
       "      <td>0.195551</td>\n",
       "      <td>0.688468</td>\n",
       "      <td>0.492248</td>\n",
       "      <td>0.633838</td>\n",
       "      <td>0.321590</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.377518</td>\n",
       "      <td>0.613715</td>\n",
       "      <td>0.684615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739809</td>\n",
       "      <td>0.608066</td>\n",
       "      <td>0.374152</td>\n",
       "      <td>0.563368</td>\n",
       "      <td>0.719023</td>\n",
       "      <td>0.704673</td>\n",
       "      <td>0.689965</td>\n",
       "      <td>0.781206</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.669811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12468</th>\n",
       "      <td>0.361703</td>\n",
       "      <td>0.469629</td>\n",
       "      <td>0.757315</td>\n",
       "      <td>0.116279</td>\n",
       "      <td>0.529801</td>\n",
       "      <td>0.531967</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.588738</td>\n",
       "      <td>0.737422</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527188</td>\n",
       "      <td>0.379983</td>\n",
       "      <td>0.375917</td>\n",
       "      <td>0.605895</td>\n",
       "      <td>0.669414</td>\n",
       "      <td>0.708008</td>\n",
       "      <td>0.405431</td>\n",
       "      <td>0.710836</td>\n",
       "      <td>0.372723</td>\n",
       "      <td>0.745283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12469</th>\n",
       "      <td>0.590805</td>\n",
       "      <td>0.507251</td>\n",
       "      <td>0.872942</td>\n",
       "      <td>0.531008</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.878360</td>\n",
       "      <td>0.930769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478399</td>\n",
       "      <td>0.267731</td>\n",
       "      <td>0.687231</td>\n",
       "      <td>0.625232</td>\n",
       "      <td>0.538818</td>\n",
       "      <td>0.695998</td>\n",
       "      <td>0.606325</td>\n",
       "      <td>0.783834</td>\n",
       "      <td>0.411674</td>\n",
       "      <td>0.801887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>0.863722</td>\n",
       "      <td>0.373435</td>\n",
       "      <td>0.956212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.496475</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.477085</td>\n",
       "      <td>0.818057</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792180</td>\n",
       "      <td>0.600636</td>\n",
       "      <td>0.688125</td>\n",
       "      <td>0.694820</td>\n",
       "      <td>0.463216</td>\n",
       "      <td>0.759021</td>\n",
       "      <td>0.613899</td>\n",
       "      <td>0.809395</td>\n",
       "      <td>0.280649</td>\n",
       "      <td>0.886792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12471</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551876</td>\n",
       "      <td>0.361943</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.397696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.386105</td>\n",
       "      <td>0.489765</td>\n",
       "      <td>0.359722</td>\n",
       "      <td>0.532202</td>\n",
       "      <td>0.392452</td>\n",
       "      <td>0.493204</td>\n",
       "      <td>0.122035</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MEAN     STDEV       BPS       CPS       CPX       CSH       DPS  \\\n",
       "12444  0.094055  0.093334  0.032658  0.593023  0.208539  0.251689  0.000000   \n",
       "12445  0.000000  0.052220  0.000000  0.682171  0.198291  0.473431  0.000000   \n",
       "12446  0.087462  0.100018  0.049428  0.205426  0.465784  0.433615  0.000000   \n",
       "12447  0.153808  0.012053  0.115627  0.577519  0.000000  0.120564  0.013575   \n",
       "12448  0.147095  0.054947  0.051194  0.534884  0.139073  0.358470  0.013575   \n",
       "12449  0.197957  0.162969  0.195066  0.740310  0.163355  0.555681  0.013575   \n",
       "12450  0.237328  0.127765  0.179178  0.255814  0.209713  0.477393  0.466063   \n",
       "12451  0.310290  0.276269  0.256852  0.864341  0.245033  0.164767  0.031674   \n",
       "12452  0.312487  0.412637  0.322168  0.585271  0.154525  0.366330  0.031674   \n",
       "12453  0.346015  0.078710  0.395428  0.728682  0.163355  0.699661  0.031674   \n",
       "12454  0.445362  0.164102  0.481486  0.302326  0.430464  0.626933  0.031674   \n",
       "12455  0.486848  0.122963  0.550298  0.895349  0.177351  0.385959  0.058824   \n",
       "12456  0.504697  0.140808  0.634856  0.538760  0.183041  0.368290  0.058824   \n",
       "12457  0.436745  0.000000  0.628916  0.895349  0.189452  0.488252  0.058824   \n",
       "12458  0.457114  0.360914  0.689351  0.302326  0.289211  0.578705  0.058824   \n",
       "12459  0.411755  0.254840  0.553069  0.988372  0.289486  0.000000  1.000000   \n",
       "12460  0.376414  0.376522  0.537800  0.697674  0.467830  0.338546  0.095023   \n",
       "12461  0.316145  0.166019  0.497621  0.686047  0.628082  0.702593  0.095023   \n",
       "12462  0.252885  0.140150  0.468467  0.399225  0.562309  0.231221  0.095023   \n",
       "12463  0.310918  0.137207  0.537491  0.290698  0.556414  0.087991  0.104072   \n",
       "12464  0.254980  0.043036  0.571711  0.720930  0.816726  0.462325  0.104072   \n",
       "12465  0.206408  0.163385  0.601368  0.980620  0.825716  0.753099  0.104072   \n",
       "12466  0.233682  0.269639  0.664637  0.042636  0.675957  0.269933  0.104072   \n",
       "12467  0.229971  0.195551  0.688468  0.492248  0.633838  0.321590  0.117647   \n",
       "12468  0.361703  0.469629  0.757315  0.116279  0.529801  0.531967  0.117647   \n",
       "12469  0.590805  0.507251  0.872942  0.531008  0.768212  1.000000  0.117647   \n",
       "12470  0.863722  0.373435  0.956212  0.000000  1.000000  0.496475  0.117647   \n",
       "12471  1.000000  1.000000  1.000000  1.000000  0.551876  0.361943  0.176471   \n",
       "\n",
       "            EBG       EBI       EBS  ...  \\\n",
       "12444  0.267056  0.113715  0.084615  ...   \n",
       "12445  0.435979  0.005513  0.023077  ...   \n",
       "12446  0.393485  0.000000  0.000000  ...   \n",
       "12447  0.106131  0.077877  0.000000  ...   \n",
       "12448  0.285772  0.118539  0.000000  ...   \n",
       "12449  0.465401  0.225017  0.014902  ...   \n",
       "12450  0.405140  0.232943  0.102236  ...   \n",
       "12451  0.131730  0.272226  0.096496  ...   \n",
       "12452  0.300360  0.362509  0.171945  ...   \n",
       "12453  0.644527  0.455892  0.425983  ...   \n",
       "12454  0.563358  0.504824  0.523722  ...   \n",
       "12455  0.352571  0.535837  0.242103  ...   \n",
       "12456  0.331638  0.618539  0.523077  ...   \n",
       "12457  0.445993  0.672295  0.561538  ...   \n",
       "12458  0.509878  0.642316  0.646154  ...   \n",
       "12459  0.000000  0.634735  0.345769  ...   \n",
       "12460  0.319994  0.738112  0.661538  ...   \n",
       "12461  0.609546  0.567540  0.553846  ...   \n",
       "12462  0.196447  0.556168  0.561538  ...   \n",
       "12463  0.105211  0.404204  0.446154  ...   \n",
       "12464  0.484558  0.537905  0.569231  ...   \n",
       "12465  0.758609  0.627498  0.661538  ...   \n",
       "12466  0.266065  0.929359  0.946154  ...   \n",
       "12467  0.377518  0.613715  0.684615  ...   \n",
       "12468  0.588738  0.737422  0.815385  ...   \n",
       "12469  1.000000  0.878360  0.930769  ...   \n",
       "12470  0.477085  0.818057  0.853846  ...   \n",
       "12471  0.397696  1.000000  1.000000  ...   \n",
       "\n",
       "       C_Discretionary_Close_quarterly_return  \\\n",
       "12444                                0.563929   \n",
       "12445                                0.000000   \n",
       "12446                                0.874480   \n",
       "12447                                1.000000   \n",
       "12448                                0.360339   \n",
       "12449                                0.699222   \n",
       "12450                                0.509137   \n",
       "12451                                0.867086   \n",
       "12452                                0.683277   \n",
       "12453                                0.720867   \n",
       "12454                                0.815318   \n",
       "12455                                0.351609   \n",
       "12456                                0.569146   \n",
       "12457                                0.458408   \n",
       "12458                                0.744675   \n",
       "12459                                0.614677   \n",
       "12460                                0.512965   \n",
       "12461                                0.360428   \n",
       "12462                                0.642971   \n",
       "12463                                0.502674   \n",
       "12464                                0.415448   \n",
       "12465                                0.548890   \n",
       "12466                                0.519877   \n",
       "12467                                0.739809   \n",
       "12468                                0.527188   \n",
       "12469                                0.478399   \n",
       "12470                                0.792180   \n",
       "12471                                0.552285   \n",
       "\n",
       "       C_Staples_Close_quarterly_return  Energy_Close_quarterly_return  \\\n",
       "12444                          0.553415                       0.415602   \n",
       "12445                          0.114769                       0.000000   \n",
       "12446                          0.794207                       1.000000   \n",
       "12447                          0.577455                       0.645411   \n",
       "12448                          0.443646                       0.366688   \n",
       "12449                          0.491400                       0.814483   \n",
       "12450                          0.228173                       0.484243   \n",
       "12451                          1.000000                       0.824606   \n",
       "12452                          0.337496                       0.520387   \n",
       "12453                          0.364506                       0.697205   \n",
       "12454                          0.721849                       0.718605   \n",
       "12455                          0.357904                       0.567174   \n",
       "12456                          0.518181                       0.857910   \n",
       "12457                          0.400438                       0.317990   \n",
       "12458                          0.698663                       0.239577   \n",
       "12459                          0.373267                       0.502548   \n",
       "12460                          0.240142                       0.474809   \n",
       "12461                          0.309052                       0.093218   \n",
       "12462                          0.675355                       0.516327   \n",
       "12463                          0.586610                       0.616098   \n",
       "12464                          0.532922                       0.805169   \n",
       "12465                          0.185169                       0.637586   \n",
       "12466                          0.217753                       0.716534   \n",
       "12467                          0.608066                       0.374152   \n",
       "12468                          0.379983                       0.375917   \n",
       "12469                          0.267731                       0.687231   \n",
       "12470                          0.600636                       0.688125   \n",
       "12471                          0.000000                       0.386105   \n",
       "\n",
       "       Financials_Close_quarterly_return  Health_care_Close_quarterly_return  \\\n",
       "12444                           0.374808                            0.697910   \n",
       "12445                           0.000000                            0.010463   \n",
       "12446                           0.743001                            0.775029   \n",
       "12447                           1.000000                            0.740234   \n",
       "12448                           0.352466                            0.459443   \n",
       "12449                           0.662573                            0.630793   \n",
       "12450                           0.632134                            0.395993   \n",
       "12451                           0.766053                            1.000000   \n",
       "12452                           0.669756                            0.556774   \n",
       "12453                           0.570106                            0.657827   \n",
       "12454                           0.736675                            0.784887   \n",
       "12455                           0.566311                            0.628894   \n",
       "12456                           0.557225                            0.571591   \n",
       "12457                           0.559476                            0.612164   \n",
       "12458                           0.668038                            0.685892   \n",
       "12459                           0.460867                            0.648891   \n",
       "12460                           0.542188                            0.518372   \n",
       "12461                           0.358897                            0.000000   \n",
       "12462                           0.632847                            0.753213   \n",
       "12463                           0.391941                            0.193544   \n",
       "12464                           0.552953                            0.640224   \n",
       "12465                           0.605185                            0.440133   \n",
       "12466                           0.975962                            0.250981   \n",
       "12467                           0.563368                            0.719023   \n",
       "12468                           0.605895                            0.669414   \n",
       "12469                           0.625232                            0.538818   \n",
       "12470                           0.694820                            0.463216   \n",
       "12471                           0.489765                            0.359722   \n",
       "\n",
       "       industrials_Close_quarterly_return  information_Close_quarterly_return  \\\n",
       "12444                            0.550621                            0.254508   \n",
       "12445                            0.000000                            0.000000   \n",
       "12446                            1.000000                            0.600125   \n",
       "12447                            0.875023                            1.000000   \n",
       "12448                            0.455177                            0.128563   \n",
       "12449                            0.646533                            0.580276   \n",
       "12450                            0.682694                            0.065555   \n",
       "12451                            0.856411                            0.490786   \n",
       "12452                            0.638333                            0.345915   \n",
       "12453                            0.820146                            0.483967   \n",
       "12454                            0.921757                            0.739156   \n",
       "12455                            0.585056                            0.370265   \n",
       "12456                            0.670695                            0.512528   \n",
       "12457                            0.535993                            0.457789   \n",
       "12458                            0.755673                            0.442509   \n",
       "12459                            0.542789                            0.314448   \n",
       "12460                            0.498658                            0.302671   \n",
       "12461                            0.373183                            0.134261   \n",
       "12462                            0.750282                            0.622293   \n",
       "12463                            0.706730                            0.440192   \n",
       "12464                            0.607723                            0.222635   \n",
       "12465                            0.695681                            0.687438   \n",
       "12466                            0.759025                            0.351791   \n",
       "12467                            0.704673                            0.689965   \n",
       "12468                            0.708008                            0.405431   \n",
       "12469                            0.695998                            0.606325   \n",
       "12470                            0.759021                            0.613899   \n",
       "12471                            0.532202                            0.392452   \n",
       "\n",
       "       materials_Close_quarterly_return  utilities_Close_quarterly_return  \\\n",
       "12444                          0.602163                          0.550142   \n",
       "12445                          0.000000                          0.332943   \n",
       "12446                          1.000000                          0.642385   \n",
       "12447                          0.905314                          0.190879   \n",
       "12448                          0.528287                          0.574203   \n",
       "12449                          0.751507                          0.237308   \n",
       "12450                          0.694129                          0.123988   \n",
       "12451                          0.753780                          0.874442   \n",
       "12452                          0.589659                          0.137113   \n",
       "12453                          0.884706                          0.280954   \n",
       "12454                          0.896793                          0.388603   \n",
       "12455                          0.701276                          0.744189   \n",
       "12456                          0.769539                          0.629910   \n",
       "12457                          0.640706                          0.083523   \n",
       "12458                          0.591727                          0.884659   \n",
       "12459                          0.653670                          0.036382   \n",
       "12460                          0.623027                          0.000000   \n",
       "12461                          0.200419                          0.520242   \n",
       "12462                          0.865070                          0.312259   \n",
       "12463                          0.724246                          1.000000   \n",
       "12464                          0.729638                          0.582574   \n",
       "12465                          0.720234                          0.002452   \n",
       "12466                          0.746572                          0.273155   \n",
       "12467                          0.781206                          0.577778   \n",
       "12468                          0.710836                          0.372723   \n",
       "12469                          0.783834                          0.411674   \n",
       "12470                          0.809395                          0.280649   \n",
       "12471                          0.493204                          0.122035   \n",
       "\n",
       "       lagged_EPS  \n",
       "12444    0.047170  \n",
       "12445    0.000000  \n",
       "12446    0.018868  \n",
       "12447    0.037736  \n",
       "12448    0.075472  \n",
       "12449    0.150943  \n",
       "12450    0.160377  \n",
       "12451    0.188679  \n",
       "12452    0.198113  \n",
       "12453    0.273585  \n",
       "12454    0.330189  \n",
       "12455    0.320755  \n",
       "12456    0.396226  \n",
       "12457    0.386792  \n",
       "12458    0.443396  \n",
       "12459    0.396226  \n",
       "12460    0.500000  \n",
       "12461    0.330189  \n",
       "12462    0.433962  \n",
       "12463    0.415094  \n",
       "12464    0.669811  \n",
       "12465    0.537736  \n",
       "12466    0.745283  \n",
       "12467    0.669811  \n",
       "12468    0.745283  \n",
       "12469    0.801887  \n",
       "12470    0.886792  \n",
       "12471    1.000000  \n",
       "\n",
       "[28 rows x 57 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"TROW\"][\"X_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.036697247706422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"PGR\"][\"y_test\"].iloc[1][\"EPS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class RollingWindowDataset(Dataset):\n",
    "    def __init__(self, X, y, device=device):\n",
    "        self.X = X.clone().detach().to(torch.float)\n",
    "        self.y = y.clone().detach().to(torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure idx is within the valid range\n",
    "        if idx > len(self.X):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "\n",
    "        X_window = self.X[idx]\n",
    "        y_target = self.y[idx]  \n",
    "\n",
    "        return X_window.clone().detach().to(torch.float).to(device), y_target.clone().detach().to(torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12444    0.047170\n",
       "12445    0.000000\n",
       "12446    0.018868\n",
       "12447    0.037736\n",
       "12448    0.075472\n",
       "12449    0.150943\n",
       "12450    0.160377\n",
       "12451    0.188679\n",
       "12452    0.198113\n",
       "12453    0.273585\n",
       "12454    0.330189\n",
       "12455    0.320755\n",
       "12456    0.396226\n",
       "12457    0.386792\n",
       "12458    0.443396\n",
       "12459    0.396226\n",
       "12460    0.500000\n",
       "12461    0.330189\n",
       "12462    0.433962\n",
       "12463    0.415094\n",
       "12464    0.669811\n",
       "12465    0.537736\n",
       "12466    0.745283\n",
       "12467    0.669811\n",
       "12468    0.745283\n",
       "12469    0.801887\n",
       "12470    0.886792\n",
       "12471    1.000000\n",
       "Name: lagged_EPS, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_dict[\"TROW\"][\"X_train\"][\"lagged_EPS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "time_steps = 4\n",
    "print(len(company_dict[company][\"X_train\"]))\n",
    "print((len(company_dict[company][\"X_test\"])))\n",
    "\n",
    "for company in company_list:\n",
    "    company_dict[company][\"y_train\"][\"Type\"] = \"Train\"\n",
    "    company_dict[company][\"y_test\"][\"Type\"] = \"Test\"\n",
    "\n",
    "    comp_df_X = pd.concat([company_dict[company]['X_train'], company_dict[company]['X_test']], axis=0)\n",
    "    comp_df_y = pd.concat([company_dict[company]['y_train'], company_dict[company]['y_test']], axis=0)\n",
    "\n",
    "    for i in range((len(comp_df_X)) - time_steps):\n",
    "\n",
    "        if comp_df_y.iloc[i + time_steps][\"Type\"] == \"Train\":\n",
    "            X_train.append(comp_df_X.iloc[i : (i + time_steps)])\n",
    "            y_train.append(comp_df_y.iloc[i + time_steps][\"EPS\"])\n",
    "\n",
    "        elif comp_df_y.iloc[i + time_steps][\"Type\"] == \"Test\":\n",
    "            X_test.append(comp_df_X.iloc[i : (i + time_steps)])\n",
    "            y_test.append(comp_df_y.iloc[i + time_steps][\"EPS\"])\n",
    "\n",
    "    # for i in range((len(company_dict[company][\"X_train\"])) - time_steps):\n",
    "\n",
    "    #     X_train.append(company_dict[company][\"X_train\"].iloc[i : (i + time_steps)])\n",
    "    #     y_train.append(company_dict[company][\"y_train\"][i+time_steps])\n",
    "\n",
    "    # for i in range((len(company_dict[company][\"X_test\"])) - time_steps):\n",
    "\n",
    "    #     X_test.append(company_dict[company][\"X_test\"].iloc[i : (i+ time_steps)])\n",
    "    #     y_test.append(company_dict[company][\"y_test\"][i+time_steps])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42666667, 0.64      , 0.25333333, 0.53333333, 0.44      ,\n",
       "       0.24      , 0.14666667, 0.53333333, 0.49333333, 0.29333333,\n",
       "       0.        , 0.33333333, 0.28      , 0.36      , 0.36      ,\n",
       "       0.58666667, 0.56      , 0.70666667, 0.22666667, 0.50666667,\n",
       "       0.72      , 0.54666667, 0.45333333, 1.        , 0.14473684,\n",
       "       0.11403509, 0.25438596, 0.18421053, 0.18421053, 0.19298246,\n",
       "       0.37280702, 0.25877193, 0.24561404, 0.26315789, 0.52631579,\n",
       "       0.29824561, 0.27192982, 0.24122807, 0.69298246, 0.28947368,\n",
       "       0.30701754, 0.26315789, 0.82017544, 0.33333333, 0.33333333,\n",
       "       0.26315789, 0.72807018, 1.        , 0.22131148, 0.17213115,\n",
       "       0.17213115, 0.22131148, 0.31967213, 0.30327869, 0.30327869,\n",
       "       0.36885246, 0.40983607, 0.42622951, 0.27868852, 0.49180328,\n",
       "       0.44262295, 0.29508197, 0.28688525, 0.45081967, 1.        ,\n",
       "       0.26229508, 0.        , 0.37704918, 0.48360656, 0.50819672,\n",
       "       0.57377049, 0.80327869, 0.27139535, 0.41860465, 0.50395349,\n",
       "       0.71325581, 0.62790698, 0.48837209, 0.86046512, 0.69767442,\n",
       "       0.76744186, 1.        , 0.74418605, 0.90697674, 0.53488372,\n",
       "       0.        , 0.34883721, 0.04651163, 0.41860465, 0.53488372,\n",
       "       0.41860465, 0.34883721, 0.3255814 , 0.39534884, 0.6744186 ,\n",
       "       0.44186047, 0.22058824, 0.22058824, 0.16176471, 0.11764706,\n",
       "       0.29411765, 0.26470588, 0.17647059, 0.22058824, 0.29411765,\n",
       "       0.32352941, 0.24558824, 0.36764706, 0.51470588, 0.47058824,\n",
       "       0.38235294, 0.47058824, 0.5       , 0.70588235, 0.51470588,\n",
       "       0.57352941, 0.67647059, 0.76470588, 0.72058824, 1.        ,\n",
       "       0.06976744, 0.16537468, 0.29198966, 0.2118863 , 0.34108527,\n",
       "       0.27131783, 0.54005168, 0.41343669, 0.53229974, 0.61498708,\n",
       "       0.51421189, 0.53229974, 0.5503876 , 0.56072351, 0.49612403,\n",
       "       0.36692506, 0.50387597, 0.59689922, 0.59689922, 0.49870801,\n",
       "       0.62273902, 0.79844961, 0.88113695, 1.        , 0.31395349,\n",
       "       0.52325581, 0.54651163, 0.52325581, 0.56395349, 0.52906977,\n",
       "       0.98837209, 0.61046512, 0.87209302, 0.5872093 , 0.93023256,\n",
       "       1.        , 0.45348837, 0.        , 0.41860465, 0.38372093,\n",
       "       0.45930233, 0.53488372, 0.59883721, 0.68023256, 0.54651163,\n",
       "       0.60465116, 0.61627907, 0.58139535, 0.07843137, 0.06862745,\n",
       "       0.08823529, 0.11764706, 0.16666667, 0.10784314, 0.15686275,\n",
       "       0.19607843, 0.1372549 , 0.20588235, 0.2745098 , 0.1372549 ,\n",
       "       0.17647059, 0.39215686, 0.2254902 , 0.24509804, 0.23529412,\n",
       "       0.21568627, 0.26470588, 0.41176471, 0.5       , 0.51960784,\n",
       "       0.5       , 1.        , 0.31451613, 0.21774194, 0.27419355,\n",
       "       0.29032258, 0.33870968, 0.35483871, 0.34677419, 0.31451613,\n",
       "       0.37096774, 0.36290323, 0.37096774, 0.31451613, 0.31451613,\n",
       "       0.32258065, 0.2983871 , 0.        , 0.34677419, 0.40322581,\n",
       "       0.46774194, 0.51612903, 0.63709677, 0.74193548, 0.74193548,\n",
       "       1.        , 0.21138211, 0.05691057, 0.        , 0.08130081,\n",
       "       0.24390244, 0.09756098, 0.00813008, 0.16260163, 0.11382114,\n",
       "       0.17073171, 0.2601626 , 0.28455285, 0.2601626 , 0.31707317,\n",
       "       0.23577236, 0.42276423, 0.41463415, 0.34146341, 0.41463415,\n",
       "       0.4796748 , 0.48780488, 0.45528455, 0.39837398, 1.        ,\n",
       "       0.        , 0.66111111, 0.32777778, 0.53888889, 0.58333333,\n",
       "       0.57777778, 0.35      , 0.63333333, 0.67777778, 0.57777778,\n",
       "       0.50555556, 0.65555556, 0.53333333, 0.64444444, 0.42222222,\n",
       "       0.56666667, 0.48333333, 0.6       , 0.35      , 0.4       ,\n",
       "       0.62222222, 0.92222222, 0.42777778, 1.        , 0.118763  ,\n",
       "       0.14721779, 0.18784191, 0.6314637 , 0.53368877, 0.53368877,\n",
       "       0.55625222, 0.61642141, 0.646506  , 0.6615483 , 0.52616762,\n",
       "       0.59385796, 0.6314637 , 0.66906945, 0.48856188, 0.646506  ,\n",
       "       0.73675979, 0.80445013, 0.68411175, 0.70667519, 0.68411175,\n",
       "       0.76684438, 0.76684438, 1.        , 0.33632863, 0.47368421,\n",
       "       0.8267009 , 0.65853659, 0.58279846, 0.4775353 , 0.69833119,\n",
       "       0.62387677, 0.63414634, 0.6944801 , 0.67008986, 0.8703466 ,\n",
       "       0.71758665, 0.4801027 , 0.70860077, 0.45186136, 0.58536585,\n",
       "       0.73427471, 0.75994865, 0.76893453, 0.61489089, 0.75224647,\n",
       "       0.83697047, 1.        , 0.18110236, 0.61417323, 0.42519685,\n",
       "       0.72440945, 0.51968504, 0.81102362, 0.74015748, 0.92913386,\n",
       "       0.24409449, 0.83464567, 0.75590551, 0.81889764, 0.71653543,\n",
       "       0.67716535, 0.84251969, 0.7480315 , 0.24409449, 0.83464567,\n",
       "       0.8503937 , 0.78740157, 0.81889764, 0.47244094, 0.63779528,\n",
       "       1.        , 0.09252669, 0.03558719, 0.02491103, 0.12099644,\n",
       "       0.17793594, 0.09964413, 0.11032028, 0.32384342, 0.14590747,\n",
       "       0.16370107, 0.3202847 , 0.48754448, 0.43060498, 0.4341637 ,\n",
       "       0.56227758, 0.70818505, 0.61921708, 0.54092527, 0.66192171,\n",
       "       0.71530249, 0.73309609, 0.6975089 , 0.6975089 , 1.        ,\n",
       "       0.        , 0.03125   , 0.125     , 0.34375   , 0.28125   ,\n",
       "       0.4375    , 0.53125   , 0.59375   , 0.75      , 0.71875   ,\n",
       "       0.6875    , 0.6875    , 0.6875    , 0.625     , 0.53125   ,\n",
       "       0.25      , 0.46875   , 0.59375   , 0.5625    , 0.625     ,\n",
       "       0.71875   , 0.9375    , 1.        , 0.8125    , 0.26315789,\n",
       "       0.21052632, 0.15789474, 0.15789474, 0.15789474, 0.        ,\n",
       "       0.31578947, 0.36842105, 0.26315789, 0.21052632, 0.47368421,\n",
       "       0.36842105, 0.42105263, 0.36842105, 0.42105263, 0.26315789,\n",
       "       0.42105263, 0.57894737, 0.63157895, 0.68421053, 0.78947368,\n",
       "       0.68421053, 0.73684211, 1.        , 0.14767548, 0.19507748,\n",
       "       0.07566089, 0.20054695, 0.26709207, 0.29535096, 0.15223336,\n",
       "       0.29808569, 0.36189608, 0.42570647, 0.26162261, 0.46216955,\n",
       "       0.40747493, 0.46216955, 0.35278031, 0.4165907 , 0.5077484 ,\n",
       "       0.61713765, 0.4165907 , 0.55332726, 0.63536919, 0.85414768,\n",
       "       0.67183227, 1.        , 0.20754717, 0.20125786, 0.20125786,\n",
       "       0.33962264, 0.35849057, 0.25157233, 0.26415094, 0.35849057,\n",
       "       0.43396226, 0.33962264, 0.43396226, 0.42767296, 0.53459119,\n",
       "       0.42767296, 0.41509434, 0.31446541, 0.54716981, 0.57232704,\n",
       "       0.50314465, 0.65408805, 0.67924528, 0.68553459, 0.67924528,\n",
       "       1.        , 0.04123711, 0.07216495, 0.06185567, 0.11340206,\n",
       "       0.21649485, 0.18556701, 0.11340206, 0.16494845, 0.18556701,\n",
       "       0.16494845, 0.27835052, 0.36082474, 0.35051546, 0.30927835,\n",
       "       0.36082474, 0.48453608, 0.59793814, 0.53608247, 0.59793814,\n",
       "       0.83505155, 0.72164948, 0.6185567 , 0.91752577, 1.        ,\n",
       "       0.3245614 , 0.13157895, 0.24561404, 0.42982456, 0.42105263,\n",
       "       0.19298246, 0.28947368, 0.5       , 0.48245614, 0.28070175,\n",
       "       0.36842105, 0.5877193 , 0.49122807, 0.34210526, 0.4122807 ,\n",
       "       0.59649123, 0.5877193 , 0.39473684, 0.57017544, 0.66666667,\n",
       "       0.66666667, 0.48245614, 0.71052632, 1.        , 0.06896552,\n",
       "       0.05747126, 0.09195402, 0.14942529, 0.16091954, 0.10344828,\n",
       "       0.04597701, 0.02298851, 0.12643678, 0.06896552, 0.05747126,\n",
       "       0.06896552, 0.13793103, 0.18390805, 0.25287356, 0.27586207,\n",
       "       0.37931034, 0.37931034, 0.42528736, 0.50574713, 0.5862069 ,\n",
       "       0.64367816, 0.81609195, 1.        , 0.34895833, 0.58854167,\n",
       "       0.58333333, 0.48958333, 0.5625    , 0.55729167, 0.36458333,\n",
       "       0.296875  , 0.48958333, 0.453125  , 0.45833333, 0.31770833,\n",
       "       0.48958333, 0.46354167, 0.52604167, 0.43229167, 0.515625  ,\n",
       "       0.55208333, 0.48958333, 0.5625    , 0.68229167, 0.609375  ,\n",
       "       0.84375   , 1.        , 0.04761905, 0.01619048, 0.04761905,\n",
       "       0.04761905, 0.01619048, 0.07952381, 0.12714286, 0.1747619 ,\n",
       "       0.14285714, 0.1747619 , 0.22238095, 0.30190476, 0.34952381,\n",
       "       0.42857143, 0.4447619 , 0.47619048, 0.47619048, 0.47619048,\n",
       "       0.54      , 0.77809524, 0.65095238, 0.71428571, 0.69857143,\n",
       "       1.        , 0.07954545, 0.09090909, 0.02272727, 0.04545455,\n",
       "       0.125     , 0.10227273, 0.09090909, 0.09090909, 0.22727273,\n",
       "       0.19318182, 0.30681818, 0.30681818, 0.38636364, 0.32954545,\n",
       "       0.36363636, 0.38636364, 0.44318182, 0.5       , 0.5       ,\n",
       "       0.47727273, 0.57954545, 0.63636364, 0.75      , 1.        ,\n",
       "       0.28421053, 0.        , 0.38947368, 0.35789474, 0.48421053,\n",
       "       0.47368421, 0.53684211, 0.64210526, 0.66315789, 0.77894737,\n",
       "       0.67368421, 0.67368421, 0.67368421, 0.64210526, 0.6       ,\n",
       "       0.54736842, 0.73684211, 0.73684211, 0.86315789, 0.86315789,\n",
       "       0.90526316, 0.87368421, 0.77894737, 1.        , 0.        ,\n",
       "       0.06422018, 0.12844037, 0.20183486, 0.17431193, 0.14678899,\n",
       "       0.19266055, 0.19266055, 0.2293578 , 0.23853211, 0.36697248,\n",
       "       0.23853211, 0.30275229, 0.26605505, 0.30275229, 0.20183486,\n",
       "       0.08256881, 0.14678899, 0.39449541, 0.43119266, 0.35779817,\n",
       "       0.19266055, 0.56880734, 1.        , 0.0390625 , 0.25      ,\n",
       "       0.4375    , 0.4921875 , 0.4375    , 0.5       , 0.546875  ,\n",
       "       0.5234375 , 0.546875  , 0.5       , 0.5390625 , 0.46875   ,\n",
       "       0.5703125 , 0.5859375 , 0.5625    , 0.4140625 , 0.5234375 ,\n",
       "       0.5390625 , 0.640625  , 0.6328125 , 0.7421875 , 0.7890625 ,\n",
       "       0.890625  , 1.        , 0.13432836, 0.22885572, 0.30845771,\n",
       "       0.60199005, 0.6119403 , 0.93034826, 0.56218905, 0.66169154,\n",
       "       0.70646766, 0.56218905, 0.52238806, 0.85572139, 0.91542289,\n",
       "       0.66169154, 0.43283582, 0.55223881, 0.38308458, 0.79104478,\n",
       "       0.69154229, 0.85572139, 0.50746269, 0.96517413, 0.80597015,\n",
       "       1.        , 0.64516129, 0.51612903, 0.51612903, 0.58064516,\n",
       "       0.41935484, 0.48387097, 0.51612903, 0.5483871 , 0.51612903,\n",
       "       0.5483871 , 0.29032258, 0.41935484, 0.48387097, 0.41935484,\n",
       "       0.51612903, 0.41935484, 0.48387097, 0.61290323, 0.58064516,\n",
       "       0.58064516, 0.67741935, 0.64516129, 1.        , 0.96774194,\n",
       "       0.21428571, 0.25392857, 0.25392857, 0.24595238, 0.22214286,\n",
       "       0.44440476, 0.34916667, 0.2777381 , 0.38095238, 0.45238095,\n",
       "       0.39678571, 0.31738095, 0.42857143, 0.4047619 , 0.28571429,\n",
       "       0.42059524, 0.44440476, 0.72214286, 0.55547619, 0.72214286,\n",
       "       0.70630952, 0.87297619, 0.98404762, 1.        , 0.16666667,\n",
       "       0.0952381 , 0.04761905, 0.04761905, 0.11904762, 0.21428571,\n",
       "       0.23809524, 0.26190476, 0.23809524, 0.26190476, 0.26190476,\n",
       "       0.21428571, 0.28571429, 0.33333333, 0.26190476, 0.38095238,\n",
       "       0.4047619 , 0.5       , 0.54761905, 0.61904762, 0.61904762,\n",
       "       0.69047619, 0.73809524, 1.        , 0.17171717, 0.15151515,\n",
       "       0.27272727, 0.12121212, 0.4040404 , 0.35353535, 0.31313131,\n",
       "       0.15151515, 0.55555556, 0.51515152, 0.53535354, 0.33333333,\n",
       "       0.53535354, 0.32323232, 0.37373737, 0.14141414, 0.62626263,\n",
       "       0.51515152, 0.51515152, 0.37373737, 0.83838384, 0.87878788,\n",
       "       1.        , 0.78787879, 0.0754717 , 0.1509434 , 0.16037736,\n",
       "       0.18867925, 0.19811321, 0.27358491, 0.33018868, 0.32075472,\n",
       "       0.39622642, 0.38679245, 0.44339623, 0.39622642, 0.5       ,\n",
       "       0.33018868, 0.43396226, 0.41509434, 0.66981132, 0.53773585,\n",
       "       0.74528302, 0.66981132, 0.74528302, 0.80188679, 0.88679245,\n",
       "       1.        , 0.52798054, 0.76155718, 0.39659367, 0.78345499,\n",
       "       0.73965937, 0.79318735, 0.87347932, 0.93917275, 0.69099757,\n",
       "       0.85644769, 0.96836983, 0.83698297, 0.81021898, 0.93430657,\n",
       "       0.9270073 , 0.78832117, 0.756691  , 0.8053528 , 1.        ,\n",
       "       0.93917275, 0.68856448, 0.44282238, 0.77615572, 0.81995134,\n",
       "       0.28571429, 0.42857143, 0.52380952, 0.52380952, 0.66666667,\n",
       "       0.69047619, 0.71428571, 0.83333333, 0.73809524, 0.76190476,\n",
       "       0.76190476, 0.80952381, 0.78571429, 0.83333333, 0.78571429,\n",
       "       0.69047619, 0.73809524, 0.78571429, 0.78571429, 0.71428571,\n",
       "       0.88095238, 0.80952381, 0.64285714, 1.        , 0.43834772,\n",
       "       0.38347719, 0.42447596, 0.56165228, 0.50678175, 0.60265105,\n",
       "       0.71239211, 0.9176942 , 0.67108508, 1.        , 0.54778052,\n",
       "       0.64395808, 0.65752158, 0.79438964, 0.76726264, 0.76726264,\n",
       "       0.67108508, 0.75339088, 0.67108508, 0.50678175, 0.42447596,\n",
       "       0.        , 0.56165228, 0.9176942 , 0.15053763, 0.19354839,\n",
       "       0.30107527, 0.37634409, 0.30107527, 0.30107527, 0.31182796,\n",
       "       0.2688172 , 0.43010753, 0.25806452, 0.21505376, 0.22580645,\n",
       "       0.2688172 , 0.2688172 , 0.29032258, 0.23655914, 0.30107527,\n",
       "       0.44086022, 0.47311828, 0.48387097, 0.61290323, 0.60215054,\n",
       "       0.68817204, 1.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.40441024, 0.64873217, 1.47272285, 1.26012248, 1.29191799,\n",
       "       1.27418872, 1.09090909, 1.21851056, 0.30404311, 1.35645689,\n",
       "       0.50955074, 1.30324101, 1.2963925 , 0.27663551, 1.3359401 ,\n",
       "       1.38360942, 1.36559814, 1.128     , 1.33610392, 0.6855884 ,\n",
       "       0.56396824, 0.0726525 , 0.2744881 , 1.11973536, 0.20119581,\n",
       "       0.57886804, 1.0867052 , 0.27453271, 0.28205128, 0.69148982,\n",
       "       1.2921872 , 1.16636059, 0.01659059, 1.0049137 , 1.14622172,\n",
       "       1.04219589, 1.36131041, 0.9252487 , 1.59734281, 0.        ,\n",
       "       0.97190903, 0.25889315, 0.69890595, 0.25336927, 0.47416963,\n",
       "       0.37197563, 0.20511715, 0.50460477, 0.61826574, 0.37704129,\n",
       "       0.54957345, 0.3153813 , 0.58876301, 0.42588808, 0.63028039,\n",
       "       0.71503503, 1.37333333])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[6][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3733333333333329"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 4, 57)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 4, 57)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.10666667,  1.02666667,  1.        ,  1.26666667,  1.29333333,\n",
       "        1.37333333,  1.02666667,  0.44736842,  0.26754386,  0.64473684,\n",
       "        1.14912281,  0.51754386,  0.33333333,  0.80701754,  0.78688525,\n",
       "        0.81967213,  0.70491803,  0.92622951,  0.97540984,  0.98360656,\n",
       "        0.94262295,  0.37209302,  1.25581395, -0.11627907,  0.30232558,\n",
       "        0.13953488,  0.04651163,  0.25581395,  0.89705882,  0.94117647,\n",
       "        0.83823529,  0.76470588,  0.86764706,  0.95588235,  0.86764706,\n",
       "        0.98966408,  1.2118863 ,  0.83979328,  0.97674419,  0.9250646 ,\n",
       "        1.11627907,  1.42377261,  0.72674419,  0.64534884,  0.23255814,\n",
       "        0.45930233,  0.53488372,  0.54069767,  0.62209302,  0.67647059,\n",
       "        0.68627451,  1.15686275,  0.73529412,  0.75490196,  0.91176471,\n",
       "        0.83333333,  1.23387097,  1.2983871 ,  1.24193548,  1.42741935,\n",
       "        1.29032258,  1.30645161,  1.21774194,  0.90243902,  0.66666667,\n",
       "        0.92682927,  0.80487805,  0.91869919,  1.03252033,  0.72357724,\n",
       "        1.3       ,  1.20555556,  0.58333333,  1.15555556,  1.25      ,\n",
       "        1.30555556,  0.79444444,  1.06769034,  1.17298642,  1.15794413,\n",
       "        1.24819791,  1.37605744,  1.40614204,  1.3234094 ,  0.87548139,\n",
       "        0.9139923 ,  0.88318357,  0.84082157,  0.85365854,  0.72272144,\n",
       "        0.70988447,  0.88976378,  0.90551181,  0.61417323,  1.09448819,\n",
       "        1.04724409,  1.18110236,  1.12598425,  1.        ,  0.91103203,\n",
       "        1.07117438,  1.03558719,  1.07117438,  1.28469751,  1.08896797,\n",
       "        0.78125   ,  0.78125   ,  0.09375   ,  0.46875   ,  0.75      ,\n",
       "        0.90625   ,  0.71875   ,  1.31578947,  1.36842105,  1.52631579,\n",
       "        1.10526316,  1.10526316,  1.52631579,  1.52631579,  1.14585232,\n",
       "        1.25524157,  1.04557885,  1.25524157,  1.35551504,  1.59252507,\n",
       "        1.41932543,  1.01257862,  0.79245283,  0.75471698,  1.03144654,\n",
       "        1.03144654,  1.08176101,  0.98742138,  0.79381443,  0.74226804,\n",
       "        0.93814433,  1.12371134,  1.        ,  1.15463918,  1.05154639,\n",
       "        0.75438596,  0.47368421,  0.74561404,  1.12280702,  0.8245614 ,\n",
       "        0.46491228,  0.83333333,  0.98850575,  1.04597701,  1.        ,\n",
       "        1.27586207,  1.26436782,  1.42528736,  1.4137931 ,  1.15625   ,\n",
       "        1.296875  ,  1.41666667,  1.203125  ,  1.33854167,  1.265625  ,\n",
       "        1.33333333,  0.9047619 ,  0.85714286,  1.03190476,  0.96857143,\n",
       "        0.96857143,  1.04761905,  1.07952381,  1.14772727,  1.03409091,\n",
       "        1.13636364,  0.92045455,  1.22727273,  1.15909091,  1.28409091,\n",
       "        0.94736842,  1.28421053,  0.69473684,  1.03157895,  1.12631579,\n",
       "        0.82105263,  1.01052632,  0.87155963,  1.03669725,  0.95412844,\n",
       "        1.03669725,  1.11926606,  1.03669725,  0.89908257,  1.2265625 ,\n",
       "        1.3046875 ,  1.25      ,  1.140625  ,  1.3515625 ,  1.3984375 ,\n",
       "        1.421875  ,  0.96517413,  1.03482587,  0.68159204,  0.960199  ,\n",
       "        1.02985075,  1.06965174,  0.62686567,  0.87096774,  1.03225806,\n",
       "        1.03225806,  1.03225806,  1.03225806,  1.09677419,  1.09677419,\n",
       "        0.93642857,  1.03964286,  1.12690476,  1.14285714,  1.13488095,\n",
       "        1.29357143,  1.20630952,  1.11904762,  1.23809524,  1.23809524,\n",
       "        1.33333333,  1.26190476,  1.45238095,  1.19047619,  1.22222222,\n",
       "        1.04040404,  0.84848485,  0.4040404 ,  0.61616162,  0.67676768,\n",
       "        1.15151515,  1.        ,  1.5       ,  0.66037736,  1.30188679,\n",
       "        1.24528302,  1.33962264,  1.24528302,  0.66180049,  0.83941606,\n",
       "        0.73965937,  0.90997567,  0.71289538,  0.56934307,  1.02919708,\n",
       "        0.9047619 ,  1.0952381 ,  0.92857143,  0.78571429,  1.42857143,\n",
       "        0.88095238,  0.54761905,  0.87669544,  1.0823058 ,  0.83569667,\n",
       "        0.9041307 ,  1.23304562,  1.2946979 ,  1.00678175,  0.78494624,\n",
       "        0.94623656,  0.98924731,  0.94623656,  0.89247312,  1.08602151,\n",
       "        0.87096774])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266, 4, 57)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(912, 4, 57)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1067, device='cuda:0')\n",
      "tensor([[0.7432, 0.4825, 0.8156, 0.6459, 1.0000, 1.0000, 0.5909, 1.0000, 0.5816,\n",
      "         1.0000, 0.8129, 1.0000, 1.0000, 0.2243, 1.0000, 0.7923, 1.0000, 0.3800,\n",
      "         1.0000, 0.3262, 0.4363, 0.1056, 0.2579, 0.4033, 0.0000, 0.3073, 0.6127,\n",
      "         0.2367, 0.4444, 0.7656, 0.8313, 0.8931, 0.2397, 0.8881, 0.9814, 0.8995,\n",
      "         0.8489, 0.8803, 1.0000, 0.0000, 0.7087, 0.1488, 0.6628, 0.2940, 0.9171,\n",
      "         0.2943, 0.1143, 0.4784, 0.2677, 0.6872, 0.6252, 0.5388, 0.6960, 0.6063,\n",
      "         0.7838, 0.4117, 0.5467],\n",
      "        [0.9142, 0.6524, 0.2951, 0.3846, 0.4970, 0.5145, 0.6818, 0.6338, 0.0000,\n",
      "         0.3960, 0.5283, 0.5697, 0.5133, 1.0000, 0.4722, 1.0000, 0.6182, 0.2480,\n",
      "         0.4351, 0.2447, 0.2462, 0.0000, 0.2125, 0.4785, 0.0457, 0.3398, 0.8035,\n",
      "         0.3457, 0.5791, 0.6857, 0.9609, 1.0000, 0.3306, 1.0000, 1.0000, 1.0000,\n",
      "         0.9648, 1.0000, 0.9811, 0.0000, 0.7957, 0.2997, 0.6308, 0.3747, 0.8576,\n",
      "         0.6245, 0.1207, 0.7922, 0.6006, 0.6881, 0.6948, 0.4632, 0.7590, 0.6139,\n",
      "         0.8094, 0.2806, 0.4533],\n",
      "        [1.0000, 0.4356, 1.0000, 0.0744, 0.3186, 0.3333, 1.0000, 0.5013, 0.3675,\n",
      "         0.2377, 0.9252, 0.2574, 0.2455, 0.2280, 0.3961, 0.9754, 0.4454, 0.8760,\n",
      "         0.1623, 0.5684, 0.1180, 0.1501, 0.2346, 0.2879, 0.3127, 0.3715, 1.0000,\n",
      "         0.3854, 1.0000, 0.6680, 1.0000, 0.8436, 0.2140, 0.9814, 0.9749, 0.9703,\n",
      "         1.0000, 0.8848, 0.8712, 0.0000, 0.4803, 0.6149, 0.6287, 0.3556, 0.6861,\n",
      "         1.0000, 0.2125, 0.5523, 0.0000, 0.3861, 0.4898, 0.3597, 0.5322, 0.3925,\n",
      "         0.4932, 0.1220, 1.0000],\n",
      "        [0.9789, 0.7974, 0.9800, 0.6032, 0.8202, 0.8056, 1.0000, 0.8261, 0.2670,\n",
      "         0.7173, 0.6984, 0.8340, 0.8647, 0.2879, 0.8957, 0.9368, 0.8356, 0.9840,\n",
      "         0.8809, 0.6397, 0.3778, 0.1960, 0.3038, 0.4468, 0.1967, 0.2700, 1.1156,\n",
      "         0.4925, 1.1090, 0.5963, 1.1206, 0.8032, 0.4191, 0.9279, 1.0153, 0.9130,\n",
      "         1.0968, 0.9211, 0.9443, 0.0000, 0.9038, 0.1270, 0.4445, 0.3187, 0.8089,\n",
      "         0.5352, 0.1326, 0.7350, 0.2517, 0.8643, 0.4374, 0.5155, 0.4848, 0.5383,\n",
      "         0.6935, 0.4460, 1.1067]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float, device=device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float, device=device)\n",
    "\n",
    "train_data = RollingWindowDataset(X_train_tensor, y_train_tensor, device=device)\n",
    "test_data = RollingWindowDataset(X_test_tensor, y_test_tensor, device=device)\n",
    "\n",
    "print(test_data.__getitem__(0)[1])\n",
    "print(test_data.__getitem__(1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class VanillaLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, layer_size, output_dim, dropout_prob):\n",
    "        super(VanillaLSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_size = layer_size\n",
    "        self.hn = None\n",
    "\n",
    "        self.gru = nn.GRU(input_size = input_dim, hidden_size = self.hidden_size, num_layers=self.layer_size,\n",
    "                            dropout=(dropout_prob if self.layer_size > 1 else 0), batch_first=True)\n",
    "                            \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, output_dim)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(device)\n",
    "        return h0\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hn = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.hn == None:\n",
    "            self.hn = self.init_hidden(x.size(0))\n",
    "        \n",
    "        else:\n",
    "            self.hn = self.hn.detach()\n",
    "            last_hn = self.hn[:,-1:,:]\n",
    "\n",
    "            self.hn = last_hn.repeat(1, x.size(0), 1)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, self.hn = self.gru(x, self.hn)\n",
    "\n",
    "        out = self.dropout(out[:, -1, :])  # Add dropout\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VanillaLSTMModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_size, layer_size, output_dim, dropout_prob):\n",
    "#         super(VanillaLSTMModel, self).__init__()\n",
    "\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.layer_size = layer_size\n",
    "#         self.hn = nn.Parameter(torch.zeros(self.layer_size, 1, self.hidden_size))\n",
    "#         self.cn = nn.Parameter(torch.zeros(self.layer_size, 1, self.hidden_size))\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size = input_dim, hidden_size = self.hidden_size, num_layers=self.layer_size,\n",
    "#                             dropout=(dropout_prob if self.layer_size > 1 else 0), batch_first=True)\n",
    "                            \n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "#         self.fc = nn.Linear(self.hidden_size, output_dim)\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         # Initialize hidden and cell states with zeros\n",
    "#         h0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(device)\n",
    "#         c0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(device)\n",
    "#         return (h0, c0)\n",
    "    \n",
    "#     def reset_hidden(self):\n",
    "#         self.hn = None\n",
    "#         self.cn = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         hn = self.hn.repeat(1, x.size(0), 1)\n",
    "#         cn = self.cn.repeat(1, x.size(0), 1)\n",
    "        \n",
    "#         # Forward propagate LSTM\n",
    "#         out, (hn, cn) = self.lstm(x, (hn, cn))\n",
    "\n",
    "#         out = self.dropout(out[:, -1, :])  # Add dropout\n",
    "\n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# class VanillaLSTMModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_size, layer_size, output_dim, dropout_prob):\n",
    "#         super(VanillaLSTMModel, self).__init__()\n",
    "\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.layer_size = layer_size\n",
    "#         self.hn, self.cn = None, None\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_size = input_dim, hidden_size = self.hidden_size, num_layers=self.layer_size,\n",
    "#                             dropout=(dropout_prob if self.layer_size > 1 else 0), batch_first=True)\n",
    "                            \n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "#         self.fc = nn.Linear(self.hidden_size, output_dim)\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         # Initialize hidden and cell states with zeros\n",
    "#         h0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(device)\n",
    "#         c0 = torch.zeros(self.layer_size, batch_size, self.hidden_size).to(device)\n",
    "#         return (h0, c0)\n",
    "    \n",
    "#     def reset_hidden(self):\n",
    "#         self.hn = None\n",
    "#         self.cn = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # self.hn, self.cn = self.init_hidden(x.size(0))\n",
    "#         if self.hn == None or self.cn == None:\n",
    "#             self.hn, self.cn = self.init_hidden(1)\n",
    "#         else:\n",
    "#             self.hn, self.cn = self.hn.detach(), self.cn.detach()\n",
    "\n",
    "#         x_longer = x.view(1,x.shape[0]*x.shape[1],x.shape[2])\n",
    "\n",
    "#         out_longer, (self.hn, self.cn) = self.lstm(x_longer, (self.hn, self.cn))\n",
    "\n",
    "#         out = out_longer.view(x.shape[0],x.shape[1],out_longer.shape[2])\n",
    "\n",
    "#         out = self.dropout(out[:,-1,:])\n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def find_supported_splits(comp_size):\n",
    "    supported_splits = []\n",
    "    for n_splits in range(1, comp_size + 1):\n",
    "        if comp_size % n_splits != 0:\n",
    "            continue\n",
    "        \n",
    "        supported_splits.append(n_splits)\n",
    "    return supported_splits\n",
    "\n",
    "\n",
    "def custom_time_series_folds(data, n_splits):\n",
    "\n",
    "    total_size = len(data)\n",
    "    comp_size = total_size // len(company_list)\n",
    "    comp_fold_size = comp_size//n_splits\n",
    "\n",
    "    if comp_size % n_splits != 0:\n",
    "        supported_splits = find_supported_splits(comp_size)\n",
    "        print(supported_splits)\n",
    "        print(f\"fold_size: {comp_fold_size} comp_size: {comp_size}\")\n",
    "        raise ValueError(\"Fold size must be divisible by the number of companies.\")\n",
    "\n",
    "    accumulated_train_idx = []     \n",
    "\n",
    "    for i in range(n_splits-1):\n",
    "        current_fold_val_idx = []\n",
    "        current_fold_train_idx = []\n",
    "\n",
    "        for j in range(len(company_list)):\n",
    "\n",
    "            start_idx = j * comp_size\n",
    "            val_start_idx = start_idx + (i+1) * comp_fold_size \n",
    "        \n",
    "            end_idx = val_start_idx + comp_fold_size\n",
    "        \n",
    "            current_comp_train_idx = list(range(start_idx, val_start_idx))\n",
    "            current_fold_train_idx.extend(current_comp_train_idx)  \n",
    "        \n",
    "            val_idx = list(range(val_start_idx, end_idx))\n",
    "            current_fold_val_idx.extend(val_idx)  \n",
    "\n",
    "        # print(f\"train: {current_fold_train_idx}\")\n",
    "        # print(f\"test: {current_fold_val_idx}\")\n",
    "        yield current_fold_train_idx, current_fold_val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class ModelActioner:\n",
    "\n",
    "    def __init__(self, train_data, test_data, device):\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def custom_tscv(self, config, trial):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        epochs = config[\"epochs\"]\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        num_layers = config[\"layer_size\"]\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        dropout_prob = config[\"dropout_prob\"]\n",
    "        weight_decay = config[\"weight_decay\"]\n",
    "        lr_step_size = epochs//config[\"lr_step_size\"]\n",
    "        gamma = config[\"gamma\"]\n",
    "\n",
    "        suffle = False\n",
    "\n",
    "        num_of_fold = 6\n",
    "\n",
    "        fold_results = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(custom_time_series_folds(self.train_data, num_of_fold)):\n",
    "            print(f\"Fold: {fold+1}/{num_of_fold}\")\n",
    "\n",
    "            train_subset = Subset(self.train_data, train_idx)\n",
    "            val_subset = Subset(self.train_data, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=suffle)\n",
    "            val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=suffle)\n",
    "\n",
    "            self.model = VanillaLSTMModel(input_dim=self.train_data.__getitem__(0)[0].shape[1], hidden_size=hidden_size, layer_size=num_layers, dropout_prob=dropout_prob, output_dim=1).to(self.device)\n",
    "\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            scheduler = ReduceLROnPlateau(self.optimizer, patience=lr_step_size, factor=gamma, mode=\"min\", verbose=True) \n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "\n",
    "                running_loss = 0.0\n",
    "                total_sample_train = 0\n",
    "                \n",
    "                self.model.reset_hidden()\n",
    "                self.model.train()\n",
    "\n",
    "                for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                    data, target = data.to(self.device), target.to(self.device)\n",
    "                    target = target.view(-1,1) \n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    preds = self.model(data)\n",
    "\n",
    "                    loss = self.criterion(preds, target)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step() # Update model params\n",
    "\n",
    "                    running_loss += loss.item() * data.size(0)\n",
    "                    total_sample_train += data.size(0)\n",
    "\n",
    "                train_loss = running_loss/total_sample_train\n",
    "\n",
    "                self.model.eval()\n",
    "                val_running_loss = 0.0\n",
    "                total_sample_val = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "                        data, target = data.to(self.device), target.to(self.device)\n",
    "                        target = target.view(-1,1)\n",
    "\n",
    "                        preds = self.model(data)\n",
    "                        loss = self.criterion(preds, target)\n",
    "\n",
    "                        val_running_loss += loss.item() * data.size(0)\n",
    "                        total_sample_val += data.size(0)\n",
    "                \n",
    "                val_loss = val_running_loss/total_sample_val\n",
    "                fold_results.append(val_loss)\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                unique_step = fold * epochs + epoch\n",
    "                trial.report(val_loss, unique_step)\n",
    "\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "                current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "                print(f'Current Learning Rate: {current_lr}')\n",
    "                print(f\"train_loss: {train_loss}, val_loss: {val_loss}\")\n",
    "                \n",
    "        mean_val_loss = np.mean(fold_results)\n",
    "        print(f\"Mean validation loss: {mean_val_loss}\")\n",
    "        return mean_val_loss\n",
    "\n",
    "\n",
    "                    \n",
    "    def train(self, config):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        epochs = config[\"epochs\"]\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        num_layers = config[\"layer_size\"]\n",
    "        learning_rate = config[\"learning_rate\"]\n",
    "        dropout_prob = config[\"dropout_prob\"]\n",
    "        weight_decay = config[\"weight_decay\"]\n",
    "        lr_step_size = epochs//config[\"lr_step_size\"]\n",
    "        gamma = config[\"gamma\"]\n",
    "\n",
    "        self.model = VanillaLSTMModel(input_dim=self.train_data.__getitem__(0)[0].shape[1], hidden_size=hidden_size, layer_size=num_layers, dropout_prob=dropout_prob, output_dim=1).to(self.device)\n",
    "\n",
    "        # Update optimizer with updated lr\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        # Creating data loader\n",
    "        train_loader = DataLoader(dataset=self.train_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, patience=lr_step_size, factor=gamma, mode=\"min\", verbose=True)  \n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print('epochs {}/{}'.format(epoch+1,epochs))\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            total_sample_train = 0\n",
    "\n",
    "            self.model.reset_hidden()\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                target = target.view(-1,1)  \n",
    "                # print(data)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.model(data)\n",
    "\n",
    "                loss = self.criterion(preds, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step() # Update model params\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                total_sample_train += data.size(0)\n",
    "\n",
    "            train_loss = running_loss/total_sample_train\n",
    "            scheduler.step(train_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "            print(f'Current Learning Rate: {current_lr}')\n",
    "            print(f\"train_loss: {train_loss}\")\n",
    "        \n",
    "        return self.model\n",
    "            \n",
    "    \n",
    "    def test(self, config):\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        all_preds = []\n",
    "\n",
    "        test_loader = DataLoader(dataset=self.test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        running_loss = .0\n",
    "        total_sample = 0\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                target = target.view(-1,1)\n",
    "                \n",
    "                preds = self.model(data)\n",
    "                loss = self.criterion(preds, target)\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                total_sample += data.size(0)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            test_loss = running_loss/total_sample\n",
    "            print(f\"test_loss: {test_loss}\")\n",
    "\n",
    "        return all_preds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    config = {\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 22, 100),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 50, 400),\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 50, 1000),\n",
    "        \"layer_size\": trial.suggest_int(\"layer_size\", 1, 6),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-1),\n",
    "        \"dropout_prob\": trial.suggest_float(\"dropout_prob\", 0.15, 0.4),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-1, log=True),\n",
    "        \"lr_step_size\": trial.suggest_int(\"lr_step_size\", 3, 10), \n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-2, 1e-1)\n",
    "    }\n",
    "\n",
    "    trainer = ModelActioner(train_data, test_data, device)\n",
    "\n",
    "    val_loss = trainer.custom_tscv(config, trial)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting study 'Vanilla-LSTM-Tunner'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:15:20,819] A new study created in RDB with name: Vanilla-LSTM-Tunner\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e943cac9f55e446ea4499330e7f8f805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimizing:   0%|          | 0/20 [00:00<?, ?trial/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/6\n",
      "epochs 1/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 45.59612133942152, val_loss: 10.76699763850162\n",
      "epochs 2/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 9.634511106892637, val_loss: 0.8581271767616272\n",
      "epochs 3/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.1602155417203903, val_loss: 0.802559168715226\n",
      "epochs 4/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 10.772649234847018, val_loss: 2.968139510405691\n",
      "epochs 5/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.138788369141127, val_loss: 0.12849599985699905\n",
      "epochs 6/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5122445068861309, val_loss: 0.05668957443221619\n",
      "epochs 7/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.6601213812828064, val_loss: 0.13121156865044645\n",
      "epochs 8/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.7797368262943468, val_loss: 0.360221277726324\n",
      "epochs 9/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5546134498558546, val_loss: 0.05827501475026733\n",
      "epochs 10/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5648245089932492, val_loss: 0.06883980412232249\n",
      "epochs 11/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.4923350952173534, val_loss: 0.14027321770002968\n",
      "epochs 12/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.6459036281234339, val_loss: 0.050487011082862555\n",
      "epochs 13/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.4446265791591845, val_loss: 0.4999719246437675\n",
      "epochs 14/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3925050144132815, val_loss: 0.1412931414026963\n",
      "epochs 15/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5436611912752453, val_loss: 0.1861491646421583\n",
      "epochs 16/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3485460846047652, val_loss: 0.11617413261219074\n",
      "epochs 17/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3157987186783238, val_loss: 0.05061027329219015\n",
      "epochs 18/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3360949009656906, val_loss: 0.15906195891530892\n",
      "epochs 19/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3023103044221276, val_loss: 0.07136925662818708\n",
      "epochs 20/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2639923134916707, val_loss: 0.11033812851498001\n",
      "epochs 21/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.23306515342310855, val_loss: 0.05076996668388969\n",
      "epochs 22/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2804572190109052, val_loss: 0.12962496810053525\n",
      "epochs 23/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.26210191924321025, val_loss: 0.07327443243641603\n",
      "epochs 24/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.21535081298727737, val_loss: 0.09319177505217101\n",
      "epochs 25/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.20911597931071332, val_loss: 0.06302098018166266\n",
      "epochs 26/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.22077790530104385, val_loss: 0.06695245059305116\n",
      "epochs 27/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.24629142802012594, val_loss: 0.12665730950079465\n",
      "epochs 28/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.14824778233703814, val_loss: 0.05620049910717889\n",
      "epochs 29/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.16791063233425743, val_loss: 0.0625955848709533\n",
      "epochs 30/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.14408610055321142, val_loss: 0.07743955070250913\n",
      "epochs 31/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1378470800424877, val_loss: 0.08985740926704909\n",
      "epochs 32/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.19739028262464622, val_loss: 0.07291342847441372\n",
      "epochs 33/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1309208473876903, val_loss: 0.12329050329955\n",
      "epochs 34/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.16070757453378878, val_loss: 0.052868154488111795\n",
      "epochs 35/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.13884636132340683, val_loss: 0.12814128477322428\n",
      "epochs 36/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.18113844920145838, val_loss: 0.055838548529304956\n",
      "epochs 37/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.12616731736220813, val_loss: 0.10369841538761791\n",
      "epochs 38/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.13909447271572917, val_loss: 0.06648448521369382\n",
      "epochs 39/134\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.2252e-03.\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.10216112630931955, val_loss: 0.07634069143157256\n",
      "epochs 40/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07962308804455556, val_loss: 0.07632689726980109\n",
      "epochs 41/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08838124063454177, val_loss: 0.06726724654436111\n",
      "epochs 42/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07477666810154915, val_loss: 0.057650781481673845\n",
      "epochs 43/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.09550830878709492, val_loss: 0.05329202880200587\n",
      "epochs 44/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.10964000774057288, val_loss: 0.05239712250860114\n",
      "epochs 45/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08862111129258808, val_loss: 0.06199040744257601\n",
      "epochs 46/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08685316615983059, val_loss: 0.066977114171574\n",
      "epochs 47/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.10199981418095137, val_loss: 0.07107530279379141\n",
      "epochs 48/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08080153951519414, val_loss: 0.07371752856201247\n",
      "epochs 49/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08688402960174962, val_loss: 0.06675168597384502\n",
      "epochs 50/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08079058167181517, val_loss: 0.0654593812007653\n",
      "epochs 51/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.1043299683614781, val_loss: 0.05704321133855142\n",
      "epochs 52/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08124841867308867, val_loss: 0.05755303309936272\n",
      "epochs 53/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.09815657413319538, val_loss: 0.05478162475322422\n",
      "epochs 54/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0867714211344719, val_loss: 0.055268948603617515\n",
      "epochs 55/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.09145468042085045, val_loss: 0.05842946861919604\n",
      "epochs 56/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07245885227855883, val_loss: 0.05845721203245615\n",
      "epochs 57/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.11130102214060332, val_loss: 0.05860804835040318\n",
      "epochs 58/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.09341297730019218, val_loss: 0.08001530033193137\n",
      "epochs 59/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0835875790370138, val_loss: 0.059314485639333725\n",
      "epochs 60/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0894712433218956, val_loss: 0.06353851956756491\n",
      "epochs 61/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08932700047367498, val_loss: 0.06343409272008821\n",
      "epochs 62/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07609546164933004, val_loss: 0.06800376467014614\n",
      "epochs 63/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.09051103301738438, val_loss: 0.06320295002507537\n",
      "epochs 64/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08690445046675832, val_loss: 0.06367764072982889\n",
      "epochs 65/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08576687033239164, val_loss: 0.06214440623788457\n",
      "epochs 66/134\n",
      "Epoch 00066: reducing learning rate of group 0 to 2.9597e-05.\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07761771231889725, val_loss: 0.06731768442611945\n",
      "epochs 67/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08099201125533957, val_loss: 0.06761233567407257\n",
      "epochs 68/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06694722057957399, val_loss: 0.0706163693807627\n",
      "epochs 69/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0797098004504254, val_loss: 0.06963329232837025\n",
      "epochs 70/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07103393815065685, val_loss: 0.07187413336022903\n",
      "epochs 71/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08791579619834297, val_loss: 0.07052751914843132\n",
      "epochs 72/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07805143100650687, val_loss: 0.07092306282567351\n",
      "epochs 73/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07632073603178326, val_loss: 0.0701722632113256\n",
      "epochs 74/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.09640326233286607, val_loss: 0.07476173753016874\n",
      "epochs 75/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07899662871894084, val_loss: 0.0718764949001764\n",
      "epochs 76/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08777669091757975, val_loss: 0.07026781494680204\n",
      "epochs 77/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08649649627898869, val_loss: 0.07218660629893604\n",
      "epochs 78/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06525579957585585, val_loss: 0.07357084329583143\n",
      "epochs 79/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0720094371783106, val_loss: 0.07152223420378409\n",
      "epochs 80/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06984283814304754, val_loss: 0.0721584734948058\n",
      "epochs 81/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0770947529297126, val_loss: 0.06723672622128536\n",
      "epochs 82/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08274923637509346, val_loss: 0.0710496182896589\n",
      "epochs 83/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07520119610585664, val_loss: 0.07406357479722876\n",
      "epochs 84/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0853948773522126, val_loss: 0.06771818891559776\n",
      "epochs 85/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07690464273879402, val_loss: 0.07264748862699459\n",
      "epochs 86/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.09529958234021538, val_loss: 0.0672457960287207\n",
      "epochs 87/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08470978195730008, val_loss: 0.07252442179933975\n",
      "epochs 88/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07873794436454773, val_loss: 0.06759161021756499\n",
      "epochs 89/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0884239167759293, val_loss: 0.0732720951108556\n",
      "epochs 90/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.09156129979773571, val_loss: 0.06996814690922436\n",
      "epochs 91/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07446461131698207, val_loss: 0.06666482886985729\n",
      "epochs 92/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.09226832421202409, val_loss: 0.06877328328004009\n",
      "epochs 93/134\n",
      "Epoch 00093: reducing learning rate of group 0 to 7.1496e-07.\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07700961514523155, val_loss: 0.06547984225969565\n",
      "epochs 94/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08952767637215163, val_loss: 0.07556170538852089\n",
      "epochs 95/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08347696026689128, val_loss: 0.07026359399682597\n",
      "epochs 96/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.086873648982299, val_loss: 0.0646731243713906\n",
      "epochs 97/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.1005533776785198, val_loss: 0.06978677213191986\n",
      "epochs 98/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08561237509313382, val_loss: 0.06932411244825314\n",
      "epochs 99/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0870347493573239, val_loss: 0.06922789565042446\n",
      "epochs 100/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08022127084826168, val_loss: 0.06545771148643996\n",
      "epochs 101/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08857552393486626, val_loss: 0.06541708160779978\n",
      "epochs 102/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.1031119811691736, val_loss: 0.07132992726799689\n",
      "epochs 103/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07513307191823658, val_loss: 0.06865483777303445\n",
      "epochs 104/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.09019922504299566, val_loss: 0.06503090850616756\n",
      "epochs 105/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0810049917352827, val_loss: 0.06436688835291486\n",
      "epochs 106/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08128764990128969, val_loss: 0.06851228071670783\n",
      "epochs 107/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0790882071382121, val_loss: 0.06613056832238247\n",
      "epochs 108/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08354760706424713, val_loss: 0.06521714449320969\n",
      "epochs 109/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08441161599598433, val_loss: 0.06919528151813306\n",
      "epochs 110/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08600990885966703, val_loss: 0.06881112673957097\n",
      "epochs 111/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06224924914146725, val_loss: 0.06845887505302303\n",
      "epochs 112/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0852656027204112, val_loss: 0.07476180987922769\n",
      "epochs 113/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.10068336286042866, val_loss: 0.06987193697377254\n",
      "epochs 114/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.09371994710282276, val_loss: 0.0687934028867044\n",
      "epochs 115/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.09345602126497972, val_loss: 0.06963011573411916\n",
      "epochs 116/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08429094011846341, val_loss: 0.06736926734447479\n",
      "epochs 117/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07978352394543196, val_loss: 0.06939943399476378\n",
      "epochs 118/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.08290618971774452, val_loss: 0.07053350343515999\n",
      "epochs 119/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07754031529552058, val_loss: 0.06548428211949374\n",
      "epochs 120/134\n",
      "Epoch 00120: reducing learning rate of group 0 to 1.7271e-08.\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.08165127784013748, val_loss: 0.0666843995844063\n",
      "epochs 121/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.09129419216984197, val_loss: 0.06712441175783936\n",
      "epochs 122/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07032881402655651, val_loss: 0.07123647424343385\n",
      "epochs 123/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07836082656132548, val_loss: 0.06544555517795839\n",
      "epochs 124/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.1010119934615336, val_loss: 0.06920710166818217\n",
      "epochs 125/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0859924609723844, val_loss: 0.06589906143122598\n",
      "epochs 126/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.08672579652384708, val_loss: 0.06672385374182149\n",
      "epochs 127/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.08892364329413364, val_loss: 0.06271730745701413\n",
      "epochs 128/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07618115959983122, val_loss: 0.07100522086808556\n",
      "epochs 129/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0794659582407851, val_loss: 0.06766266571848016\n",
      "epochs 130/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.08296865401299376, val_loss: 0.06931064317100927\n",
      "epochs 131/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.073448270363243, val_loss: 0.06823710116900895\n",
      "epochs 132/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0970040057834826, val_loss: 0.06641791643280733\n",
      "epochs 133/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0845677001695884, val_loss: 0.0681364856856434\n",
      "epochs 134/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07249089663750247, val_loss: 0.07078233253406851\n",
      "Fold: 2/6\n",
      "epochs 1/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 43.03308414158068, val_loss: 19.88158687792326\n",
      "epochs 2/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 12.923282410753401, val_loss: 0.5358434758688274\n",
      "epochs 3/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 4.2910383202527695, val_loss: 1.7435533749429803\n",
      "epochs 4/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.592676459958679, val_loss: 0.055829462840368875\n",
      "epochs 5/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.4276311303439893, val_loss: 2.0711608999653865\n",
      "epochs 6/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.23507743211169, val_loss: 0.15941545014318667\n",
      "epochs 7/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.9390547581409153, val_loss: 0.18420584970398954\n",
      "epochs 8/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.41963837491838557, val_loss: 0.059712258804785576\n",
      "epochs 9/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3531305131159331, val_loss: 0.15320151299238205\n",
      "epochs 10/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.4505158325559215, val_loss: 6.818232310445685\n",
      "epochs 11/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 2.698683927325826, val_loss: 0.08655989013220135\n",
      "epochs 12/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.6256076685692135, val_loss: 0.5154307029749218\n",
      "epochs 13/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.26697961789997, val_loss: 0.10375167488267548\n",
      "epochs 14/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.29837510185806376, val_loss: 0.0564240211326825\n",
      "epochs 15/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.26806255292735603, val_loss: 0.06716477400378178\n",
      "epochs 16/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2477534554506603, val_loss: 0.05374247914082125\n",
      "epochs 17/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.18279362293450455, val_loss: 0.05193574746188365\n",
      "epochs 18/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.26041459252959803, val_loss: 0.07872501231337849\n",
      "epochs 19/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.20243851252292333, val_loss: 0.05810501348031195\n",
      "epochs 20/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.15848860575964577, val_loss: 0.059353584913831005\n",
      "epochs 21/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.17944042933614632, val_loss: 0.06178070663621551\n",
      "epochs 22/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.12547895351522848, val_loss: 0.06360446014686634\n",
      "epochs 23/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1499553714927874, val_loss: 0.054592327067726536\n",
      "epochs 24/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.14028030281004153, val_loss: 0.06258752216633998\n",
      "epochs 25/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1573765189631989, val_loss: 0.12384624269447829\n",
      "epochs 26/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1962617440639358, val_loss: 0.21768825618844284\n",
      "epochs 27/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.18054921434898125, val_loss: 0.0738986458041166\n",
      "epochs 28/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1351894189260508, val_loss: 0.05346357606743511\n",
      "epochs 29/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.16092805936932564, val_loss: 0.08801965219409842\n",
      "epochs 30/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.12489448036802442, val_loss: 0.0887368168485792\n",
      "epochs 31/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.17335684391620912, val_loss: 0.22321792417450956\n",
      "epochs 32/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.22208319437739096, val_loss: 0.08481675033506594\n",
      "epochs 33/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.09736699865836847, val_loss: 0.05723055118792936\n",
      "epochs 34/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2339920380005711, val_loss: 0.09980945720484383\n",
      "epochs 35/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.09307628605318696, val_loss: 0.05909781804994533\n",
      "epochs 36/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.09096901363840229, val_loss: 0.09991726789035295\n",
      "epochs 37/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 3.855407467896217, val_loss: 7.64935154663889\n",
      "epochs 38/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 2.2725001871585846, val_loss: 1.6778946299301951\n",
      "epochs 39/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.9167563460375133, val_loss: 0.2676117506466414\n",
      "epochs 40/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.6271124827234369, val_loss: 0.05446776865344299\n",
      "epochs 41/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.41476970911026, val_loss: 0.08545293423690294\n",
      "epochs 42/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.4678854636455837, val_loss: 0.3810570538043976\n",
      "epochs 43/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5010889329408345, val_loss: 0.20355177631503657\n",
      "epochs 44/134\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.2252e-03.\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.34155783919911636, val_loss: 0.1523811338763488\n",
      "epochs 45/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.3719048688286229, val_loss: 0.051725468941425025\n",
      "epochs 46/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.24736122159581436, val_loss: 0.07440164489181418\n",
      "epochs 47/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2723253953613733, val_loss: 0.08209260396267239\n",
      "epochs 48/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.30377766882118423, val_loss: 0.08064465460024382\n",
      "epochs 49/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.29621537814014837, val_loss: 0.07456057871642865\n",
      "epochs 50/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2815778655441184, val_loss: 0.06465296623738188\n",
      "epochs 51/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.3068938882727372, val_loss: 0.061457486136963495\n",
      "epochs 52/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2844908394311604, val_loss: 0.0671880392259673\n",
      "epochs 53/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.3110745051189473, val_loss: 0.06908945405953809\n",
      "epochs 54/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.24500237326872976, val_loss: 0.06499822653437916\n",
      "epochs 55/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2801820933818817, val_loss: 0.06016430356784871\n",
      "epochs 56/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.32364083708901153, val_loss: 0.0635799240124853\n",
      "epochs 57/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2809040197416356, val_loss: 0.06839289731885258\n",
      "epochs 58/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.3200637592296851, val_loss: 0.07283143679562368\n",
      "epochs 59/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.30341514435253647, val_loss: 0.07148800966771025\n",
      "epochs 60/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.31713918086729553, val_loss: 0.07250683244905974\n",
      "epochs 61/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.3183901192326295, val_loss: 0.0549423757352327\n",
      "epochs 62/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2879105433821678, val_loss: 0.059359182652674224\n",
      "epochs 63/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2921221640549208, val_loss: 0.07553103664203693\n",
      "epochs 64/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.2758283991562693, val_loss: 0.07349563095914691\n",
      "epochs 65/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.29877305658240066, val_loss: 0.06377521451366575\n",
      "epochs 66/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.29879280571874817, val_loss: 0.06380625696558702\n",
      "epochs 67/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.29230156697724996, val_loss: 0.0720340984040185\n",
      "epochs 68/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.28091816055147273, val_loss: 0.0745587982237339\n",
      "epochs 69/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.23309898807814247, val_loss: 0.07126231060216301\n",
      "epochs 70/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.28459641023686055, val_loss: 0.07102147587820103\n",
      "epochs 71/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.27619295175138275, val_loss: 0.0872465836766519\n",
      "epochs 72/134\n",
      "Epoch 00072: reducing learning rate of group 0 to 2.9597e-05.\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2955310344696045, val_loss: 0.0690211885069546\n",
      "epochs 73/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.24399133379522123, val_loss: 0.0685275481327584\n",
      "epochs 74/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.27883940150863246, val_loss: 0.06426574956429632\n",
      "epochs 75/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.24851225708660327, val_loss: 0.07147431393202983\n",
      "epochs 76/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.23375006765127182, val_loss: 0.0644268401359257\n",
      "epochs 77/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.26979496212382065, val_loss: 0.06562710533800878\n",
      "epochs 78/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.26407075908623245, val_loss: 0.08693728086195494\n",
      "epochs 79/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.3075285514718608, val_loss: 0.0671816625093159\n",
      "epochs 80/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2533900388761571, val_loss: 0.07180262690311984\n",
      "epochs 81/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.22392826409716354, val_loss: 0.0669394441341099\n",
      "epochs 82/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.25122182384917613, val_loss: 0.06824091919942905\n",
      "epochs 83/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2644164754371894, val_loss: 0.0657687128374451\n",
      "epochs 84/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2597684366138358, val_loss: 0.07637929132110194\n",
      "epochs 85/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.26450127676913615, val_loss: 0.06637467541976978\n",
      "epochs 86/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.23278708677542836, val_loss: 0.0689215775775282\n",
      "epochs 87/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2431617273311866, val_loss: 0.06519645472106181\n",
      "epochs 88/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.26618626988247823, val_loss: 0.06791924802880538\n",
      "epochs 89/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2640431009625134, val_loss: 0.07251644683511634\n",
      "epochs 90/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.24744568843590586, val_loss: 0.07284324988722801\n",
      "epochs 91/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.26020808243437815, val_loss: 0.07237671139208894\n",
      "epochs 92/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2749262612901236, val_loss: 0.06801929187617804\n",
      "epochs 93/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.3044577684057386, val_loss: 0.07157723429171663\n",
      "epochs 94/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.25728194101860646, val_loss: 0.0767058912468584\n",
      "epochs 95/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.29091490334586095, val_loss: 0.0727005408782708\n",
      "epochs 96/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.2437614959321524, val_loss: 0.06833752617239952\n",
      "epochs 97/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.22197537359438443, val_loss: 0.07006880856658283\n",
      "epochs 98/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.23633821602714689, val_loss: 0.07116818016296939\n",
      "epochs 99/134\n",
      "Epoch 00099: reducing learning rate of group 0 to 7.1496e-07.\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.28051325482757467, val_loss: 0.06779636109345838\n",
      "epochs 100/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.26598328312760905, val_loss: 0.06780791772823584\n",
      "epochs 101/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2507603360634101, val_loss: 0.0698193815585814\n",
      "epochs 102/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.27784244833808197, val_loss: 0.0700771892933469\n",
      "epochs 103/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.275698458677844, val_loss: 0.06665159742298879\n",
      "epochs 104/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.26977479810777466, val_loss: 0.07256403173270978\n",
      "epochs 105/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.29213879414294897, val_loss: 0.08595660054369976\n",
      "epochs 106/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.26671121112610163, val_loss: 0.07650935512624289\n",
      "epochs 107/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2516929083748868, val_loss: 0.06826583139206234\n",
      "epochs 108/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.26339447851243775, val_loss: 0.07077050679608395\n",
      "epochs 109/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.21957082066096759, val_loss: 0.0707277816377188\n",
      "epochs 110/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.31122288617648575, val_loss: 0.0745686998492793\n",
      "epochs 111/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2512343227863312, val_loss: 0.09509471117665894\n",
      "epochs 112/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.29493159918408646, val_loss: 0.07075703222500651\n",
      "epochs 113/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2761701631702875, val_loss: 0.07229415033208697\n",
      "epochs 114/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.24940934698832662, val_loss: 0.0711602552940971\n",
      "epochs 115/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2615562998934796, val_loss: 0.06987078409445913\n",
      "epochs 116/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2348755256910073, val_loss: 0.06644018739461899\n",
      "epochs 117/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.26473193498034225, val_loss: 0.0771787411680347\n",
      "epochs 118/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2848973964390002, val_loss: 0.07689282749044268\n",
      "epochs 119/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.32087923821650055, val_loss: 0.07048412255550686\n",
      "epochs 120/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2617739960551262, val_loss: 0.06635347165559467\n",
      "epochs 121/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2328028173038834, val_loss: 0.0738717903824229\n",
      "epochs 122/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2893413007259369, val_loss: 0.07646792519249414\n",
      "epochs 123/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2693607673833245, val_loss: 0.0847276990350924\n",
      "epochs 124/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.2553227312470737, val_loss: 0.06761762539022848\n",
      "epochs 125/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.29368988071617325, val_loss: 0.06996240113910876\n",
      "epochs 126/134\n",
      "Epoch 00126: reducing learning rate of group 0 to 1.7271e-08.\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.27045696934587077, val_loss: 0.07132612639351894\n",
      "epochs 127/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.2753943389183597, val_loss: 0.07761267572641373\n",
      "epochs 128/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.29951340509088414, val_loss: 0.06738605546323877\n",
      "epochs 129/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.27480777196193995, val_loss: 0.0798927071063142\n",
      "epochs 130/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.2217298318680964, val_loss: 0.08667869689433198\n",
      "epochs 131/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.25285019960842636, val_loss: 0.07099770166371998\n",
      "epochs 132/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.2418698586131397, val_loss: 0.06879106889429845\n",
      "epochs 133/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.25848017319252614, val_loss: 0.0709069114374487\n",
      "epochs 134/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.23048980102727287, val_loss: 0.06858064587178983\n",
      "Fold: 3/6\n",
      "epochs 1/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 17.616324143451557, val_loss: 0.9270542138501218\n",
      "epochs 2/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 8.958395958992472, val_loss: 1.5143610991929706\n",
      "epochs 3/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 9.00905855130731, val_loss: 1.96908238059596\n",
      "epochs 4/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.8274675565853453, val_loss: 0.03877072634273454\n",
      "epochs 5/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5885036462231686, val_loss: 0.05022309985207884\n",
      "epochs 6/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.6874740359030271, val_loss: 1.0280119306162785\n",
      "epochs 7/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.1835261562414336, val_loss: 0.046798502732264366\n",
      "epochs 8/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5767029163084532, val_loss: 0.050472601954089966\n",
      "epochs 9/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.374711833502117, val_loss: 1.0310600650937933\n",
      "epochs 10/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.44290508512865034, val_loss: 0.08534069869079088\n",
      "epochs 11/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 2.4551760941221, val_loss: 0.39398202143217387\n",
      "epochs 12/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 2.507281348893517, val_loss: 0.37342911488131475\n",
      "epochs 13/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.798535086868102, val_loss: 0.034717951166002375\n",
      "epochs 14/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.25704193507370193, val_loss: 0.03710364756223403\n",
      "epochs 15/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2633631807147411, val_loss: 0.03464389043418985\n",
      "epochs 16/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2325182920485212, val_loss: 0.08689333459264353\n",
      "epochs 17/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.27203628584219697, val_loss: 0.03453467766705312\n",
      "epochs 18/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1719881361513807, val_loss: 0.0893510501635702\n",
      "epochs 19/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1675244156728711, val_loss: 0.03464778130383868\n",
      "epochs 20/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.13122566260005297, val_loss: 0.03433905963442827\n",
      "epochs 21/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.13965769857168198, val_loss: 0.03573494590818882\n",
      "epochs 22/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.11989535548185047, val_loss: 0.07446593791246414\n",
      "epochs 23/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.11630758734648688, val_loss: 0.0350702459874906\n",
      "epochs 24/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1835281563955441, val_loss: 0.10356154959452779\n",
      "epochs 25/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.10471991939764273, val_loss: 0.10482100201280493\n",
      "epochs 26/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.11980669487986648, val_loss: 0.0854738499773176\n",
      "epochs 27/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.20835815906001812, val_loss: 0.05335933244542072\n",
      "epochs 28/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.183101805940009, val_loss: 0.06145197545227252\n",
      "epochs 29/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.44525044698987093, val_loss: 0.04124111368467933\n",
      "epochs 30/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1057422187244683, val_loss: 0.04062920348032525\n",
      "epochs 31/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.18241848548253378, val_loss: 0.12818399503042824\n",
      "epochs 32/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.12522935730062032, val_loss: 0.04323235105134939\n",
      "epochs 33/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.09075467275422916, val_loss: 0.05282543050615411\n",
      "epochs 34/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.17750439753657893, val_loss: 0.06194745945303064\n",
      "epochs 35/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.10427086055278778, val_loss: 0.04059320433359397\n",
      "epochs 36/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.13341442632832026, val_loss: 0.04074074131877799\n",
      "epochs 37/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5060530858800599, val_loss: 0.1043854042103416\n",
      "epochs 38/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.11227872536370628, val_loss: 0.038030291564370454\n",
      "epochs 39/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1464567729517033, val_loss: 0.047113824635744095\n",
      "epochs 40/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.27455879198877436, val_loss: 0.23484344231454948\n",
      "epochs 41/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.11502118683175037, val_loss: 0.05484011690867575\n",
      "epochs 42/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.09570414991232387, val_loss: 0.05953155987356838\n",
      "epochs 43/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.24119601581703154, val_loss: 0.11940982467249821\n",
      "epochs 44/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1340755986932077, val_loss: 0.0722854807972908\n",
      "epochs 45/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.10847885919767514, val_loss: 0.07247858769015263\n",
      "epochs 46/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.10081608279755241, val_loss: 0.034769708486764056\n",
      "epochs 47/134\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.2252e-03.\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.1899971816884844, val_loss: 0.044391221513873654\n",
      "epochs 48/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.11066251659863874, val_loss: 0.06420093676761578\n",
      "epochs 49/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.1135731547286636, val_loss: 0.03487759220756983\n",
      "epochs 50/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08278565703515421, val_loss: 0.07967299634688779\n",
      "epochs 51/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08974116511250797, val_loss: 0.04874306623088686\n",
      "epochs 52/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08464893881689038, val_loss: 0.03529880107625535\n",
      "epochs 53/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07633135307645589, val_loss: 0.05795477448325408\n",
      "epochs 54/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08911418790618579, val_loss: 0.05677725786441251\n",
      "epochs 55/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07981962560300242, val_loss: 0.03569823709365569\n",
      "epochs 56/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07380081164209466, val_loss: 0.05584539022100599\n",
      "epochs 57/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08106231820165065, val_loss: 0.053866822958776824\n",
      "epochs 58/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07493436140449423, val_loss: 0.03695996811515406\n",
      "epochs 59/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07917520188187298, val_loss: 0.046840552632745946\n",
      "epochs 60/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07475455098769121, val_loss: 0.05411426419098126\n",
      "epochs 61/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07030695077162563, val_loss: 0.04170267617231921\n",
      "epochs 62/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07444325716871965, val_loss: 0.04653406662768439\n",
      "epochs 63/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07465567132621481, val_loss: 0.050715271775659765\n",
      "epochs 64/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06868893430944074, val_loss: 0.04606535658240318\n",
      "epochs 65/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0705974758194204, val_loss: 0.051518741799028295\n",
      "epochs 66/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06953084295648232, val_loss: 0.0469772239264689\n",
      "epochs 67/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07852933333631147, val_loss: 0.04667982037522291\n",
      "epochs 68/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0798322322467963, val_loss: 0.06110459997465736\n",
      "epochs 69/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07512642894136279, val_loss: 0.04078839572244569\n",
      "epochs 70/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06770567025727871, val_loss: 0.04983493342603508\n",
      "epochs 71/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07471781204405584, val_loss: 0.04546257922131764\n",
      "epochs 72/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08004297061186087, val_loss: 0.042602480242126865\n",
      "epochs 73/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0728081388300971, val_loss: 0.05979080851140775\n",
      "epochs 74/134\n",
      "Epoch 00074: reducing learning rate of group 0 to 2.9597e-05.\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07696963284622159, val_loss: 0.04014828546266807\n",
      "epochs 75/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08813873390879548, val_loss: 0.036508354309358095\n",
      "epochs 76/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.09431232793027894, val_loss: 0.03585450164973736\n",
      "epochs 77/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08115721853417263, val_loss: 0.035414629076656545\n",
      "epochs 78/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07590796654684502, val_loss: 0.03601461590120667\n",
      "epochs 79/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08105117363626496, val_loss: 0.035664938586322886\n",
      "epochs 80/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0746642917786774, val_loss: 0.03775936580802265\n",
      "epochs 81/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06959859242564753, val_loss: 0.038676656782627106\n",
      "epochs 82/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07499171224863906, val_loss: 0.04501804876092233\n",
      "epochs 83/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07242628298046296, val_loss: 0.04214222856650227\n",
      "epochs 84/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06970295934169962, val_loss: 0.04177705953387838\n",
      "epochs 85/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.062141962284058855, val_loss: 0.042718777531071714\n",
      "epochs 86/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07368795874348857, val_loss: 0.05098352483228633\n",
      "epochs 87/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06758599208765909, val_loss: 0.05387940316608077\n",
      "epochs 88/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07164971170979634, val_loss: 0.053780994721149146\n",
      "epochs 89/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07316171016805527, val_loss: 0.05353117587142869\n",
      "epochs 90/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07452508343154923, val_loss: 0.052667034201716124\n",
      "epochs 91/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0704918671399355, val_loss: 0.05497232510855323\n",
      "epochs 92/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07414167942969423, val_loss: 0.05199286322060384\n",
      "epochs 93/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06872562034741829, val_loss: 0.047585422369210345\n",
      "epochs 94/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07315780611283947, val_loss: 0.05145143688117203\n",
      "epochs 95/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07209134840390138, val_loss: 0.05033090220470177\n",
      "epochs 96/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07378843204494108, val_loss: 0.044351840391755104\n",
      "epochs 97/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06904064654781107, val_loss: 0.050830542060889695\n",
      "epochs 98/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07094542165858704, val_loss: 0.050767246339666214\n",
      "epochs 99/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0685861095001823, val_loss: 0.0512115894571731\n",
      "epochs 100/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07344279542826769, val_loss: 0.04653930320943657\n",
      "epochs 101/134\n",
      "Epoch 00101: reducing learning rate of group 0 to 7.1496e-07.\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07175570242760475, val_loss: 0.06284178675789583\n",
      "epochs 102/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07108549597231965, val_loss: 0.055905461507408244\n",
      "epochs 103/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06798805886258681, val_loss: 0.05448237208551482\n",
      "epochs 104/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0742111728248889, val_loss: 0.05509541036659166\n",
      "epochs 105/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07270011129347902, val_loss: 0.043247765322264875\n",
      "epochs 106/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07065360761133202, val_loss: 0.050867111667206415\n",
      "epochs 107/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07082827085335004, val_loss: 0.04850798453155317\n",
      "epochs 108/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06937561259327228, val_loss: 0.055537658694543336\n",
      "epochs 109/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07384272422968295, val_loss: 0.0562569628420629\n",
      "epochs 110/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07278565219358395, val_loss: 0.049968809771694635\n",
      "epochs 111/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06852223946337115, val_loss: 0.04372293531502548\n",
      "epochs 112/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07116530489241868, val_loss: 0.05228558016058646\n",
      "epochs 113/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06780547681346275, val_loss: 0.049183965219478855\n",
      "epochs 114/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07574005326942394, val_loss: 0.05193564687904559\n",
      "epochs 115/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06878113524432768, val_loss: 0.050183688829603945\n",
      "epochs 116/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06864033600217417, val_loss: 0.04561447146299638\n",
      "epochs 117/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06541365484723397, val_loss: 0.0459137713830722\n",
      "epochs 118/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07176900360929339, val_loss: 0.048911462096791515\n",
      "epochs 119/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06885441608334843, val_loss: 0.04753272382444457\n",
      "epochs 120/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07040637787104699, val_loss: 0.051825944432302526\n",
      "epochs 121/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06886483630851696, val_loss: 0.049691941981252874\n",
      "epochs 122/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06704745572387126, val_loss: 0.04770594100026708\n",
      "epochs 123/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07113709807265223, val_loss: 0.04827995951238431\n",
      "epochs 124/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07560249855905249, val_loss: 0.05216394933430772\n",
      "epochs 125/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06994649277705896, val_loss: 0.05739298090338707\n",
      "epochs 126/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07069017388449426, val_loss: 0.0533075968881971\n",
      "epochs 127/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06891256626368615, val_loss: 0.05004264002567843\n",
      "epochs 128/134\n",
      "Epoch 00128: reducing learning rate of group 0 to 1.7271e-08.\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07092564618378355, val_loss: 0.05483176225894376\n",
      "epochs 129/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0677707067184281, val_loss: 0.04742092207858437\n",
      "epochs 130/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07277325975398223, val_loss: 0.05522554985394603\n",
      "epochs 131/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07329367490060497, val_loss: 0.05061175301671028\n",
      "epochs 132/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0758502980073293, val_loss: 0.06058729655648533\n",
      "epochs 133/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07011168691934201, val_loss: 0.05201879643688077\n",
      "epochs 134/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.067497130270982, val_loss: 0.04112441229977106\n",
      "Fold: 4/6\n",
      "epochs 1/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 18.974205964686053, val_loss: 0.04071253038158542\n",
      "epochs 2/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 3.2394784469353524, val_loss: 0.06544808612058037\n",
      "epochs 3/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.8775452563637182, val_loss: 0.06118415178436982\n",
      "epochs 4/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.2652366290751256, val_loss: 0.19388285671409808\n",
      "epochs 5/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.2100211389754947, val_loss: 0.08861558903989039\n",
      "epochs 6/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.1086922872223353, val_loss: 0.034011206148486385\n",
      "epochs 7/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.6090820882665483, val_loss: 0.042084056393880596\n",
      "epochs 8/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.36497234631525843, val_loss: 0.09078043248308332\n",
      "epochs 9/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.24316402701170822, val_loss: 0.054240882200630086\n",
      "epochs 10/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2465377000993804, val_loss: 0.07237764467534266\n",
      "epochs 11/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.23719886631557816, val_loss: 0.0893478779808471\n",
      "epochs 12/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5689657226597008, val_loss: 24.46664137589304\n",
      "epochs 13/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.24906805128251253, val_loss: 0.03381658012145444\n",
      "epochs 14/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.9610417042123643, val_loss: 0.16107539911019175\n",
      "epochs 15/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 5.497503682971001, val_loss: 1.9944899082183838\n",
      "epochs 16/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 5.509338455764871, val_loss: 4.4222555662456315\n",
      "epochs 17/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.5134062410185212, val_loss: 0.03216907636899697\n",
      "epochs 18/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.7953042427175923, val_loss: 1.1679712596692537\n",
      "epochs 19/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.4351785947617732, val_loss: 0.13561177214509562\n",
      "epochs 20/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.31134067318941416, val_loss: 0.13585842636070752\n",
      "epochs 21/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3429886371289429, val_loss: 0.19441359294088265\n",
      "epochs 22/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.24712052392332176, val_loss: 0.05621640462624399\n",
      "epochs 23/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1546749818677965, val_loss: 0.03893978344766717\n",
      "epochs 24/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.15534094358353237, val_loss: 0.030593338863630044\n",
      "epochs 25/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.30867214540117666, val_loss: 0.1207360252737999\n",
      "epochs 26/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.18515625096073277, val_loss: 0.04845661003338663\n",
      "epochs 27/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2418810213474851, val_loss: 0.23987369160903127\n",
      "epochs 28/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.18148114914564709, val_loss: 0.039373586640546195\n",
      "epochs 29/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3593082574049109, val_loss: 0.09708622725386369\n",
      "epochs 30/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5144130322886141, val_loss: 0.4079047174830186\n",
      "epochs 31/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2565158508521946, val_loss: 0.09373426221703228\n",
      "epochs 32/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.20490567690055622, val_loss: 0.14327499780215716\n",
      "epochs 33/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.19694329690384238, val_loss: 0.14065168915610565\n",
      "epochs 34/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1649679479336268, val_loss: 0.03443430452362487\n",
      "epochs 35/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1983421481165447, val_loss: 0.1255742911445467\n",
      "epochs 36/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.13171956031338164, val_loss: 0.030347454802770363\n",
      "epochs 37/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.20606886154334797, val_loss: 0.04623317296959852\n",
      "epochs 38/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.11784308942917146, val_loss: 0.045459422037789694\n",
      "epochs 39/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.12079169008096582, val_loss: 0.038110362954045594\n",
      "epochs 40/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.14890110482902905, val_loss: 0.04056874025417002\n",
      "epochs 41/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.138635807376551, val_loss: 0.0356614108344442\n",
      "epochs 42/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.19330995147557636, val_loss: 0.04836499798846872\n",
      "epochs 43/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.31883296762642105, val_loss: 0.053748977713679015\n",
      "epochs 44/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.139176112354586, val_loss: 0.03660973220279342\n",
      "epochs 45/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.28969330762169865, val_loss: 0.2265098236109081\n",
      "epochs 46/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2338371714086909, val_loss: 0.2436219594980541\n",
      "epochs 47/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.20038966613968737, val_loss: 0.14714383177067103\n",
      "epochs 48/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.16621102887744965, val_loss: 0.19997102414306842\n",
      "epochs 49/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.17902048789945088, val_loss: 0.03774809062873062\n",
      "epochs 50/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.3161473810476692, val_loss: 0.3682904557177895\n",
      "epochs 51/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.28807548315901504, val_loss: 0.3297114434995149\n",
      "epochs 52/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.18252664083909048, val_loss: 0.2235508310167413\n",
      "epochs 53/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1380862322096762, val_loss: 0.10727891953367936\n",
      "epochs 54/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5151702843018269, val_loss: 0.1471903959387227\n",
      "epochs 55/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.976736208718074, val_loss: 0.0870598723229609\n",
      "epochs 56/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2465234587067052, val_loss: 0.06919536700374201\n",
      "epochs 57/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.12255619686881178, val_loss: 0.07738250141081057\n",
      "epochs 58/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.13977481160116823, val_loss: 0.045078950885095094\n",
      "epochs 59/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.08037573265794076, val_loss: 0.07379528546803876\n",
      "epochs 60/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.14044436709465166, val_loss: 0.21164029287664513\n",
      "epochs 61/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.21859839637028544, val_loss: 0.07604474907642916\n",
      "epochs 62/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.15735938757854073, val_loss: 0.03956223102776628\n",
      "epochs 63/134\n",
      "Epoch 00063: reducing learning rate of group 0 to 1.2252e-03.\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 1.1623215106757063, val_loss: 0.608599371031711\n",
      "epochs 64/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 1.6540266079337973, val_loss: 0.1216107881382892\n",
      "epochs 65/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.15375135868395629, val_loss: 0.3800208176437177\n",
      "epochs 66/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.23227181020928056, val_loss: 0.17035750338905736\n",
      "epochs 67/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08347315409857976, val_loss: 0.03339082512416338\n",
      "epochs 68/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08937545375604379, val_loss: 0.035608165750378055\n",
      "epochs 69/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07314421819817078, val_loss: 0.059317308232972495\n",
      "epochs 70/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07324222965460074, val_loss: 0.06508779545363627\n",
      "epochs 71/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06987339821889212, val_loss: 0.055302435826314125\n",
      "epochs 72/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07318367485545184, val_loss: 0.05230326872122915\n",
      "epochs 73/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0672813305435212, val_loss: 0.055431951621645374\n",
      "epochs 74/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06904520535547483, val_loss: 0.058081429451704025\n",
      "epochs 75/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07173742589197661, val_loss: 0.057990803726409614\n",
      "epochs 76/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07173425415040631, val_loss: 0.05505117028951645\n",
      "epochs 77/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06788169504388382, val_loss: 0.05248841623726644\n",
      "epochs 78/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06943666964377228, val_loss: 0.054276945363534126\n",
      "epochs 79/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07598297694992077, val_loss: 0.057266683170669956\n",
      "epochs 80/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06993815062665626, val_loss: 0.05694700993205372\n",
      "epochs 81/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06993741037226037, val_loss: 0.054466017178798974\n",
      "epochs 82/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07143014586089473, val_loss: 0.056542963181671346\n",
      "epochs 83/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06999608874320984, val_loss: 0.05563717159001451\n",
      "epochs 84/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06474094779083603, val_loss: 0.05650126404668156\n",
      "epochs 85/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06848426790613878, val_loss: 0.05602658049840676\n",
      "epochs 86/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06989410784291594, val_loss: 0.05541090275111951\n",
      "epochs 87/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0689535766938015, val_loss: 0.05578143561356946\n",
      "epochs 88/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06816130984378488, val_loss: 0.054838566207572034\n",
      "epochs 89/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06735486149983971, val_loss: 0.05584833476888506\n",
      "epochs 90/134\n",
      "Epoch 00090: reducing learning rate of group 0 to 2.9597e-05.\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07088722327822133, val_loss: 0.059775049553105704\n",
      "epochs 91/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06990434513672401, val_loss: 0.06255788571740452\n",
      "epochs 92/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07029693990357612, val_loss: 0.06035522606812025\n",
      "epochs 93/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06822788563409918, val_loss: 0.0608886970501197\n",
      "epochs 94/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06558240614341278, val_loss: 0.06305659287854244\n",
      "epochs 95/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0661585547128006, val_loss: 0.06296411725251298\n",
      "epochs 96/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07050263484645831, val_loss: 0.06353757157921791\n",
      "epochs 97/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06470781417661592, val_loss: 0.06080365631925432\n",
      "epochs 98/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06175564405949492, val_loss: 0.060283545796808446\n",
      "epochs 99/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07241598903936775, val_loss: 0.060927786325153555\n",
      "epochs 100/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07292718886348762, val_loss: 0.05821644142270088\n",
      "epochs 101/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06454107434929986, val_loss: 0.05861799320892284\n",
      "epochs 102/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06801156454572552, val_loss: 0.057647789975530224\n",
      "epochs 103/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06846096763681424, val_loss: 0.060216941331562246\n",
      "epochs 104/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06508915572378196, val_loss: 0.06052259904773612\n",
      "epochs 105/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06473862149409558, val_loss: 0.05815540175688894\n",
      "epochs 106/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06487551777574577, val_loss: 0.05913284499394266\n",
      "epochs 107/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06574704267672803, val_loss: 0.058313243875378055\n",
      "epochs 108/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06763281565355628, val_loss: 0.05734442743031602\n",
      "epochs 109/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06512418654012053, val_loss: 0.05800691972437658\n",
      "epochs 110/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06853362344401448, val_loss: 0.05957538654145442\n",
      "epochs 111/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0680677047685573, val_loss: 0.05879682086800274\n",
      "epochs 112/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0680829016001601, val_loss: 0.057985047760762666\n",
      "epochs 113/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06439514776789829, val_loss: 0.05836068956475509\n",
      "epochs 114/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07352380214357063, val_loss: 0.05873979549658926\n",
      "epochs 115/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06725092408688445, val_loss: 0.057233183007491265\n",
      "epochs 116/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.064747888153713, val_loss: 0.05791375746852473\n",
      "epochs 117/134\n",
      "Epoch 00117: reducing learning rate of group 0 to 7.1496e-07.\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0690140056080724, val_loss: 0.05680726115640841\n",
      "epochs 118/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06415859810811908, val_loss: 0.05815777809996354\n",
      "epochs 119/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06459964640242488, val_loss: 0.05818648499093557\n",
      "epochs 120/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07378495224800549, val_loss: 0.057758913228386326\n",
      "epochs 121/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06926856430149392, val_loss: 0.05682247739873434\n",
      "epochs 122/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07308754476865656, val_loss: 0.05664976725452825\n",
      "epochs 123/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06663607948116566, val_loss: 0.05832174283109213\n",
      "epochs 124/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06519520655274391, val_loss: 0.058395621219747944\n",
      "epochs 125/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06620119389538702, val_loss: 0.0579487006915243\n",
      "epochs 126/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.067643955742058, val_loss: 0.058442514585821254\n",
      "epochs 127/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06752520436911207, val_loss: 0.05843242649969302\n",
      "epochs 128/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06711190012528707, val_loss: 0.05641409872393859\n",
      "epochs 129/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06701940768643429, val_loss: 0.058440843302952614\n",
      "epochs 130/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06961873134500102, val_loss: 0.05859605909178132\n",
      "epochs 131/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07010721983878236, val_loss: 0.05788016299668111\n",
      "epochs 132/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06553349624338903, val_loss: 0.05612805740613686\n",
      "epochs 133/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06523082723939105, val_loss: 0.056363807893113085\n",
      "epochs 134/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07062696626311854, val_loss: 0.057500829037867095\n",
      "Fold: 5/6\n",
      "epochs 1/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 8.66489280713232, val_loss: 0.18969513554322093\n",
      "epochs 2/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.4112389169241253, val_loss: 0.040905514633969256\n",
      "epochs 3/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 4.524434609475889, val_loss: 0.047749362885951996\n",
      "epochs 4/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 6.185522641633686, val_loss: 1.4796692258433293\n",
      "epochs 5/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 3.168376362323761, val_loss: 0.04248284607341415\n",
      "epochs 6/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.031104290014819, val_loss: 0.06971627650292296\n",
      "epochs 7/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.36939035387415636, val_loss: 0.06766058190872795\n",
      "epochs 8/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.28685707669509086, val_loss: 0.15236001148035652\n",
      "epochs 9/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.2167427982938917, val_loss: 0.058721027876201426\n",
      "epochs 10/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1693577626818105, val_loss: 0.07766479214555339\n",
      "epochs 11/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.20096442064172343, val_loss: 0.04129637503310254\n",
      "epochs 12/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.22599940378414957, val_loss: 0.16857788790213435\n",
      "epochs 13/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.1369351669361717, val_loss: 0.04603832960128784\n",
      "epochs 14/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.8679388038421932, val_loss: 0.19710003388555428\n",
      "epochs 15/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.5343480828561281, val_loss: 0.06709357527525801\n",
      "epochs 16/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 7.016565282721269, val_loss: 0.27123018158109563\n",
      "epochs 17/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 3.201949817883341, val_loss: 0.041409075260162354\n",
      "epochs 18/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.0551306699451648, val_loss: 0.07018781551405003\n",
      "epochs 19/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 1.0783864658129843, val_loss: 0.0738985481622972\n",
      "epochs 20/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.469222849293759, val_loss: 0.08389461746341303\n",
      "epochs 21/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.4667449955877505, val_loss: 0.3562692027342947\n",
      "epochs 22/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.24093627945372934, val_loss: 0.12304729340892089\n",
      "epochs 23/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.25845485815876407, val_loss: 0.045058312776841615\n",
      "epochs 24/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.739934026802841, val_loss: 0.05853115826060897\n",
      "epochs 25/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.17675590068101882, val_loss: 0.11857765011097256\n",
      "epochs 26/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.136740842461586, val_loss: 0.042111217975616455\n",
      "epochs 27/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.24812059845579298, val_loss: 0.14904720532266716\n",
      "epochs 28/134\n",
      "Current Learning Rate: 0.050720663019121384\n",
      "train_loss: 0.11771805258957964, val_loss: 0.1087818275156774\n",
      "epochs 29/134\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.2252e-03.\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.10215740603835959, val_loss: 0.1439684354945233\n",
      "epochs 30/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0627424826355357, val_loss: 0.1603102158558996\n",
      "epochs 31/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0610013147718028, val_loss: 0.12828354498273448\n",
      "epochs 32/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06790947231807207, val_loss: 0.14391490738642843\n",
      "epochs 33/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06714048295428879, val_loss: 0.14340722835377642\n",
      "epochs 34/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06551099733302468, val_loss: 0.13061444853481494\n",
      "epochs 35/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06500174971787553, val_loss: 0.14907284277050117\n",
      "epochs 36/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.060619261311857325, val_loss: 0.14489833775319552\n",
      "epochs 37/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06457236824851287, val_loss: 0.13405829471977135\n",
      "epochs 38/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06834307881562333, val_loss: 0.14321541707766683\n",
      "epochs 39/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06486142665932053, val_loss: 0.13475364093717776\n",
      "epochs 40/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07287814362268699, val_loss: 0.1417173131516105\n",
      "epochs 41/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06218843448318933, val_loss: 0.1491977345002325\n",
      "epochs 42/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0629002323668254, val_loss: 0.13092838619884692\n",
      "epochs 43/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06875076388057909, val_loss: 0.17774318942898198\n",
      "epochs 44/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06860704951380428, val_loss: 0.16061564417261825\n",
      "epochs 45/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06225153235228438, val_loss: 0.15241591004948868\n",
      "epochs 46/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06711690033736982, val_loss: 0.16811357360137136\n",
      "epochs 47/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.06367712714954427, val_loss: 0.1292784394402253\n",
      "epochs 48/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07515850263206582, val_loss: 0.21753127872943878\n",
      "epochs 49/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07900204936924733, val_loss: 0.10092316293402721\n",
      "epochs 50/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.09511016340632188, val_loss: 0.2122441114563691\n",
      "epochs 51/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0806528463959694, val_loss: 0.14205315944395566\n",
      "epochs 52/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.07921318572602774, val_loss: 0.17849703446814888\n",
      "epochs 53/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.08359669101865667, val_loss: 0.11764352337310188\n",
      "epochs 54/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.0991059363084404, val_loss: 0.24894831917787852\n",
      "epochs 55/134\n",
      "Current Learning Rate: 0.0012252284929454716\n",
      "train_loss: 0.10043948426058417, val_loss: 0.08819838848553206\n",
      "epochs 56/134\n",
      "Epoch 00056: reducing learning rate of group 0 to 2.9597e-05.\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.15411643303538625, val_loss: 0.30286004433506414\n",
      "epochs 57/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.33668894516794307, val_loss: 0.8164652115420291\n",
      "epochs 58/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.21750040979761825, val_loss: 0.5813134594967491\n",
      "epochs 59/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.16876494515883295, val_loss: 0.4641207158565521\n",
      "epochs 60/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.16555206814878864, val_loss: 0.4764646621126878\n",
      "epochs 61/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.13140014908031414, val_loss: 0.37398410157153483\n",
      "epochs 62/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.09485953465888375, val_loss: 0.2896379922565661\n",
      "epochs 63/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.12288918597133536, val_loss: 0.35920206967153046\n",
      "epochs 64/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.11038391445028155, val_loss: 0.3060969757406335\n",
      "epochs 65/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07187983930895203, val_loss: 0.20447291512238353\n",
      "epochs 66/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07301799380465558, val_loss: 0.19028502389004356\n",
      "epochs 67/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07898633519285604, val_loss: 0.22159967610710546\n",
      "epochs 68/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06786731834474363, val_loss: 0.1729261726140976\n",
      "epochs 69/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06743710531215918, val_loss: 0.1630981329240297\n",
      "epochs 70/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08132052147074749, val_loss: 0.23472189903259277\n",
      "epochs 71/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0695716249707498, val_loss: 0.1773755542541805\n",
      "epochs 72/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07105587364027374, val_loss: 0.1660687876375098\n",
      "epochs 73/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06491483963633839, val_loss: 0.15831637421720907\n",
      "epochs 74/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0798100856966094, val_loss: 0.20740240577020144\n",
      "epochs 75/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07467883291997407, val_loss: 0.2043133911333586\n",
      "epochs 76/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07718452375970389, val_loss: 0.2025601228601054\n",
      "epochs 77/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0689151712938359, val_loss: 0.1666489690542221\n",
      "epochs 78/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.06918511465191841, val_loss: 0.13404258144529244\n",
      "epochs 79/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.08753923075763803, val_loss: 0.22857796047863208\n",
      "epochs 80/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.07464453083904166, val_loss: 0.19991341703816465\n",
      "epochs 81/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.1103827250631232, val_loss: 0.28622545458768545\n",
      "epochs 82/134\n",
      "Current Learning Rate: 2.959710639743597e-05\n",
      "train_loss: 0.0685765746000566, val_loss: 0.12893446181949816\n",
      "epochs 83/134\n",
      "Epoch 00083: reducing learning rate of group 0 to 7.1496e-07.\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07058590262344008, val_loss: 0.14518121786807714\n",
      "epochs 84/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07524138822367317, val_loss: 0.18933167818345523\n",
      "epochs 85/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07079684440242617, val_loss: 0.15682242653871836\n",
      "epochs 86/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07238763441380701, val_loss: 0.15905552749571047\n",
      "epochs 87/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06805906127157964, val_loss: 0.1157284265286044\n",
      "epochs 88/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07249643673238002, val_loss: 0.15836884081363678\n",
      "epochs 89/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07224456385562295, val_loss: 0.11876497103979713\n",
      "epochs 90/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07162264742349324, val_loss: 0.15146905889636592\n",
      "epochs 91/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0726904903587542, val_loss: 0.1467091217637062\n",
      "epochs 92/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06929082458740786, val_loss: 0.126651136890838\n",
      "epochs 93/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07091565202725561, val_loss: 0.12502967252543098\n",
      "epochs 94/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07377631358410183, val_loss: 0.20830514634910383\n",
      "epochs 95/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07511672275631051, val_loss: 0.13065945161016365\n",
      "epochs 96/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06896278548397516, val_loss: 0.1426926514035777\n",
      "epochs 97/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06754938187567812, val_loss: 0.12138915297232176\n",
      "epochs 98/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06975089288071583, val_loss: 0.1285927774090516\n",
      "epochs 99/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07412239281754744, val_loss: 0.18445799460536555\n",
      "epochs 100/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0664853730484059, val_loss: 0.1657225693527021\n",
      "epochs 101/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06981364888580222, val_loss: 0.17992343635935532\n",
      "epochs 102/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.076218449834146, val_loss: 0.09925348194021928\n",
      "epochs 103/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07115608634133087, val_loss: 0.1565448763339143\n",
      "epochs 104/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06932191440933629, val_loss: 0.1329409919287029\n",
      "epochs 105/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06752308532595634, val_loss: 0.14863587876683787\n",
      "epochs 106/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.0694692237204627, val_loss: 0.15551211057524933\n",
      "epochs 107/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07092345133423805, val_loss: 0.1490974983102397\n",
      "epochs 108/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.07279429749438637, val_loss: 0.12114811061244261\n",
      "epochs 109/134\n",
      "Current Learning Rate: 7.149594644140641e-07\n",
      "train_loss: 0.06920326359962162, val_loss: 0.16834795710287595\n",
      "epochs 110/134\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.7271e-08.\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06812321077051915, val_loss: 0.14467609556097732\n",
      "epochs 111/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07455883700596659, val_loss: 0.1984769803913016\n",
      "epochs 112/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06960252464601868, val_loss: 0.1322916028530974\n",
      "epochs 113/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06605414042347356, val_loss: 0.16813637943644272\n",
      "epochs 114/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0631579891239342, val_loss: 0.16842506747496755\n",
      "epochs 115/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06875088901111955, val_loss: 0.1530490254885272\n",
      "epochs 116/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06679701479642015, val_loss: 0.1350518931683741\n",
      "epochs 117/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07253262906482345, val_loss: 0.13858321191448914\n",
      "epochs 118/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0668703994076503, val_loss: 0.16391302566779287\n",
      "epochs 119/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07026410463609194, val_loss: 0.11673065392594588\n",
      "epochs 120/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07705208137631417, val_loss: 0.1912885457277298\n",
      "epochs 121/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06879067299397368, val_loss: 0.1406955464105857\n",
      "epochs 122/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06517857124930934, val_loss: 0.14054016376796521\n",
      "epochs 123/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06927544588321134, val_loss: 0.1502518002924166\n",
      "epochs 124/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06938435435295105, val_loss: 0.13757584910643728\n",
      "epochs 125/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07477232786385636, val_loss: 0.1105011152593713\n",
      "epochs 126/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07449248170382099, val_loss: 0.11573787700188787\n",
      "epochs 127/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07153858750274307, val_loss: 0.1415547367773558\n",
      "epochs 128/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07823245674371719, val_loss: 0.17493597692564913\n",
      "epochs 129/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07273824897251631, val_loss: 0.17725718413528643\n",
      "epochs 130/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0726681932022697, val_loss: 0.142540065081496\n",
      "epochs 131/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.07154600322246552, val_loss: 0.16760536165613876\n",
      "epochs 132/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.08223105872932233, val_loss: 0.22069817624594035\n",
      "epochs 133/134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:16:36,251] Trial 0 finished with value: 0.23577630477200226 and parameters: {'batch_size': 56, 'epochs': 134, 'hidden_size': 368, 'layer_size': 3, 'learning_rate': 0.050720663019121384, 'dropout_prob': 0.21055577962475083, 'weight_decay': 0.0007103288681899751, 'lr_step_size': 5, 'gamma': 0.02415639741309312}. Best is trial 0 with value: 0.23577630477200226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.06798985137751228, val_loss: 0.1640766380648864\n",
      "epochs 134/134\n",
      "Current Learning Rate: 1.727084495663834e-08\n",
      "train_loss: 0.0688602033022203, val_loss: 0.1425735240704135\n",
      "Mean validation loss: 0.23577630477200226\n",
      "Fold: 1/6\n",
      "epochs 1/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 37.17221382868133, val_loss: 8.05691909790039\n",
      "epochs 2/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 3.964218829807482, val_loss: 1.621004360286813\n",
      "epochs 3/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 2.5958820542222574, val_loss: 0.4731923896623285\n",
      "epochs 4/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 2.070673850806136, val_loss: 0.051412414808414485\n",
      "epochs 5/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 1.9194479929773431, val_loss: 0.4544682445886888\n",
      "epochs 6/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 2.069275456039529, val_loss: 1.2446987083867977\n",
      "epochs 7/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 2.76982034664405, val_loss: 0.43645516214402097\n",
      "epochs 8/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 5.247316812214098, val_loss: 2.971526862759339\n",
      "epochs 9/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 19.29281886314091, val_loss: 11.63491886540463\n",
      "epochs 10/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 18.130368946414244, val_loss: 0.21172257061851651\n",
      "epochs 11/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 7.639250836874309, val_loss: 0.9390820962818045\n",
      "epochs 12/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 3.25101306799211, val_loss: 3.534020119591763\n",
      "epochs 13/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 3.6713588190706155, val_loss: 3.0679240650252293\n",
      "epochs 14/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 2.8369454545410058, val_loss: 0.6533221876934955\n",
      "epochs 15/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 2.210943921616203, val_loss: 2.330476065999583\n",
      "epochs 16/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 2.360954702684754, val_loss: 1.1801098279262845\n",
      "epochs 17/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 1.629774086569485, val_loss: 0.9091358569107557\n",
      "epochs 18/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 1.783825980205285, val_loss: 0.9948990380293444\n",
      "epochs 19/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 1.5423155623046976, val_loss: 1.13732088278783\n",
      "epochs 20/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 1.3040498832338734, val_loss: 0.056076556260354425\n",
      "epochs 21/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 0.5554361900216654, val_loss: 0.053790205746496976\n",
      "epochs 22/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 0.6495737335399577, val_loss: 0.09757643063111525\n",
      "epochs 23/90\n",
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 0.4318694074295069, val_loss: 0.05911440340002484\n",
      "epochs 24/90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:16:37,835] Trial 1 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.07286801385896725\n",
      "train_loss: 0.34306602003543, val_loss: 0.07945980624246754\n",
      "epochs 25/90\n",
      "Fold: 1/6\n",
      "epochs 1/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 18.389017377293815, val_loss: 1.4474280139333324\n",
      "epochs 2/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 15.852058518874017, val_loss: 4.940267873437781\n",
      "epochs 3/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 33.044348308914586, val_loss: 1.0324076818008172\n",
      "epochs 4/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 1.8337519215910059, val_loss: 0.05492654320244726\n",
      "epochs 5/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.46578436756604596, val_loss: 0.281669747574549\n",
      "epochs 6/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.8940088984213377, val_loss: 0.12071063046000506\n",
      "epochs 7/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.5469976730252567, val_loss: 0.08734476095751713\n",
      "epochs 8/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.5000971616490891, val_loss: 0.4676637973048185\n",
      "epochs 9/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 1.0692588047761666, val_loss: 0.2645396053987114\n",
      "epochs 10/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.49190891239988177, val_loss: 0.42676519975066185\n",
      "epochs 11/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.6678826601097458, val_loss: 0.22267245814988487\n",
      "epochs 12/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.8820848770831761, val_loss: 0.1051164833632739\n",
      "epochs 13/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.6443886901987227, val_loss: 0.2572870437840098\n",
      "epochs 14/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.6318560423035371, val_loss: 0.0707286462481869\n",
      "epochs 15/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.5657273727027994, val_loss: 0.08314382495652688\n",
      "epochs 16/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.6515246539523727, val_loss: 0.13058737851679325\n",
      "epochs 17/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.5149720789570558, val_loss: 0.11425356199278643\n",
      "epochs 18/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.526669799497253, val_loss: 0.05717056715174725\n",
      "epochs 19/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.48114046885779027, val_loss: 0.05133234859003048\n",
      "epochs 20/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.4726193527642049, val_loss: 0.06372148021565456\n",
      "epochs 21/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.3994104830842269, val_loss: 0.05109796548066171\n",
      "epochs 22/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.3886249494787894, val_loss: 0.053200290048200836\n",
      "epochs 23/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.27912684696677487, val_loss: 0.09289042592832916\n",
      "epochs 24/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.3332625643203133, val_loss: 0.1049147886469176\n",
      "epochs 25/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.2794573697212495, val_loss: 0.05083872341109734\n",
      "epochs 26/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.2575120114182171, val_loss: 0.055614517879133164\n",
      "epochs 27/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.24862225971331722, val_loss: 0.09952438108034824\n",
      "epochs 28/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.20636038776291044, val_loss: 0.056765490285071885\n",
      "epochs 29/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.17607010351984123, val_loss: 0.09677161251831996\n",
      "epochs 30/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.15230917940406422, val_loss: 0.051030692938519154\n",
      "epochs 31/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.12493171758557621, val_loss: 0.05167548544704914\n",
      "epochs 32/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.1385974148778539, val_loss: 0.05304636392056158\n",
      "epochs 33/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.10596241703943203, val_loss: 0.08647338413682423\n",
      "epochs 34/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.12467135699759972, val_loss: 0.1057481280478992\n",
      "epochs 35/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.10596993609674667, val_loss: 0.06398438549551524\n",
      "epochs 36/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.10468196589499712, val_loss: 0.05213093419412249\n",
      "epochs 37/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.09413858478594768, val_loss: 0.07674877340660284\n",
      "epochs 38/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.09374220144787901, val_loss: 0.11498654138688978\n",
      "epochs 39/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.09285676680309207, val_loss: 0.05060710963842116\n",
      "epochs 40/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.07709376636500422, val_loss: 0.051340463252640085\n",
      "epochs 41/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.05966982078787528, val_loss: 0.08404704639197964\n",
      "epochs 42/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.055136080610712894, val_loss: 0.07460190894964494\n",
      "epochs 43/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.07036551482681382, val_loss: 0.06883791790000703\n",
      "epochs 44/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.07400803133159091, val_loss: 0.05565578198844665\n",
      "epochs 45/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.04893231149272699, val_loss: 0.07496475420990273\n",
      "epochs 46/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.07157610498956944, val_loss: 0.10075449100450466\n",
      "epochs 47/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.0534150128890025, val_loss: 0.05538139203073163\n",
      "epochs 48/353\n",
      "Current Learning Rate: 0.07758526699901094\n",
      "train_loss: 0.04505158370164664, val_loss: 0.05557825989825161\n",
      "epochs 49/353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:16:39,915] Trial 2 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/6\n",
      "epochs 1/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 11.6845912576506, val_loss: 0.17334539384434097\n",
      "epochs 2/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 2.19931869365667, val_loss: 2.5912295454426815\n",
      "epochs 3/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 2.4653740352705906, val_loss: 0.08461467784486319\n",
      "epochs 4/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.967828000846662, val_loss: 0.5533992193247143\n",
      "epochs 5/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.3247216875223737, val_loss: 0.5264530628919601\n",
      "epochs 6/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.8143791321076845, val_loss: 0.06629057949114787\n",
      "epochs 7/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.2811587960704377, val_loss: 0.482634622407587\n",
      "epochs 8/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.4854351317411975, val_loss: 0.05167423536706912\n",
      "epochs 9/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.35164911888147654, val_loss: 0.2093798492692019\n",
      "epochs 10/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.3721390092059186, val_loss: 0.13972671496632852\n",
      "epochs 11/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.24802174536805405, val_loss: 0.22547788467062146\n",
      "epochs 12/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.2733678717754389, val_loss: 0.06667271829945476\n",
      "epochs 13/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.31899908126184817, val_loss: 0.06617399324712\n",
      "epochs 14/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.28227833756490756, val_loss: 0.07318384783636582\n",
      "epochs 15/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.1790029947694979, val_loss: 0.0911641270902596\n",
      "epochs 16/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.1660013967438748, val_loss: 0.05263858930648942\n",
      "epochs 17/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.2094505272413555, val_loss: 0.05657708713490712\n",
      "epochs 18/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.18759478609028615, val_loss: 0.08011560338108163\n",
      "epochs 19/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.16109067632963783, val_loss: 0.05200845125670496\n",
      "epochs 20/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.1814619567441313, val_loss: 0.05975671543886787\n",
      "epochs 21/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.17676627949664467, val_loss: 0.06117980554699898\n",
      "epochs 22/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.1402083762774342, val_loss: 0.05568933692809783\n",
      "epochs 23/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.13546527961367055, val_loss: 0.0744831672330436\n",
      "epochs 24/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.12719189630527244, val_loss: 0.06451130491730414\n",
      "epochs 25/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.10724714486614655, val_loss: 0.05096624600455949\n",
      "epochs 26/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.12118851569922347, val_loss: 0.05617139365916189\n",
      "epochs 27/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.11097150688108645, val_loss: 0.07423142961373455\n",
      "epochs 28/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.10752268772768347, val_loss: 0.050867172546292604\n",
      "epochs 29/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.1045978568298252, val_loss: 0.055615264187125785\n",
      "epochs 30/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.097811522060319, val_loss: 0.08009028478868697\n",
      "epochs 31/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07933998088303365, val_loss: 0.05616458985758455\n",
      "epochs 32/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07024699900495379, val_loss: 0.05504654896886725\n",
      "epochs 33/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.09196971043160088, val_loss: 0.0712477874599005\n",
      "epochs 34/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07525119499156349, val_loss: 0.051951966983707326\n",
      "epochs 35/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.08251677816243548, val_loss: 0.06475085877862416\n",
      "epochs 36/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.059296511162660624, val_loss: 0.05835831836846314\n",
      "epochs 37/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07275295071303844, val_loss: 0.05163033056612078\n",
      "epochs 38/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.08526872681747925, val_loss: 0.06972286428667997\n",
      "epochs 39/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.06164796684721583, val_loss: 0.05460816607075302\n",
      "epochs 40/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.0604401282770069, val_loss: 0.06185257768160418\n",
      "epochs 41/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.06285847483300849, val_loss: 0.06010727108897347\n",
      "epochs 42/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.05878688762650678, val_loss: 0.056529458171050796\n",
      "epochs 43/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.05422335113153646, val_loss: 0.06241089532053784\n",
      "epochs 44/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.06101547833532095, val_loss: 0.05248631801652281\n",
      "epochs 45/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.0673067633454737, val_loss: 0.06912719110321057\n",
      "epochs 46/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.048683353307607924, val_loss: 0.054462829261626065\n",
      "epochs 47/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.053046821665607, val_loss: 0.05668126218216984\n",
      "epochs 48/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.05289226397871971, val_loss: 0.058199356800239334\n",
      "epochs 49/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.05170812095074277, val_loss: 0.061269067178823446\n",
      "epochs 50/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.04205627974710966, val_loss: 0.05715710875627242\n",
      "epochs 51/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.0488965451031139, val_loss: 0.0612072108901645\n",
      "epochs 52/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.0466122992434784, val_loss: 0.06115812965129551\n",
      "epochs 53/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.04522231760385789, val_loss: 0.05699351655417367\n",
      "epochs 54/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.04882845370785186, val_loss: 0.05934183098571865\n",
      "epochs 55/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.045983368972022286, val_loss: 0.05830791367119864\n",
      "epochs 56/163\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.1591e-03.\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.049180396685474796, val_loss: 0.05871691760656081\n",
      "epochs 57/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04397636617680913, val_loss: 0.05810596220391361\n",
      "epochs 58/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04518407445989157, val_loss: 0.058122779879915085\n",
      "epochs 59/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.0423300873096052, val_loss: 0.058349612365035636\n",
      "epochs 60/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.043688740973409856, val_loss: 0.058122345050306695\n",
      "epochs 61/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.042704124640869465, val_loss: 0.058342938164347095\n",
      "epochs 62/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04305163785619171, val_loss: 0.058492767918658886\n",
      "epochs 63/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.0447225215795793, val_loss: 0.058831675027153994\n",
      "epochs 64/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04239131054399829, val_loss: 0.059481023418668066\n",
      "epochs 65/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.045555729468009974, val_loss: 0.059633205497735424\n",
      "epochs 66/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04305336256756594, val_loss: 0.05982665144103138\n",
      "epochs 67/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04612937226499382, val_loss: 0.05954096304546846\n",
      "epochs 68/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.041821711353565515, val_loss: 0.06022393982857466\n",
      "epochs 69/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.046160550886078885, val_loss: 0.06060458783452448\n",
      "epochs 70/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04039310379640052, val_loss: 0.060332109269342925\n",
      "epochs 71/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04328139150809301, val_loss: 0.06011873326803509\n",
      "epochs 72/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04657544426031803, val_loss: 0.0599854058635078\n",
      "epochs 73/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04329597327466074, val_loss: 0.059207742053427194\n",
      "epochs 74/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04427401623443553, val_loss: 0.059623128587478084\n",
      "epochs 75/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04310463359089274, val_loss: 0.05943631787637347\n",
      "epochs 76/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.043182269474001306, val_loss: 0.05977246072143316\n",
      "epochs 77/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04152826018827526, val_loss: 0.05960329548504792\n",
      "epochs 78/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.0422072187556248, val_loss: 0.05931399890074605\n",
      "epochs 79/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04509461136828912, val_loss: 0.05906695343161884\n",
      "epochs 80/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04611904482896391, val_loss: 0.05945474120151056\n",
      "epochs 81/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04276039809184639, val_loss: 0.059760841933128084\n",
      "epochs 82/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.039183933827045714, val_loss: 0.05994639898601331\n",
      "epochs 83/163\n",
      "Current Learning Rate: 0.0011591086977148569\n",
      "train_loss: 0.04268816588936668, val_loss: 0.05992251296380633\n",
      "epochs 84/163\n",
      "Epoch 00084: reducing learning rate of group 0 to 3.1435e-05.\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.03955879072217565, val_loss: 0.05998261601321007\n",
      "epochs 85/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04083429115187181, val_loss: 0.05992927725769972\n",
      "epochs 86/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.044131436483248285, val_loss: 0.060233057672648055\n",
      "epochs 87/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.043726137888274695, val_loss: 0.05987539617834907\n",
      "epochs 88/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04482966622239665, val_loss: 0.05991924505092596\n",
      "epochs 89/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04078459857325805, val_loss: 0.05990169305158289\n",
      "epochs 90/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.042046338270761464, val_loss: 0.060065530828739465\n",
      "epochs 91/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.039926838717962564, val_loss: 0.06005395292059371\n",
      "epochs 92/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.043456054096551316, val_loss: 0.060211484222427794\n",
      "epochs 93/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04280276567135986, val_loss: 0.05995720384740516\n",
      "epochs 94/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04000658228209144, val_loss: 0.05987129487881535\n",
      "epochs 95/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04341007626958584, val_loss: 0.06020707754712356\n",
      "epochs 96/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.0423176484182477, val_loss: 0.05994437360449841\n",
      "epochs 97/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04295505593089681, val_loss: 0.059881489919988734\n",
      "epochs 98/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.042850857600569725, val_loss: 0.060164564779322395\n",
      "epochs 99/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.0417685126395602, val_loss: 0.06007138698508865\n",
      "epochs 100/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04543801029457858, val_loss: 0.0600937613237061\n",
      "epochs 101/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.042558807438533554, val_loss: 0.060065288341751226\n",
      "epochs 102/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.042676423774345926, val_loss: 0.06021829797445159\n",
      "epochs 103/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04444482285333307, val_loss: 0.06020539778431779\n",
      "epochs 104/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04618955918244625, val_loss: 0.0601995574114354\n",
      "epochs 105/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04242744007589001, val_loss: 0.060142754381032365\n",
      "epochs 106/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.038761914021482595, val_loss: 0.059866190593885746\n",
      "epochs 107/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.042837061919271946, val_loss: 0.06007018180466012\n",
      "epochs 108/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04352486408070514, val_loss: 0.060061262479346046\n",
      "epochs 109/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.044014482513854376, val_loss: 0.059921386063491046\n",
      "epochs 110/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04550737260203613, val_loss: 0.059994793055873165\n",
      "epochs 111/163\n",
      "Current Learning Rate: 3.143454459531035e-05\n",
      "train_loss: 0.04429029815487171, val_loss: 0.05980568094865272\n",
      "epochs 112/163\n",
      "Epoch 00112: reducing learning rate of group 0 to 8.5249e-07.\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.03731062733813336, val_loss: 0.0601366332879192\n",
      "epochs 113/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04231321105831548, val_loss: 0.06004080499865507\n",
      "epochs 114/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04121592189920576, val_loss: 0.060051393744192626\n",
      "epochs 115/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.040185307275111735, val_loss: 0.05999887014101995\n",
      "epochs 116/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04200407632283474, val_loss: 0.060109067365135015\n",
      "epochs 117/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04323583542320289, val_loss: 0.06001531822901023\n",
      "epochs 118/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04345066492494784, val_loss: 0.05968621647671649\n",
      "epochs 119/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04454726269958835, val_loss: 0.05998026251204704\n",
      "epochs 120/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.044779996583728415, val_loss: 0.059849238366280734\n",
      "epochs 121/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.0423310339744938, val_loss: 0.05922693391575625\n",
      "epochs 122/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04294584539571875, val_loss: 0.05948001126709737\n",
      "epochs 123/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04414959859691168, val_loss: 0.0601618112015881\n",
      "epochs 124/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.040873514194237556, val_loss: 0.05995603636103241\n",
      "epochs 125/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04079148864471599, val_loss: 0.06006835768685529\n",
      "epochs 126/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04137172897983538, val_loss: 0.06003784084398495\n",
      "epochs 127/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04385565829120184, val_loss: 0.060050335124527156\n",
      "epochs 128/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04115041864937857, val_loss: 0.059962114025103416\n",
      "epochs 129/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.042606778050723826, val_loss: 0.06010301327823024\n",
      "epochs 130/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.041319820490714754, val_loss: 0.05970491699286198\n",
      "epochs 131/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04507867339998484, val_loss: 0.05975362565368414\n",
      "epochs 132/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.042868324163320815, val_loss: 0.059900606149121335\n",
      "epochs 133/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04305632554582859, val_loss: 0.0600874069097795\n",
      "epochs 134/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.039450963125809244, val_loss: 0.059882061114828834\n",
      "epochs 135/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04409213614110884, val_loss: 0.060136305413355955\n",
      "epochs 136/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04167759516521504, val_loss: 0.06004676237506302\n",
      "epochs 137/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.045442822438321616, val_loss: 0.06013551594591454\n",
      "epochs 138/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.0441564920015241, val_loss: 0.06000983141558735\n",
      "epochs 139/163\n",
      "Current Learning Rate: 8.524917428905684e-07\n",
      "train_loss: 0.04278404172509909, val_loss: 0.06001349965012387\n",
      "epochs 140/163\n",
      "Epoch 00140: reducing learning rate of group 0 to 2.3119e-08.\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.041886962362025916, val_loss: 0.059916404222971516\n",
      "epochs 141/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04190157858752891, val_loss: 0.05989538946826207\n",
      "epochs 142/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04399404586537888, val_loss: 0.06004956487174097\n",
      "epochs 143/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04412665080867315, val_loss: 0.06002940217915334\n",
      "epochs 144/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04366297436583983, val_loss: 0.059627564525917956\n",
      "epochs 145/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04186404119000623, val_loss: 0.05974671715184262\n",
      "epochs 146/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04471831332500044, val_loss: 0.06000719734124447\n",
      "epochs 147/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.042226195580473073, val_loss: 0.059907716012706884\n",
      "epochs 148/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04380188367672657, val_loss: 0.06005993706027144\n",
      "epochs 149/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.043271953485121854, val_loss: 0.06011468564209185\n",
      "epochs 150/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.043601489282752336, val_loss: 0.06005343280144428\n",
      "epochs 151/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04210222135052869, val_loss: 0.06001945114449451\n",
      "epochs 152/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.03852080352800457, val_loss: 0.05971206474657122\n",
      "epochs 153/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.039280449383352935, val_loss: 0.059866434061213544\n",
      "epochs 154/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04246343858540058, val_loss: 0.05997044691129735\n",
      "epochs 155/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04218158370962268, val_loss: 0.05989241369656826\n",
      "epochs 156/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.043347000340489966, val_loss: 0.05988683242742952\n",
      "epochs 157/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04333236401802615, val_loss: 0.06005593286337037\n",
      "epochs 158/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.043538499623537064, val_loss: 0.05984774089761471\n",
      "epochs 159/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.042709710223502236, val_loss: 0.0600585834564347\n",
      "epochs 160/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04091324544462718, val_loss: 0.06021646674918501\n",
      "epochs 161/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04452166618093064, val_loss: 0.06024203032843376\n",
      "epochs 162/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04254835393083723, val_loss: 0.060065062226433506\n",
      "epochs 163/163\n",
      "Current Learning Rate: 2.311922062344813e-08\n",
      "train_loss: 0.04206412883573457, val_loss: 0.059731086372937024\n",
      "Fold: 2/6\n",
      "epochs 1/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 8.576318593891827, val_loss: 0.3158586929111104\n",
      "epochs 2/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.14426594124616762, val_loss: 0.0850375834852457\n",
      "epochs 3/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.25637321860382434, val_loss: 0.08240577269737658\n",
      "epochs 4/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.28675853313976213, val_loss: 0.06752581958119806\n",
      "epochs 5/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.27195425321789163, val_loss: 0.09949029835039064\n",
      "epochs 6/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.508242946902388, val_loss: 0.057213423234459604\n",
      "epochs 7/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.2980690637701436, val_loss: 0.06228468650461812\n",
      "epochs 8/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.39574902739964035, val_loss: 0.07409271828242038\n",
      "epochs 9/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.3057410851317017, val_loss: 0.06772760566520064\n",
      "epochs 10/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.27108973617616455, val_loss: 0.14741997499214976\n",
      "epochs 11/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.2412476886652018, val_loss: 0.09441134696336169\n",
      "epochs 12/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.26233592658842864, val_loss: 0.2019507659501151\n",
      "epochs 13/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.2968406390006605, val_loss: 0.051843661569843165\n",
      "epochs 14/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.2065875308686181, val_loss: 0.05178393903923662\n",
      "epochs 15/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.22795319267989775, val_loss: 0.2345462276747352\n",
      "epochs 16/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.1420877706259489, val_loss: 0.05228467315043274\n",
      "epochs 17/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.13469327956830202, val_loss: 0.08205278276612885\n",
      "epochs 18/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.1385215174308733, val_loss: 0.0925903900673515\n",
      "epochs 19/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.10413409700911296, val_loss: 0.054437450745976286\n",
      "epochs 20/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.11125683407054136, val_loss: 0.07740091924604617\n",
      "epochs 21/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.11357157393113564, val_loss: 0.0986620462254474\n",
      "epochs 22/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.09055756213829706, val_loss: 0.05704863259176675\n",
      "epochs 23/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.11392314423267778, val_loss: 0.07006947240351062\n",
      "epochs 24/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.09583254013896773, val_loss: 0.08735153961338495\n",
      "epochs 25/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07696957258801711, val_loss: 0.06135933258031544\n",
      "epochs 26/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07477151063319884, val_loss: 0.08226421294047644\n",
      "epochs 27/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.08827864319870346, val_loss: 0.1133633816712781\n",
      "epochs 28/163\n",
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07325049051034607, val_loss: 0.06893044455271018\n",
      "epochs 29/163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:16:50,237] Trial 3 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.04274065332947975\n",
      "train_loss: 0.07133738899995622, val_loss: 0.060577842676521915\n",
      "epochs 30/163\n",
      "Fold: 1/6\n",
      "epochs 1/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 12.670090256160812, val_loss: 2.8955719470977783\n",
      "epochs 2/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 4.446963511015239, val_loss: 0.12067207919531747\n",
      "epochs 3/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 3.1028101765795757, val_loss: 0.9060299545526505\n",
      "epochs 4/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 1.2123832777142525, val_loss: 0.3655903166846225\n",
      "epochs 5/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 1.229714924763692, val_loss: 0.4129126020952275\n",
      "epochs 6/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 3.1481594098241708, val_loss: 0.051625689638680534\n",
      "epochs 7/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.47452982456276294, val_loss: 1.6466908988199735\n",
      "epochs 8/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 1.110595652931615, val_loss: 0.34485429917511184\n",
      "epochs 9/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.23830318666602435, val_loss: 0.21534686143461026\n",
      "epochs 10/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.508769841962739, val_loss: 0.21980062283967672\n",
      "epochs 11/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.3698439519656332, val_loss: 0.0875731929156341\n",
      "epochs 12/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.28355086281111364, val_loss: 0.306094465679244\n",
      "epochs 13/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.3800290477903266, val_loss: 0.1289167806114021\n",
      "epochs 14/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.2074287649440138, val_loss: 0.05548939569608161\n",
      "epochs 15/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.27876590780521693, val_loss: 0.0590620966334092\n",
      "epochs 16/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.25746268288869606, val_loss: 0.08475631102919579\n",
      "epochs 17/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.22364413581396403, val_loss: 0.1717764492097654\n",
      "epochs 18/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.2818896350891967, val_loss: 0.08302048152606738\n",
      "epochs 19/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.21544418189870684, val_loss: 0.053557559847831726\n",
      "epochs 20/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.23588770314266808, val_loss: 0.05394208240077684\n",
      "epochs 21/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.22937521083574547, val_loss: 0.10115806052559301\n",
      "epochs 22/224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:16:51,121] Trial 4 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.230464647082906, val_loss: 0.18951544008756938\n",
      "epochs 23/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.2228237251309972, val_loss: 0.06762664907268788\n",
      "epochs 24/224\n",
      "Current Learning Rate: 0.06032104918576558\n",
      "train_loss: 0.16549460099715935, val_loss: 0.05906634944442071\n",
      "epochs 25/224\n",
      "Fold: 1/6\n",
      "epochs 1/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 161.39793474442865, val_loss: 0.2701046627603079\n",
      "epochs 2/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 38.89285432743399, val_loss: 112.17927300302605\n",
      "epochs 3/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 94.76683936621014, val_loss: 0.5368332141324094\n",
      "epochs 4/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 149.0251747934442, val_loss: 46.89433376412643\n",
      "epochs 5/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 65.18583280161808, val_loss: 37.92562294006348\n",
      "epochs 6/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 57.08888218120525, val_loss: 7.4927268279226205\n",
      "epochs 7/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 103.39024250130905, val_loss: 0.4380804882629922\n",
      "epochs 8/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 29.09270315421255, val_loss: 5.770299218202892\n",
      "epochs 9/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.329419245845393, val_loss: 1.39167399547602\n",
      "epochs 10/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.8026683706986275, val_loss: 0.08931789061936893\n",
      "epochs 11/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 3.435936185874437, val_loss: 0.38150284537359286\n",
      "epochs 12/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 3.217126836117945, val_loss: 1.0969057357624958\n",
      "epochs 13/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.563188060333854, val_loss: 0.13133030958277614\n",
      "epochs 14/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 3.7965352017628518, val_loss: 0.8024650682744227\n",
      "epochs 15/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.3385425319797113, val_loss: 0.2552746300046381\n",
      "epochs 16/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 3.316762757928748, val_loss: 0.14550711420413695\n",
      "epochs 17/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.9111526639837968, val_loss: 0.13139752994634604\n",
      "epochs 18/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.3374517803129398, val_loss: 0.12224702203744336\n",
      "epochs 19/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.5634582842651166, val_loss: 0.23784178769902178\n",
      "epochs 20/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.8997985242228759, val_loss: 0.27889718546679143\n",
      "epochs 21/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.5299091386167625, val_loss: 0.11991425485987413\n",
      "epochs 22/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.502908218848078, val_loss: 0.7391509833304506\n",
      "epochs 23/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.7801633947773983, val_loss: 0.17012939641350194\n",
      "epochs 24/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.37378360095777, val_loss: 0.20461789146065712\n",
      "epochs 25/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.9536653913949666, val_loss: 0.05106656522931237\n",
      "epochs 26/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.3738030948136983, val_loss: 0.4756313893747957\n",
      "epochs 27/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.087007808842157, val_loss: 0.14198544263643653\n",
      "epochs 28/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.9629094279126118, val_loss: 0.0773506521884548\n",
      "epochs 29/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.399528221080178, val_loss: 0.19388762067415213\n",
      "epochs 30/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.140552133321762, val_loss: 0.3837574828220041\n",
      "epochs 31/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.8015816392082917, val_loss: 0.12681550292396232\n",
      "epochs 32/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.8634143840325506, val_loss: 0.23774362188812934\n",
      "epochs 33/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.958245658560803, val_loss: 0.2865122005735573\n",
      "epochs 34/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.4356793871051385, val_loss: 0.16446213894768766\n",
      "epochs 35/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.709670699740711, val_loss: 0.2913928890698834\n",
      "epochs 36/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.321750894188881, val_loss: 0.14334001803868696\n",
      "epochs 37/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.5852178272448088, val_loss: 0.35360249190738324\n",
      "epochs 38/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.7325203677541332, val_loss: 0.4424428169272448\n",
      "epochs 39/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.8602991966824782, val_loss: 0.18635565630699458\n",
      "epochs 40/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.7672514068452936, val_loss: 0.13600558091543222\n",
      "epochs 41/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.6482451965934353, val_loss: 0.5477809290352621\n",
      "epochs 42/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.556962234409232, val_loss: 0.4544802333571409\n",
      "epochs 43/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.6461676272906756, val_loss: 0.07192766396818977\n",
      "epochs 44/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.6085228684701418, val_loss: 0.07089228285966735\n",
      "epochs 45/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 0.9693132964404005, val_loss: 0.1299260636595519\n",
      "epochs 46/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.11537388594527, val_loss: 0.051496374837465976\n",
      "epochs 47/258\n",
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 2.4909858562444387, val_loss: 0.05172198334414708\n",
      "epochs 48/258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:16:57,039] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.057602604163558294\n",
      "train_loss: 1.5697565306174128, val_loss: 0.9648122924723124\n",
      "epochs 49/258\n",
      "Fold: 1/6\n",
      "epochs 1/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.7979477518973382, val_loss: 0.1933930732898022\n",
      "epochs 2/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.08187861184246446, val_loss: 0.04462031207635606\n",
      "epochs 3/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03859892404197078, val_loss: 0.059584920453888024\n",
      "epochs 4/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.033021176969142335, val_loss: 0.03426026845792014\n",
      "epochs 5/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03458030493684897, val_loss: 0.0484491453481544\n",
      "epochs 6/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.025070877414882967, val_loss: 0.030087110169820096\n",
      "epochs 7/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029344611825715555, val_loss: 0.03862304923026577\n",
      "epochs 8/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02217884066416637, val_loss: 0.024547662965855317\n",
      "epochs 9/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.023132864018215946, val_loss: 0.02482742919145446\n",
      "epochs 10/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.021125805214978755, val_loss: 0.028193835818551873\n",
      "epochs 11/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019933443491984355, val_loss: 0.021774386190564224\n",
      "epochs 12/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019725407528887063, val_loss: 0.020974614020240933\n",
      "epochs 13/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.022806497860552843, val_loss: 0.02355324880121962\n",
      "epochs 14/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02248857509785969, val_loss: 0.026113536429444428\n",
      "epochs 15/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019595268157948004, val_loss: 0.021634464838394995\n",
      "epochs 16/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.017934189227066542, val_loss: 0.02025711091578399\n",
      "epochs 17/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.020645101456657836, val_loss: 0.020270300918797914\n",
      "epochs 18/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.021055801606148873, val_loss: 0.023413859877588327\n",
      "epochs 19/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.0184263630929452, val_loss: 0.02121834610711391\n",
      "epochs 20/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018794444524437973, val_loss: 0.019374773788609002\n",
      "epochs 21/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018064022799463647, val_loss: 0.019202167266293577\n",
      "epochs 22/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018378978821841117, val_loss: 0.01928622012124642\n",
      "epochs 23/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01822828661993538, val_loss: 0.019407644812752933\n",
      "epochs 24/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018497615439915342, val_loss: 0.018884388539989135\n",
      "epochs 25/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01949959269749295, val_loss: 0.018851088335443484\n",
      "epochs 26/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018329251122562902, val_loss: 0.018958581378683448\n",
      "epochs 27/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01898287859818849, val_loss: 0.01878126899368669\n",
      "epochs 28/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01864448914573969, val_loss: 0.01915301322495859\n",
      "epochs 29/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01835647788462474, val_loss: 0.019642252861031967\n",
      "epochs 30/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.017467427698552217, val_loss: 0.019381113313628656\n",
      "epochs 31/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01554459000104352, val_loss: 0.01858586234065067\n",
      "epochs 32/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.017910573619270797, val_loss: 0.020274067181162536\n",
      "epochs 33/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02047629801458434, val_loss: 0.01914055056036695\n",
      "epochs 34/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02231868216767907, val_loss: 0.025006395041648494\n",
      "epochs 35/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01886779440591406, val_loss: 0.02420982999089909\n",
      "epochs 36/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01706506950952309, val_loss: 0.019939228279614134\n",
      "epochs 37/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01839954817152925, val_loss: 0.020997591230920272\n",
      "epochs 38/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.022421211090036912, val_loss: 0.02179169437333353\n",
      "epochs 39/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019081125452526305, val_loss: 0.02471323319906859\n",
      "epochs 40/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01724915706405514, val_loss: 0.01876085992338822\n",
      "epochs 41/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01740016961641806, val_loss: 0.020008197538309583\n",
      "epochs 42/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019802338719416997, val_loss: 0.019661744505068975\n",
      "epochs 43/78\n",
      "Epoch 00043: reducing learning rate of group 0 to 4.5049e-04.\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.019313019835145065, val_loss: 0.02172972133236104\n",
      "epochs 44/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01709017142236821, val_loss: 0.019040679938993173\n",
      "epochs 45/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01645733859684122, val_loss: 0.018625436062728495\n",
      "epochs 46/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.016594195805833135, val_loss: 0.018892199824270057\n",
      "epochs 47/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.015904759436747747, val_loss: 0.019380298234816445\n",
      "epochs 48/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.015805011020826275, val_loss: 0.01877420318410977\n",
      "epochs 49/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.015973420721772862, val_loss: 0.018597757027141358\n",
      "epochs 50/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.015951260936593537, val_loss: 0.01879065824476512\n",
      "epochs 51/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01584083340008204, val_loss: 0.018987537153359307\n",
      "epochs 52/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.016014595748856664, val_loss: 0.01872508958178131\n",
      "epochs 53/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01677920185887304, val_loss: 0.018608514550387076\n",
      "epochs 54/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.015943067252488907, val_loss: 0.018678134203104203\n",
      "epochs 55/78\n",
      "Epoch 00055: reducing learning rate of group 0 to 4.3351e-05.\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.015791632516897823, val_loss: 0.01897790423275805\n",
      "epochs 56/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.015601514906034265, val_loss: 0.01895044419277263\n",
      "epochs 57/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.015552441822364926, val_loss: 0.018889759906175498\n",
      "epochs 58/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.015846799072613448, val_loss: 0.01880427628240891\n",
      "epochs 59/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01584370013350915, val_loss: 0.018729462435371\n",
      "epochs 60/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.015889908033913297, val_loss: 0.018682297104724535\n",
      "epochs 61/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.0152739597864351, val_loss: 0.01865576907355142\n",
      "epochs 62/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01582403421573537, val_loss: 0.01862060436121139\n",
      "epochs 63/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01548431756734652, val_loss: 0.018612102740199157\n",
      "epochs 64/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.015159186141222324, val_loss: 0.018629119198426213\n",
      "epochs 65/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.015478037383505389, val_loss: 0.018653205559147817\n",
      "epochs 66/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01574250256740733, val_loss: 0.018670157094388025\n",
      "epochs 67/78\n",
      "Epoch 00067: reducing learning rate of group 0 to 4.1717e-06.\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015514060040004551, val_loss: 0.01866360756838204\n",
      "epochs 68/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015389378323513819, val_loss: 0.01866215572839505\n",
      "epochs 69/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01520652134633182, val_loss: 0.018661552396798998\n",
      "epochs 70/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01584330112305715, val_loss: 0.018659413510345314\n",
      "epochs 71/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015396235884461356, val_loss: 0.01865795011460585\n",
      "epochs 72/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015506882631023856, val_loss: 0.01865698204155227\n",
      "epochs 73/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015849188074322517, val_loss: 0.018656729830821093\n",
      "epochs 74/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01600925345586515, val_loss: 0.018657067331093315\n",
      "epochs 75/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015959092238182693, val_loss: 0.018658638055632382\n",
      "epochs 76/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01598583656902376, val_loss: 0.018657983072396172\n",
      "epochs 77/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015816927170626036, val_loss: 0.018658155697937075\n",
      "epochs 78/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.015091029921007392, val_loss: 0.018659707416143072\n",
      "Fold: 2/6\n",
      "epochs 1/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.47485265955574024, val_loss: 0.13811620873840233\n",
      "epochs 2/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.0743887109289828, val_loss: 0.03941469112607209\n",
      "epochs 3/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.0604708501333861, val_loss: 0.07260667625814676\n",
      "epochs 4/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.034527945344483384, val_loss: 0.03240975403030844\n",
      "epochs 5/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03079067978481027, val_loss: 0.02842127763372111\n",
      "epochs 6/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029419769447198825, val_loss: 0.02687434789626614\n",
      "epochs 7/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.025082019574352, val_loss: 0.02049984635883256\n",
      "epochs 8/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.021079414971417895, val_loss: 0.020743852260669594\n",
      "epochs 9/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.020657376680327088, val_loss: 0.02714486079486577\n",
      "epochs 10/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01983591057145723, val_loss: 0.03049816696071311\n",
      "epochs 11/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02117218904650623, val_loss: 0.03351734729336673\n",
      "epochs 12/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.022950337472138926, val_loss: 0.03775993398831863\n",
      "epochs 13/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.027116798795759678, val_loss: 0.03340905935405508\n",
      "epochs 14/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03252331620560175, val_loss: 0.020733676127795326\n",
      "epochs 15/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.031419908928096686, val_loss: 0.02851011931871701\n",
      "epochs 16/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.024049006181916122, val_loss: 0.02096145736715315\n",
      "epochs 17/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02002954052908248, val_loss: 0.017643605922593883\n",
      "epochs 18/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019189591488898976, val_loss: 0.021315432202659156\n",
      "epochs 19/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018887277190132362, val_loss: 0.02792888588124984\n",
      "epochs 20/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.020605781685413892, val_loss: 0.03158822726752413\n",
      "epochs 21/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.025986626266354795, val_loss: 0.026615567713681804\n",
      "epochs 22/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029667751376492607, val_loss: 0.019525600070329874\n",
      "epochs 23/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.026493971787491126, val_loss: 0.023854016975842808\n",
      "epochs 24/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.021145902044066276, val_loss: 0.018878147998628646\n",
      "epochs 25/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018557802875109605, val_loss: 0.01788718930421103\n",
      "epochs 26/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01841632661709905, val_loss: 0.021291066046902223\n",
      "epochs 27/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.018788931828519134, val_loss: 0.02963430012919401\n",
      "epochs 28/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.020320930902686853, val_loss: 0.03056793106033614\n",
      "epochs 29/78\n",
      "Epoch 00029: reducing learning rate of group 0 to 4.5049e-04.\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.024910234787967056, val_loss: 0.02850611669648635\n",
      "epochs 30/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02278878687464289, val_loss: 0.020188721368628506\n",
      "epochs 31/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018649372645947886, val_loss: 0.019641805133831344\n",
      "epochs 32/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.017982963926011796, val_loss: 0.0190181742838927\n",
      "epochs 33/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01781053011501698, val_loss: 0.019600476947073873\n",
      "epochs 34/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01778091049728621, val_loss: 0.018412626899876876\n",
      "epochs 35/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.017522645676140917, val_loss: 0.01846671991638447\n",
      "epochs 36/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01679249280622523, val_loss: 0.018887219564548058\n",
      "epochs 37/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01691756658807495, val_loss: 0.018433226505294442\n",
      "epochs 38/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.015899659658316523, val_loss: 0.018242672012236557\n",
      "epochs 39/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.016146843946587882, val_loss: 0.018180125478753133\n",
      "epochs 40/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.016989252631135873, val_loss: 0.018253120556963903\n",
      "epochs 41/78\n",
      "Epoch 00041: reducing learning rate of group 0 to 4.3351e-05.\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.017252035347107602, val_loss: 0.017901891621908073\n",
      "epochs 42/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016990389907732606, val_loss: 0.01791815988498887\n",
      "epochs 43/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.017036611321504767, val_loss: 0.017952943833446817\n",
      "epochs 44/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.017249299770858335, val_loss: 0.01801051926112881\n",
      "epochs 45/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01660091043327396, val_loss: 0.018063789251946696\n",
      "epochs 46/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016675063918994152, val_loss: 0.01812310705549623\n",
      "epochs 47/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01624744964800285, val_loss: 0.018162604231436393\n",
      "epochs 48/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016091595774540973, val_loss: 0.018183051314401\n",
      "epochs 49/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01674060456111635, val_loss: 0.01820283237305519\n",
      "epochs 50/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016650665269576405, val_loss: 0.018203396037662106\n",
      "epochs 51/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.0163782527608993, val_loss: 0.0181751491832792\n",
      "epochs 52/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016304235280442395, val_loss: 0.01813760847098341\n",
      "epochs 53/78\n",
      "Epoch 00053: reducing learning rate of group 0 to 4.1717e-06.\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016952870444296615, val_loss: 0.018125943380015854\n",
      "epochs 54/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01677090303947855, val_loss: 0.018122002482414246\n",
      "epochs 55/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01656564252794181, val_loss: 0.01811575715577132\n",
      "epochs 56/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017183009378832617, val_loss: 0.018111611177262506\n",
      "epochs 57/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016935965380214742, val_loss: 0.01811035284404888\n",
      "epochs 58/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016412467400139002, val_loss: 0.018109636957217987\n",
      "epochs 59/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016477588712740198, val_loss: 0.018107537762261927\n",
      "epochs 60/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017336505266078012, val_loss: 0.018108346217655037\n",
      "epochs 61/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016268821160529592, val_loss: 0.01810740047796188\n",
      "epochs 62/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.0164917350385191, val_loss: 0.018105196429563587\n",
      "epochs 63/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016193373851772202, val_loss: 0.018100868003736986\n",
      "epochs 64/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016892127642150673, val_loss: 0.01809580723975638\n",
      "epochs 65/78\n",
      "Epoch 00065: reducing learning rate of group 0 to 4.0144e-07.\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.016893562857723355, val_loss: 0.018084390039898847\n",
      "epochs 66/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.016978875551630107, val_loss: 0.018084152595532175\n",
      "epochs 67/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01658676818662666, val_loss: 0.01808360872765709\n",
      "epochs 68/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.017005906121707277, val_loss: 0.018083314196892866\n",
      "epochs 69/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.016766317464460275, val_loss: 0.018083395718253757\n",
      "epochs 70/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.016399166257255467, val_loss: 0.01808351955352057\n",
      "epochs 71/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01708211034450582, val_loss: 0.01808375348704622\n",
      "epochs 72/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.016700099905536166, val_loss: 0.018084039727814104\n",
      "epochs 73/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.016615128460168643, val_loss: 0.01808466359139665\n",
      "epochs 74/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.0165392463738834, val_loss: 0.01808439388773159\n",
      "epochs 75/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01657194981663978, val_loss: 0.01808409796611063\n",
      "epochs 76/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.017118074229648828, val_loss: 0.01808364134845569\n",
      "epochs 77/78\n",
      "Epoch 00077: reducing learning rate of group 0 to 3.8631e-08.\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.016632762930576542, val_loss: 0.018082976243213603\n",
      "epochs 78/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.01667085490618391, val_loss: 0.018082953143962903\n",
      "Fold: 3/6\n",
      "epochs 1/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.4127226839829522, val_loss: 0.035319995496569105\n",
      "epochs 2/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.04274381600369356, val_loss: 0.03441551986983732\n",
      "epochs 3/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.033709116370964534, val_loss: 0.023375052820866632\n",
      "epochs 4/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029950721632574864, val_loss: 0.027822552352996643\n",
      "epochs 5/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.026809609540035587, val_loss: 0.024916593175332406\n",
      "epochs 6/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.021640285191416184, val_loss: 0.023791120528537584\n",
      "epochs 7/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019874614867659516, val_loss: 0.022485894439006716\n",
      "epochs 8/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.025426348348621997, val_loss: 0.026828653032058163\n",
      "epochs 9/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.031117748338274714, val_loss: 0.03203517773286685\n",
      "epochs 10/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02387450401482738, val_loss: 0.02229094429027387\n",
      "epochs 11/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019186072772483163, val_loss: 0.023898689943292226\n",
      "epochs 12/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.025110375440450746, val_loss: 0.025548116003751363\n",
      "epochs 13/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.023702673254790665, val_loss: 0.027837302548026566\n",
      "epochs 14/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.021355631900134318, val_loss: 0.022417660024458247\n",
      "epochs 15/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01810989410640966, val_loss: 0.02408704938563077\n",
      "epochs 16/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03194323496325268, val_loss: 0.035670470593399124\n",
      "epochs 17/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02532797658472861, val_loss: 0.022468324900776344\n",
      "epochs 18/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.023513507651698188, val_loss: 0.027141388542459982\n",
      "epochs 19/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019495189550900505, val_loss: 0.02284048484809893\n",
      "epochs 20/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019940735630307085, val_loss: 0.022093047246974157\n",
      "epochs 21/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.027264118388605613, val_loss: 0.023545003784085184\n",
      "epochs 22/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.020832684522470675, val_loss: 0.022389101406779924\n",
      "epochs 23/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01783203193254099, val_loss: 0.02213710248102679\n",
      "epochs 24/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.021763049817660397, val_loss: 0.024572057753654297\n",
      "epochs 25/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.024042411724031695, val_loss: 0.03145734834068112\n",
      "epochs 26/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.023471396964372327, val_loss: 0.021608823735732585\n",
      "epochs 27/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01728373132809474, val_loss: 0.02366326780573122\n",
      "epochs 28/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02568934345413653, val_loss: 0.027760566457321768\n",
      "epochs 29/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.025741535708843195, val_loss: 0.022398747414914204\n",
      "epochs 30/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01881882091810551, val_loss: 0.022491082432679832\n",
      "epochs 31/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01970515064200746, val_loss: 0.02251082404798485\n",
      "epochs 32/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02576714277414507, val_loss: 0.029634371142540323\n",
      "epochs 33/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02356382238374129, val_loss: 0.022641567243753297\n",
      "epochs 34/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.017203127927802455, val_loss: 0.021866305455516436\n",
      "epochs 35/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.01926065672683252, val_loss: 0.023345880792476237\n",
      "epochs 36/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.024160664693019435, val_loss: 0.025078500124723895\n",
      "epochs 37/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.026852311745690378, val_loss: 0.02900868418969606\n",
      "epochs 38/78\n",
      "Epoch 00038: reducing learning rate of group 0 to 4.5049e-04.\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.023915201528149754, val_loss: 0.02186517949265085\n",
      "epochs 39/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01668733901756799, val_loss: 0.02273987619647462\n",
      "epochs 40/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01687413465418885, val_loss: 0.022191153413395545\n",
      "epochs 41/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01663226994589837, val_loss: 0.022116213411975064\n",
      "epochs 42/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01666808245282849, val_loss: 0.022121245997320665\n",
      "epochs 43/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.016827979528961964, val_loss: 0.021985066500124766\n",
      "epochs 44/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01628391860459376, val_loss: 0.02203350601484999\n",
      "epochs 45/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01674225743351455, val_loss: 0.021898285660427064\n",
      "epochs 46/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.016642484679131916, val_loss: 0.021965006550185774\n",
      "epochs 47/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01682707973898352, val_loss: 0.02165212927962114\n",
      "epochs 48/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01665116166731694, val_loss: 0.02184822959947939\n",
      "epochs 49/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.016409050585424297, val_loss: 0.021680462563189826\n",
      "epochs 50/78\n",
      "Epoch 00050: reducing learning rate of group 0 to 4.3351e-05.\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016454541833107277, val_loss: 0.022116106763286025\n",
      "epochs 51/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01649316855534715, val_loss: 0.02176320150580355\n",
      "epochs 52/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016198231655340477, val_loss: 0.02147541097732947\n",
      "epochs 53/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01624784666682101, val_loss: 0.021409534539806804\n",
      "epochs 54/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016228941906439632, val_loss: 0.02145610863379644\n",
      "epochs 55/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.0162439009046035, val_loss: 0.02149276400756973\n",
      "epochs 56/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.017299834203808324, val_loss: 0.021488170075156774\n",
      "epochs 57/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.0170590607633226, val_loss: 0.021442346915137023\n",
      "epochs 58/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01618907337583471, val_loss: 0.021421687861316298\n",
      "epochs 59/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016889145860260583, val_loss: 0.021421729204073352\n",
      "epochs 60/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016932120124948324, val_loss: 0.02140912647040463\n",
      "epochs 61/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01614160318677661, val_loss: 0.021465382391684933\n",
      "epochs 62/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01681809857811395, val_loss: 0.02151778207074753\n",
      "epochs 63/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016238419136355157, val_loss: 0.021506501315774296\n",
      "epochs 64/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.016230956947925222, val_loss: 0.021467721855611\n",
      "epochs 65/78\n",
      "Epoch 00065: reducing learning rate of group 0 to 4.1717e-06.\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016930073187706927, val_loss: 0.021437270061333516\n",
      "epochs 66/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016643930428248847, val_loss: 0.02144041334854831\n",
      "epochs 67/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016394151328233676, val_loss: 0.021442130891189567\n",
      "epochs 68/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01613723289138809, val_loss: 0.021443712996858124\n",
      "epochs 69/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016592508695076072, val_loss: 0.02144427938497086\n",
      "epochs 70/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01640571352107687, val_loss: 0.021445166337990054\n",
      "epochs 71/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017014233880521108, val_loss: 0.021443695709182833\n",
      "epochs 72/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016534257638181372, val_loss: 0.02144188449509736\n",
      "epochs 73/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016634313011309132, val_loss: 0.021443556997263313\n",
      "epochs 74/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016410338743937023, val_loss: 0.021444579659912148\n",
      "epochs 75/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.016841355867463312, val_loss: 0.021443441755285387\n",
      "epochs 76/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01709307809237783, val_loss: 0.02144504850423944\n",
      "epochs 77/78\n",
      "Epoch 00077: reducing learning rate of group 0 to 4.0144e-07.\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.016824471468632845, val_loss: 0.02144606579646566\n",
      "epochs 78/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01703702124104255, val_loss: 0.021446000224003864\n",
      "Fold: 4/6\n",
      "epochs 1/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.3577294767233788, val_loss: 0.071626529244608\n",
      "epochs 2/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.04875978447441747, val_loss: 0.027582139242440462\n",
      "epochs 3/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03386357565882223, val_loss: 0.025636951647404777\n",
      "epochs 4/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03125022457587827, val_loss: 0.02884481716165809\n",
      "epochs 5/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03284495351774814, val_loss: 0.028723018562519236\n",
      "epochs 6/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03235433554877282, val_loss: 0.023226886252431495\n",
      "epochs 7/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02737955213371223, val_loss: 0.022493336931802332\n",
      "epochs 8/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.023819814901798964, val_loss: 0.02270301328481812\n",
      "epochs 9/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02321765198537737, val_loss: 0.022663015835477335\n",
      "epochs 10/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02227592577784028, val_loss: 0.023107193986346062\n",
      "epochs 11/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02218424403690733, val_loss: 0.022201898085002444\n",
      "epochs 12/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02741537469218632, val_loss: 0.028619215799201476\n",
      "epochs 13/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.034498537214331965, val_loss: 0.02574327029287815\n",
      "epochs 14/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.030150066556464145, val_loss: 0.02681973606568614\n",
      "epochs 15/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.020000466681698238, val_loss: 0.02467772420373206\n",
      "epochs 16/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019672144323091122, val_loss: 0.02438529596149333\n",
      "epochs 17/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.020020932581055126, val_loss: 0.03139363182708621\n",
      "epochs 18/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.019293893310915383, val_loss: 0.025706832646392286\n",
      "epochs 19/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02321336825231188, val_loss: 0.026478640931217295\n",
      "epochs 20/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.028615314112118397, val_loss: 0.03460260008853909\n",
      "epochs 21/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.038656494860525095, val_loss: 0.022350210163399185\n",
      "epochs 22/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.025859422573519527, val_loss: 0.024026738302046925\n",
      "epochs 23/78\n",
      "Epoch 00023: reducing learning rate of group 0 to 4.5049e-04.\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.019738462346906186, val_loss: 0.023659179526332178\n",
      "epochs 24/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01919676193057584, val_loss: 0.029006691337072926\n",
      "epochs 25/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.017909501779573282, val_loss: 0.027007412226674587\n",
      "epochs 26/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018455225189283834, val_loss: 0.027072622348848534\n",
      "epochs 27/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018186787890858556, val_loss: 0.027926052301680965\n",
      "epochs 28/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018021001269899944, val_loss: 0.026983881222182197\n",
      "epochs 29/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018207188886375854, val_loss: 0.02728922456184304\n",
      "epochs 30/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018574604007115244, val_loss: 0.02707544954395608\n",
      "epochs 31/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01871479009078987, val_loss: 0.02644062937408882\n",
      "epochs 32/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018414171391142237, val_loss: 0.02711857048990695\n",
      "epochs 33/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018014482429287836, val_loss: 0.026495304663273458\n",
      "epochs 34/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.018975396310500065, val_loss: 0.026685341114276333\n",
      "epochs 35/78\n",
      "Epoch 00035: reducing learning rate of group 0 to 4.3351e-05.\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.018395587657180027, val_loss: 0.026420039088024122\n",
      "epochs 36/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.018437740991143266, val_loss: 0.027488077090023773\n",
      "epochs 37/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.018138773938469393, val_loss: 0.028330724527079024\n",
      "epochs 38/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.017931815869432238, val_loss: 0.0285467303137442\n",
      "epochs 39/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.018048596918417485, val_loss: 0.028468053156853114\n",
      "epochs 40/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.018227924384470833, val_loss: 0.02852663591406063\n",
      "epochs 41/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01784771330054163, val_loss: 0.028371340221431302\n",
      "epochs 42/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.018428437316048842, val_loss: 0.028504428747845322\n",
      "epochs 43/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.017412546782380606, val_loss: 0.028610169550551002\n",
      "epochs 44/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.017998842200062138, val_loss: 0.028335551210483045\n",
      "epochs 45/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01800664231230162, val_loss: 0.028253130014299563\n",
      "epochs 46/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.018342682348080223, val_loss: 0.028334357193671167\n",
      "epochs 47/78\n",
      "Epoch 00047: reducing learning rate of group 0 to 4.1717e-06.\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01803812727647662, val_loss: 0.02848872858540792\n",
      "epochs 48/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01833589543668112, val_loss: 0.0285280392578754\n",
      "epochs 49/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.018466366164686502, val_loss: 0.028551474667007203\n",
      "epochs 50/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017838357815267762, val_loss: 0.02856403321493417\n",
      "epochs 51/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.018510060780834846, val_loss: 0.02857305899899649\n",
      "epochs 52/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017818190489043973, val_loss: 0.028559258281204262\n",
      "epochs 53/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017512231900534397, val_loss: 0.028541569352934237\n",
      "epochs 54/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01807447409450297, val_loss: 0.028522996624049387\n",
      "epochs 55/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017622072773519903, val_loss: 0.02854104160551766\n",
      "epochs 56/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.018184799769905544, val_loss: 0.028547170970245804\n",
      "epochs 57/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.017335595615900513, val_loss: 0.028535550209007374\n",
      "epochs 58/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01815556468576879, val_loss: 0.028565397332912607\n",
      "epochs 59/78\n",
      "Epoch 00059: reducing learning rate of group 0 to 4.0144e-07.\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.018053607969226216, val_loss: 0.028595942881350454\n",
      "epochs 60/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.017620258683516448, val_loss: 0.028597093450738804\n",
      "epochs 61/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.017286725055064858, val_loss: 0.028600750080505877\n",
      "epochs 62/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.017993474301826944, val_loss: 0.02860028475621029\n",
      "epochs 63/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.017682546552877237, val_loss: 0.02860009388409947\n",
      "epochs 64/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.018191726421814804, val_loss: 0.028599201101123503\n",
      "epochs 65/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01765297101136591, val_loss: 0.028599207865466412\n",
      "epochs 66/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.017835510298793548, val_loss: 0.028598553550086524\n",
      "epochs 67/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01770544398995729, val_loss: 0.028597752551401134\n",
      "epochs 68/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01794854547417919, val_loss: 0.02860102602158134\n",
      "epochs 69/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01780722058902029, val_loss: 0.028605384494815218\n",
      "epochs 70/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.018434557171483328, val_loss: 0.028606870995932503\n",
      "epochs 71/78\n",
      "Epoch 00071: reducing learning rate of group 0 to 3.8631e-08.\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.0182979579059113, val_loss: 0.02861085633967856\n",
      "epochs 72/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.017955994227361913, val_loss: 0.028611325475044157\n",
      "epochs 73/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.017418002554270635, val_loss: 0.028611423447728157\n",
      "epochs 74/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.01799423823448658, val_loss: 0.028611390820802433\n",
      "epochs 75/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.01749170571948573, val_loss: 0.02861122954881897\n",
      "epochs 76/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.018026180985065077, val_loss: 0.02861082511586382\n",
      "epochs 77/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.018038093976333346, val_loss: 0.028610577304406387\n",
      "epochs 78/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.017902105815377774, val_loss: 0.028610517638490388\n",
      "Fold: 5/6\n",
      "epochs 1/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.3390268101847093, val_loss: 0.060740009881556034\n",
      "epochs 2/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.05851840431569144, val_loss: 0.0388337569556346\n",
      "epochs 3/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03503129908839535, val_loss: 0.042180056034244205\n",
      "epochs 4/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.04879359804142855, val_loss: 0.027738800309108275\n",
      "epochs 5/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.032871150068546594, val_loss: 0.028852476395274464\n",
      "epochs 6/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03415431622048154, val_loss: 0.03649892376147603\n",
      "epochs 7/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03634286120435909, val_loss: 0.02726507310657517\n",
      "epochs 8/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029756774834822863, val_loss: 0.029661292357272225\n",
      "epochs 9/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03098906099330634, val_loss: 0.03295674982578739\n",
      "epochs 10/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.032625521965153316, val_loss: 0.03255585650913417\n",
      "epochs 11/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03247852284632819, val_loss: 0.029416151062928532\n",
      "epochs 12/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03107102267064252, val_loss: 0.030285824372090007\n",
      "epochs 13/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.030374048617790993, val_loss: 0.03139996179379523\n",
      "epochs 14/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.030970522315903127, val_loss: 0.03294278865091895\n",
      "epochs 15/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.031244962589545667, val_loss: 0.0316901798972762\n",
      "epochs 16/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03326533378506275, val_loss: 0.035410416067430846\n",
      "epochs 17/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.033282148801893195, val_loss: 0.032068481350219565\n",
      "epochs 18/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.032614664030628966, val_loss: 0.03312749679102317\n",
      "epochs 19/78\n",
      "Epoch 00019: reducing learning rate of group 0 to 4.5049e-04.\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.032003354078992026, val_loss: 0.032392879310799275\n",
      "epochs 20/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.027577712160755732, val_loss: 0.05212078775328241\n",
      "epochs 21/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.020635729462619087, val_loss: 0.03337896115293628\n",
      "epochs 22/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.020935871460671097, val_loss: 0.038581044829793666\n",
      "epochs 23/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.020545050949103347, val_loss: 0.03636985566270979\n",
      "epochs 24/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.020202711537039203, val_loss: 0.03661794788939388\n",
      "epochs 25/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.020282536113977824, val_loss: 0.03663781391897876\n",
      "epochs 26/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01989218802497673, val_loss: 0.03636530803908643\n",
      "epochs 27/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.020648390765448933, val_loss: 0.0360458181362207\n",
      "epochs 28/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02030264261166418, val_loss: 0.035422009354653325\n",
      "epochs 29/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02030209491740128, val_loss: 0.0342669762229841\n",
      "epochs 30/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.01975084575640626, val_loss: 0.035786696219522705\n",
      "epochs 31/78\n",
      "Epoch 00031: reducing learning rate of group 0 to 4.3351e-05.\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.02003968250631404, val_loss: 0.03546354228532628\n",
      "epochs 32/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.020000505074245955, val_loss: 0.036452981839446646\n",
      "epochs 33/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.019763208683702703, val_loss: 0.03739599759788498\n",
      "epochs 34/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01960032154793704, val_loss: 0.037786619272083044\n",
      "epochs 35/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.020286040993309336, val_loss: 0.03781916443748694\n",
      "epochs 36/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.019688776109098017, val_loss: 0.03785088949983841\n",
      "epochs 37/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.019638758217058096, val_loss: 0.037495207887044864\n",
      "epochs 38/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01952621981035918, val_loss: 0.037523431703448296\n",
      "epochs 39/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01972777972696349, val_loss: 0.03772428032549981\n",
      "epochs 40/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.019773202081284438, val_loss: 0.037884547656990196\n",
      "epochs 41/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.01981192306112988, val_loss: 0.037757895041355176\n",
      "epochs 42/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.019783636417215394, val_loss: 0.037648633355274796\n",
      "epochs 43/78\n",
      "Epoch 00043: reducing learning rate of group 0 to 4.1717e-06.\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.020365685537016315, val_loss: 0.037661694088264516\n",
      "epochs 44/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.019459743895200325, val_loss: 0.03762005922678662\n",
      "epochs 45/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.019233296052430217, val_loss: 0.03758614989438731\n",
      "epochs 46/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.020120230969041586, val_loss: 0.0375851025241182\n",
      "epochs 47/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.019863518347081385, val_loss: 0.03759782811577775\n",
      "epochs 48/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.020003777634548515, val_loss: 0.03761670511755112\n",
      "epochs 49/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.019579152199807333, val_loss: 0.037650874019355365\n",
      "epochs 50/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01967886102839226, val_loss: 0.03770566605472643\n",
      "epochs 51/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.019872702526443294, val_loss: 0.03775555146613011\n",
      "epochs 52/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.019441731669327344, val_loss: 0.037760154454429685\n",
      "epochs 53/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.0197048927197772, val_loss: 0.037751042038986556\n",
      "epochs 54/78\n",
      "Current Learning Rate: 4.171651886318826e-06\n",
      "train_loss: 0.01977278791233211, val_loss: 0.03776045367856951\n",
      "epochs 55/78\n",
      "Epoch 00055: reducing learning rate of group 0 to 4.0144e-07.\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.020222910643114073, val_loss: 0.03777286526747048\n",
      "epochs 56/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01996598398545757, val_loss: 0.03777413519932643\n",
      "epochs 57/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.019805072396258384, val_loss: 0.037773712501420004\n",
      "epochs 58/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01959723318424566, val_loss: 0.0377696065697819\n",
      "epochs 59/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.018911864378759146, val_loss: 0.03776901475104846\n",
      "epochs 60/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01937325496079498, val_loss: 0.03776619180203661\n",
      "epochs 61/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.01978229478577544, val_loss: 0.03776360118124438\n",
      "epochs 62/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.019295168960908132, val_loss: 0.03776316385177013\n",
      "epochs 63/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.019982150884492223, val_loss: 0.037762904821551944\n",
      "epochs 64/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.019475658470152046, val_loss: 0.03776583857344169\n",
      "epochs 65/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.019129830921117804, val_loss: 0.03776850419020966\n",
      "epochs 66/78\n",
      "Current Learning Rate: 4.014389008457059e-07\n",
      "train_loss: 0.020001108074977406, val_loss: 0.03776575361476525\n",
      "epochs 67/78\n",
      "Epoch 00067: reducing learning rate of group 0 to 3.8631e-08.\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019742034577862604, val_loss: 0.03776169989846254\n",
      "epochs 68/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019432355848900778, val_loss: 0.037762009048540345\n",
      "epochs 69/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019436411323687553, val_loss: 0.03776228909478768\n",
      "epochs 70/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019337178165044048, val_loss: 0.03776244364531809\n",
      "epochs 71/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.01988840990050353, val_loss: 0.037762576322022234\n",
      "epochs 72/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.01976564566996929, val_loss: 0.03776254629912345\n",
      "epochs 73/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019769285857603933, val_loss: 0.03776210291605247\n",
      "epochs 74/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.020149547698613452, val_loss: 0.037761576865848745\n",
      "epochs 75/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019697289469676388, val_loss: 0.03776133627826838\n",
      "epochs 76/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019641419005653773, val_loss: 0.037760834944875615\n",
      "epochs 77/78\n",
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.019645683936113, val_loss: 0.037760528956392876\n",
      "epochs 78/78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:17:31,674] Trial 6 finished with value: 0.027111990626374455 and parameters: {'batch_size': 35, 'epochs': 78, 'hidden_size': 478, 'layer_size': 1, 'learning_rate': 0.004681379806469059, 'dropout_prob': 0.24681607903179412, 'weight_decay': 0.008333678497820345, 'lr_step_size': 7, 'gamma': 0.09623020131719237}. Best is trial 6 with value: 0.027111990626374455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 3.863054624493471e-08\n",
      "train_loss: 0.020013127365688745, val_loss: 0.03776049550230566\n",
      "Mean validation loss: 0.027111990626374455\n",
      "Fold: 1/6\n",
      "epochs 1/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 1.6428767079977613, val_loss: 547.6337786222759\n",
      "epochs 2/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 365.435469577187, val_loss: 31.12767357575266\n",
      "epochs 3/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 8.07859699663363, val_loss: 22.513882310766924\n",
      "epochs 4/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 12.688241792352576, val_loss: 37.91844618947882\n",
      "epochs 5/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 42.30628008591501, val_loss: 6.5796427726745605\n",
      "epochs 6/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 1.2145641210832094, val_loss: 11.49002628577383\n",
      "epochs 7/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 20.458429386741237, val_loss: 16.623340506302682\n",
      "epochs 8/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 54.142987426958584, val_loss: 29.59292655242117\n",
      "epochs 9/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 42.803629097185635, val_loss: 0.7731113276983562\n",
      "epochs 10/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 3.390021645709088, val_loss: 0.9840218644393118\n",
      "epochs 11/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 5.664308667182922, val_loss: 1.0986600400585878\n",
      "epochs 12/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 17.704467811082537, val_loss: 1.5546507600106692\n",
      "epochs 13/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 1.6535546026731793, val_loss: 0.2348004873645933\n",
      "epochs 14/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 34.5438529253006, val_loss: 8.489155493284526\n",
      "epochs 15/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 13.506260664839493, val_loss: 12.722442463824624\n",
      "epochs 16/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 17.1006340227629, val_loss: 0.18638932273576134\n",
      "epochs 17/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 2.4324767213118705, val_loss: 5.333406523654335\n",
      "epochs 18/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 2.211668963495054, val_loss: 1.008843943476677\n",
      "epochs 19/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 17.001879334449768, val_loss: 1.742585815881428\n",
      "epochs 20/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 12.36879112845973, val_loss: 7.129258871078491\n",
      "epochs 21/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 14.368020860772384, val_loss: 0.6777239167376569\n",
      "epochs 22/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 5.285530995381506, val_loss: 0.5190035371403945\n",
      "epochs 23/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 10.497943752690366, val_loss: 1.8735553010513908\n",
      "epochs 24/174\n",
      "Current Learning Rate: 0.0330020836295319\n",
      "train_loss: 4.25495163390511, val_loss: 0.3184102012922889\n",
      "epochs 25/174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:17:33,930] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/6\n",
      "epochs 1/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 58.97900985769535, val_loss: 104.66898908113178\n",
      "epochs 2/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 149.06921898691277, val_loss: 97.96338382520173\n",
      "epochs 3/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 111.15056121270908, val_loss: 63.3173010976691\n",
      "epochs 4/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 28.91605727923544, val_loss: 5.759925421915557\n",
      "epochs 5/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 6.590382077192006, val_loss: 0.2812883650001727\n",
      "epochs 6/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 17.48281681537628, val_loss: 858.4196395874023\n",
      "epochs 7/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 631.3971118927002, val_loss: 148.4139421362626\n",
      "epochs 8/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 89.084155929716, val_loss: 65.81441146449039\n",
      "epochs 9/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 101.38629615306854, val_loss: 16.39816232731468\n",
      "epochs 10/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 33.829330265522, val_loss: 1.985433412225623\n",
      "epochs 11/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 23.91112894760935, val_loss: 0.153045829434536\n",
      "epochs 12/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 216.55006002124986, val_loss: 118.34396246859902\n",
      "epochs 13/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 47.182915662464346, val_loss: 7.780210978106449\n",
      "epochs 14/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 34.589252848374215, val_loss: 0.8426360033060375\n",
      "epochs 15/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 28.790342224271672, val_loss: 27.119982556292886\n",
      "epochs 16/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 31.86190518579985, val_loss: 7.918475226352089\n",
      "epochs 17/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 18.56615758569617, val_loss: 19.769370292362414\n",
      "epochs 18/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 103.95125890405555, val_loss: 45.685377171165065\n",
      "epochs 19/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 17.10426281627856, val_loss: 0.05208003922904793\n",
      "epochs 20/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 24.71404472777718, val_loss: 1.1620232313871384\n",
      "epochs 21/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 21.30478278586739, val_loss: 0.5490271770640424\n",
      "epochs 22/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 11.573043798145495, val_loss: 1.7631555505489047\n",
      "epochs 23/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 10.06429987518411, val_loss: 1.3115925263417394\n",
      "epochs 24/123\n",
      "Current Learning Rate: 0.09271620698943918\n",
      "train_loss: 12.728082992528615, val_loss: 1.0289951727578515\n",
      "epochs 25/123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:17:38,445] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/6\n",
      "epochs 1/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 236.89910176533618, val_loss: 74.05994239606355\n",
      "epochs 2/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 95.53404601624138, val_loss: 78.5096946515535\n",
      "epochs 3/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 32.87714971994099, val_loss: 11.18477117387872\n",
      "epochs 4/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 32.51063402075516, val_loss: 4.819160492796647\n",
      "epochs 5/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 13.546952799746865, val_loss: 11.552638580924587\n",
      "epochs 6/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 23.51366502987711, val_loss: 12.916986854452835\n",
      "epochs 7/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 15.096562623977661, val_loss: 2.647072020329927\n",
      "epochs 8/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 11.59520639871296, val_loss: 0.07808180408258188\n",
      "epochs 9/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 6.811958952953941, val_loss: 2.274198591709137\n",
      "epochs 10/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 9.34027001732274, val_loss: 9.992450161984092\n",
      "epochs 11/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 11.095294651232267, val_loss: 2.0250381309735146\n",
      "epochs 12/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 8.410915192804838, val_loss: 2.4287789338513424\n",
      "epochs 13/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 7.795603563911037, val_loss: 0.12257734068522327\n",
      "epochs 14/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 5.794274254849083, val_loss: 0.3294004213653113\n",
      "epochs 15/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 4.655853949095073, val_loss: 0.19805411759175753\n",
      "epochs 16/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 6.831956744194031, val_loss: 0.053400125403545405\n",
      "epochs 17/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 6.085868145290174, val_loss: 0.6439900939401827\n",
      "epochs 18/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 6.597595302682174, val_loss: 0.06362543774670676\n",
      "epochs 19/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 6.376695419612684, val_loss: 0.049643517324798984\n",
      "epochs 20/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 5.862435987121181, val_loss: 0.04830193980351875\n",
      "epochs 21/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 6.411202813449659, val_loss: 0.04272397485022482\n",
      "epochs 22/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 4.802846042733443, val_loss: 0.48916168628554596\n",
      "epochs 23/195\n",
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 4.975191292009856, val_loss: 0.3347712229741247\n",
      "epochs 24/195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:17:39,709] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.06292216767836477\n",
      "train_loss: 4.9710814074466105, val_loss: 0.2889046645478198\n",
      "epochs 25/195\n",
      "Fold: 1/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.2963024139110195, val_loss: 0.04157622830059968\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.07753479968462336, val_loss: 0.141689377983934\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.04173981997576591, val_loss: 0.03542180822573995\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03821459603740981, val_loss: 0.0633527755149101\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.027405126645278773, val_loss: 0.027174497954547405\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026974519812747053, val_loss: 0.040871204195642157\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02349054674912048, val_loss: 0.022980053602766833\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.022012555052673344, val_loss: 0.023130879627148573\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.022251680064456242, val_loss: 0.027766918665484377\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02086703535659533, val_loss: 0.02170654424270125\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.019232873412731447, val_loss: 0.021583034228043335\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.018811825111410337, val_loss: 0.019541712355260785\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.017326257228361147, val_loss: 0.019470594970411377\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02045156925573553, val_loss: 0.020267328455750095\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021882173617517476, val_loss: 0.024686154578567335\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.019336361070408634, val_loss: 0.021834305842946235\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.01716645707155725, val_loss: 0.019491539434774927\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.01805811195854882, val_loss: 0.01849745247071903\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.019014205691698743, val_loss: 0.018827255349606276\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.023062413901482757, val_loss: 0.020258123753592372\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.028060244494362882, val_loss: 0.02955714170821011\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021613909090918145, val_loss: 0.02543368204006631\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.01810693900149904, val_loss: 0.018887889583742146\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.016663329240767973, val_loss: 0.019100299362387312\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.018368319736940687, val_loss: 0.019119929166902836\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.023342469400775275, val_loss: 0.020945679567950338\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.027524023708936415, val_loss: 0.032735212963368544\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02104275770436384, val_loss: 0.025769395925300687\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.017663713495590184, val_loss: 0.019011333731836395\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.017240231924102102, val_loss: 0.01860061979019328\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.019851472210384122, val_loss: 0.018628191153861974\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.022200290787670957, val_loss: 0.02263803059529317\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.022580312050290798, val_loss: 0.02814918218523656\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.018540376172351995, val_loss: 0.020272183636399477\n",
      "epochs 35/50\n",
      "Epoch 00035: reducing learning rate of group 0 to 3.5003e-04.\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017263705744162985, val_loss: 0.018938069170536965\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017987386115189446, val_loss: 0.01865470243961011\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015191103791278837, val_loss: 0.019774435345377577\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015195441609983774, val_loss: 0.01911863815774651\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017146602763157142, val_loss: 0.018674739906074184\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016938279076528392, val_loss: 0.01876581530086696\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017294330191553423, val_loss: 0.019174441486891163\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015719192427288937, val_loss: 0.01899141674910329\n",
      "epochs 43/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016291169149458017, val_loss: 0.018682535786769892\n",
      "epochs 44/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016604049045494514, val_loss: 0.019079010088094754\n",
      "epochs 45/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015107951402713201, val_loss: 0.01869422832111779\n",
      "epochs 46/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017521556289131313, val_loss: 0.018833548663870284\n",
      "epochs 47/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01633617595009702, val_loss: 0.019085788388589497\n",
      "epochs 48/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016580060253066842, val_loss: 0.018738287488782878\n",
      "epochs 49/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015601822438551799, val_loss: 0.01867645693403718\n",
      "epochs 50/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015866448067275708, val_loss: 0.01864478772653169\n",
      "Fold: 2/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.23268300749508566, val_loss: 0.036777558932571036\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.042857562093750426, val_loss: 0.05833803837824809\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.0380918673472479, val_loss: 0.05339892752664654\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.0322604145376796, val_loss: 0.057566136170766856\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02877523846875288, val_loss: 0.056483822090453224\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026135724509674076, val_loss: 0.040578169503102175\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02181654655693197, val_loss: 0.043550085423416214\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021431675266572518, val_loss: 0.03485971400024075\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.019943877555258376, val_loss: 0.03369773961113472\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.019059304840323563, val_loss: 0.03578695625458893\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.01933074196016318, val_loss: 0.034626227313358536\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021307331123879475, val_loss: 0.039049288674600814\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.01930251599007629, val_loss: 0.029289231831698043\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.01928804479661937, val_loss: 0.02906999474783477\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02414627924046822, val_loss: 0.04788423760941154\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021320335594569577, val_loss: 0.024608589733313573\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.018027505028600756, val_loss: 0.027612127290156326\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.020061851742579358, val_loss: 0.04389765868453603\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026606135015179843, val_loss: 0.04317371985924087\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.020485436445788333, val_loss: 0.019517236199278972\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.01737628397409265, val_loss: 0.02593392362867139\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02438909792659902, val_loss: 0.05216140231411708\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.023567021809101692, val_loss: 0.022247467802739458\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.016441483779712336, val_loss: 0.027699688792620834\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021615288275490075, val_loss: 0.04026014634751176\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026208730720865884, val_loss: 0.04109249805639449\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021295290051248708, val_loss: 0.021719233635322827\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.017385178558087272, val_loss: 0.026723077030558335\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.020266571939971886, val_loss: 0.0395265059839738\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.024789852133069775, val_loss: 0.03997708440415169\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.020999175576051993, val_loss: 0.0204880155966078\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.017756964527315608, val_loss: 0.022147849202156067\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02353164355736226, val_loss: 0.05680836849894963\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.027818370157068496, val_loss: 0.02266132994554937\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.017666399037759555, val_loss: 0.025182013350882028\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.018621347078710403, val_loss: 0.02625887278516434\n",
      "epochs 37/50\n",
      "Epoch 00037: reducing learning rate of group 0 to 3.5003e-04.\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.019019107557955738, val_loss: 0.027917354612758283\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02037407789918545, val_loss: 0.01862507937230954\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016437997076488835, val_loss: 0.01826761990148378\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.018199467029741134, val_loss: 0.017081348213220115\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016889954016445892, val_loss: 0.01750353366553195\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017647593867303312, val_loss: 0.01725616982500804\n",
      "epochs 43/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016850415188329, val_loss: 0.018012553606623488\n",
      "epochs 44/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015802190433820022, val_loss: 0.01801986510154644\n",
      "epochs 45/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016383686277549714, val_loss: 0.017509617744699904\n",
      "epochs 46/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.015989893437730836, val_loss: 0.01751708009549858\n",
      "epochs 47/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01648978713446444, val_loss: 0.01798252326338307\n",
      "epochs 48/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01665561898660503, val_loss: 0.017276301361179275\n",
      "epochs 49/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.0163982244955964, val_loss: 0.0178048372280931\n",
      "epochs 50/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.016703227283678165, val_loss: 0.017944240795546455\n",
      "Fold: 3/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.18987407678280743, val_loss: 0.09252980209298824\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.048323937166858004, val_loss: 0.03004173766576538\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03022537063888944, val_loss: 0.053613419203381786\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.0361484311022714, val_loss: 0.04488656865923028\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.025966610175377706, val_loss: 0.03304414038154248\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.021033829664239626, val_loss: 0.02690765691161352\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02791854706511163, val_loss: 0.04688895297677893\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03573599014954086, val_loss: 0.038514737799567614\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02694764823587448, val_loss: 0.02987003978341818\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.019441019135053483, val_loss: 0.026351012473337744\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02315440077718609, val_loss: 0.027869535414011853\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.030566967756226006, val_loss: 0.05181803544493098\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03571563274926392, val_loss: 0.040658799931406975\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02494924457890815, val_loss: 0.036578652743054066\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.020748121559766953, val_loss: 0.02875506301949683\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.024130205615636027, val_loss: 0.030034215532635387\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.0324262138946276, val_loss: 0.06583861653742037\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03428238077219902, val_loss: 0.04903062712401152\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.028343000271553665, val_loss: 0.04575481319701985\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.025763184598058854, val_loss: 0.04285963997244835\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.024340096684458683, val_loss: 0.03859729224227761\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.022264508230268563, val_loss: 0.031572827432108555\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.024017823161557317, val_loss: 0.03986447020188758\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.027957150238778507, val_loss: 0.05312448357673068\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.028883214756078496, val_loss: 0.043966642850519794\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.027890686864817615, val_loss: 0.04485010113076944\n",
      "epochs 27/50\n",
      "Epoch 00027: reducing learning rate of group 0 to 3.5003e-04.\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.026622092731198983, val_loss: 0.04259854349258699\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.021190193214192334, val_loss: 0.02107037553612731\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017994064163710726, val_loss: 0.023464241457220755\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01843079377227185, val_loss: 0.022548741955114037\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01888794197063697, val_loss: 0.02353108791928542\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017148925218191977, val_loss: 0.023073366248833115\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.018151940943330135, val_loss: 0.023567097855878894\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017809792009198612, val_loss: 0.023065155439421926\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01795982090110907, val_loss: 0.023886797961925988\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.018441094465455728, val_loss: 0.024758019264003162\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017188757501763144, val_loss: 0.022722125568084027\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01807049219292311, val_loss: 0.02449100508697723\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017715658651452446, val_loss: 0.02435189556011832\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01746479141640297, val_loss: 0.023888257332146168\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017655518577891615, val_loss: 0.023796630655660442\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01831859853853913, val_loss: 0.02501354010555109\n",
      "epochs 43/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01753699850380878, val_loss: 0.024976376811728665\n",
      "epochs 44/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.017908464959953307, val_loss: 0.025590222537223446\n",
      "epochs 45/50\n",
      "Epoch 00045: reducing learning rate of group 0 to 3.4490e-05.\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.017123205401411837, val_loss: 0.024825392156153134\n",
      "epochs 46/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.016882920212912978, val_loss: 0.0243132781899093\n",
      "epochs 47/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.01686943520836761, val_loss: 0.024057026070199515\n",
      "epochs 48/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.01679657912290279, val_loss: 0.024091230876939863\n",
      "epochs 49/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.016834684953975835, val_loss: 0.02383600729885266\n",
      "epochs 50/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.01725259086366318, val_loss: 0.024050725011253042\n",
      "Fold: 4/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.15572415765276865, val_loss: 0.02642002934589982\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.042338494641865655, val_loss: 0.029422855617380457\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.04716641604471462, val_loss: 0.0545761573716606\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.04803658337761207, val_loss: 0.04852808230115395\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.0430720036873888, val_loss: 0.04069233805823483\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03670297011405254, val_loss: 0.03299281498613326\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.032016293444413396, val_loss: 0.026011773262565072\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03138500566159277, val_loss: 0.028061250935455685\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.031323716519277935, val_loss: 0.028151469795327438\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.031033677359932642, val_loss: 0.02496260039410309\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02925457985226163, val_loss: 0.02325279791349251\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.030650615254049433, val_loss: 0.023104853548207564\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.030323284990644378, val_loss: 0.022146246726869753\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.027948891256911385, val_loss: 0.021995105866440816\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02739387836252143, val_loss: 0.02259117880787112\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.029533094061710136, val_loss: 0.02213562050785281\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.028046486701657016, val_loss: 0.0230318481764315\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.031247956748733197, val_loss: 0.023685353164757162\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.025018253440257945, val_loss: 0.022478406905735795\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.030116696243088593, val_loss: 0.024532544426620007\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.025228666212091123, val_loss: 0.022835961248921722\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.028012367552085044, val_loss: 0.02331055252273616\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02553423135663922, val_loss: 0.02271958151342053\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026317693259395463, val_loss: 0.023680121533719723\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026460693652839644, val_loss: 0.026121881196128304\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02537466452371231, val_loss: 0.023122858213211753\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026094625613615408, val_loss: 0.026091590321524756\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02491142107541428, val_loss: 0.02780099979626309\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026392031364498268, val_loss: 0.027528294539814323\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.026241007749616217, val_loss: 0.024582404508500508\n",
      "epochs 31/50\n",
      "Epoch 00031: reducing learning rate of group 0 to 3.5003e-04.\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02553852053671634, val_loss: 0.025423206903628613\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.022960360881896985, val_loss: 0.027827514145572326\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02133341860271206, val_loss: 0.029766622851112562\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02057683027324904, val_loss: 0.029255372643666833\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02017009085791774, val_loss: 0.029507793265541916\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.019970364963611292, val_loss: 0.028988278716018324\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.020348952459722857, val_loss: 0.029322976552832284\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.020139086407940147, val_loss: 0.029409630775549693\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.019800440305660135, val_loss: 0.029052923182270637\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.019985303408343737, val_loss: 0.029594986350275576\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.01953528703980785, val_loss: 0.030167683315659433\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.020023475586448944, val_loss: 0.027838831265015823\n",
      "epochs 43/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.019359716635479248, val_loss: 0.027382590746703118\n",
      "epochs 44/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.018640123051131684, val_loss: 0.02815820014496383\n",
      "epochs 45/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.019104931664371275, val_loss: 0.027018898736538465\n",
      "epochs 46/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.019307985165046136, val_loss: 0.026038812546941795\n",
      "epochs 47/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.018411052105396886, val_loss: 0.02785552712770081\n",
      "epochs 48/50\n",
      "Epoch 00048: reducing learning rate of group 0 to 3.4490e-05.\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.018643315150831386, val_loss: 0.026908682506090326\n",
      "epochs 49/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.018843595978595612, val_loss: 0.029052146122251685\n",
      "epochs 50/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.018933867286633405, val_loss: 0.03017649776302278\n",
      "Fold: 5/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.12749594871052786, val_loss: 0.03467026078387311\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.036097580472644615, val_loss: 0.03313150345102737\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03140270395057374, val_loss: 0.030241297754017932\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03229570186721455, val_loss: 0.03284893052554444\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.028676730320837937, val_loss: 0.03445753072829623\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.0305377344135195, val_loss: 0.029530975183374004\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03092439443632764, val_loss: 0.028019657383035673\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03243015836801772, val_loss: 0.03026405258692409\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03142759829122377, val_loss: 0.029832795213319753\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03111193936661278, val_loss: 0.03011313075885961\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.032275719160067017, val_loss: 0.02917206804513147\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03265484458717861, val_loss: 0.02917876003898288\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.0336642437136585, val_loss: 0.028764946974421803\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.031848201786756125, val_loss: 0.02825908284438284\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.032514322451070735, val_loss: 0.02825243386292928\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03136214827751055, val_loss: 0.02839000410351314\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03179135803059724, val_loss: 0.028474526994518544\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.028701151919698246, val_loss: 0.028629523873525232\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03282071012930062, val_loss: 0.03006416674409258\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03163278495990916, val_loss: 0.02843584779551939\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.032302549072100145, val_loss: 0.031680999225691744\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.02892353331523114, val_loss: 0.03023185492738297\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.0035522969805651663\n",
      "train_loss: 0.03449295195995977, val_loss: 0.028881933417563375\n",
      "epochs 24/50\n",
      "Epoch 00024: reducing learning rate of group 0 to 3.5003e-04.\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.030511939673880606, val_loss: 0.029642147477716208\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.025126064001386495, val_loss: 0.04020813313361846\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02324628377406809, val_loss: 0.03844375021167492\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02204647080629672, val_loss: 0.0374999940885525\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.021614928493325254, val_loss: 0.03809634743160323\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02090853335041749, val_loss: 0.03814780305286771\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.0213465950537571, val_loss: 0.03705892895691489\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02179207405884211, val_loss: 0.03522677006396024\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02140977220658801, val_loss: 0.03647514175329553\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02166958114524421, val_loss: 0.03587892573130758\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.021955181144815133, val_loss: 0.03491021626579918\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.021437816407629533, val_loss: 0.03507870467575757\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.021119218402983326, val_loss: 0.03451288079744891\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.020748535577992076, val_loss: 0.033790323319599816\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.02106745540360479, val_loss: 0.03347290564622534\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.021830759316339696, val_loss: 0.0333011114087544\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.00035002863808094854\n",
      "train_loss: 0.021545313053617354, val_loss: 0.03353790755040551\n",
      "epochs 41/50\n",
      "Epoch 00041: reducing learning rate of group 0 to 3.4490e-05.\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.02126366078951641, val_loss: 0.03369711224283827\n",
      "epochs 42/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.01963203851942365, val_loss: 0.036379462647202765\n",
      "epochs 43/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.019611518497970937, val_loss: 0.0374459355677429\n",
      "epochs 44/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.020205354797163684, val_loss: 0.037559230542300565\n",
      "epochs 45/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.020029481245498908, val_loss: 0.037621615577097\n",
      "epochs 46/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.020492121321149172, val_loss: 0.03719391249806473\n",
      "epochs 47/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.020181181553849264, val_loss: 0.03656469817322336\n",
      "epochs 48/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.019623112084196977, val_loss: 0.03656332385971358\n",
      "epochs 49/50\n",
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.020117705347212522, val_loss: 0.036799191031605005\n",
      "epochs 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:04,396] Trial 10 finished with value: 0.030260560545016475 and parameters: {'batch_size': 22, 'epochs': 50, 'hidden_size': 483, 'layer_size': 1, 'learning_rate': 0.0035522969805651663, 'dropout_prob': 0.3821932761665463, 'weight_decay': 0.00855939697448704, 'lr_step_size': 3, 'gamma': 0.09853586003534519}. Best is trial 6 with value: 0.027111990626374455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 3.4490372890306845e-05\n",
      "train_loss: 0.020287308861550533, val_loss: 0.036929080360814145\n",
      "Mean validation loss: 0.030260560545016475\n",
      "Fold: 1/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.2324972019383782, val_loss: 0.0440570916980505\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.06630638222161092, val_loss: 0.1188713257250033\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.03761145149014498, val_loss: 0.03233845375086132\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.030158024575365216, val_loss: 0.049353396618052534\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.024634040205886488, val_loss: 0.025052527553941075\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.021385230957285353, val_loss: 0.029417049531873903\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02103198363788818, val_loss: 0.021304678122856115\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01959318770585876, val_loss: 0.020708739904588776\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017376714288011977, val_loss: 0.023765887721980874\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018366434366295214, val_loss: 0.019745152247579473\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01926521844181575, val_loss: 0.022318910562286253\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01947631478603733, val_loss: 0.020185247651840512\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019167012584052588, val_loss: 0.02314681883313154\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017121330099670512, val_loss: 0.01939288193458005\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016916002637069476, val_loss: 0.018654768355190754\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01766951967913069, val_loss: 0.02010963744434871\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019354139993849554, val_loss: 0.02080273383149975\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018873385447812707, val_loss: 0.01937386785682879\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019099624435368338, val_loss: 0.019004281561233495\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017594514022532263, val_loss: 0.02084127563591066\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016674950228709923, val_loss: 0.02007204951032212\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017301502637565136, val_loss: 0.02062757073068305\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01901696391991879, val_loss: 0.019094961499305146\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018725691142639046, val_loss: 0.026273374298685474\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02317791343911698, val_loss: 0.020216724590251322\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020361065080291348, val_loss: 0.02402960861984052\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018594326770031137, val_loss: 0.019729448972563995\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.015514188838240347, val_loss: 0.018042657965500104\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.015835325209129798, val_loss: 0.01820500288158655\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017214015077211355, val_loss: 0.02065035921374434\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016723867873416134, val_loss: 0.023646905332019453\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020619707770253484, val_loss: 0.022104828373381968\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.023071766879997756, val_loss: 0.01940790431475953\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02167364445171858, val_loss: 0.027313482702562685\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01985656195565274, val_loss: 0.029236600979378347\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.021819209287825384, val_loss: 0.018896421515627912\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01993050786519521, val_loss: 0.024581303967064934\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.021699464291726287, val_loss: 0.023279582101263498\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017927292146180804, val_loss: 0.021610988353035952\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016006374167964646, val_loss: 0.018220667678274606\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01625700008222147, val_loss: 0.019587347038874502\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01766396573695697, val_loss: 0.020735042561825952\n",
      "epochs 43/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019269328074235665, val_loss: 0.019363095003523324\n",
      "epochs 44/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019437148480823164, val_loss: 0.018643025750000226\n",
      "epochs 45/50\n",
      "Epoch 00045: reducing learning rate of group 0 to 2.7068e-04.\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.01766475236141368, val_loss: 0.019326942727753992\n",
      "epochs 46/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.016843759234210377, val_loss: 0.018234966273762677\n",
      "epochs 47/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.016624292040145712, val_loss: 0.0182518092425246\n",
      "epochs 48/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.015310336345512616, val_loss: 0.018582264696689027\n",
      "epochs 49/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.016456996090710163, val_loss: 0.018677896770991777\n",
      "epochs 50/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.015055735901880422, val_loss: 0.018357836494320316\n",
      "Fold: 2/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.14336460088624767, val_loss: 0.034764616112960015\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.031062881670598137, val_loss: 0.07795527381332297\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.030054191666606226, val_loss: 0.04561478546575496\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.022504674001155717, val_loss: 0.03855976452560801\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019811291541708142, val_loss: 0.029055429426463025\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019173150367446636, val_loss: 0.030122217575186176\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017425583822554665, val_loss: 0.019495845968393904\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01789734265031783, val_loss: 0.025587674132303187\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01824803669985972, val_loss: 0.02834179321009862\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018497643526643515, val_loss: 0.02248352746430196\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017061069656751658, val_loss: 0.022809569919972045\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019266283286637383, val_loss: 0.043131466758878606\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.024302681957028414, val_loss: 0.03695306809324967\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020477078487410358, val_loss: 0.016795393790265446\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018636730989735378, val_loss: 0.04320342779943818\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.022800228460446784, val_loss: 0.02133396375728281\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01727557937173467, val_loss: 0.018597342368019253\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019401951447913523, val_loss: 0.03025778510460728\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02570634396550687, val_loss: 0.03100789191299363\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01985958363174608, val_loss: 0.019098311172504174\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017496472886322362, val_loss: 0.02099596360992444\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019070856346699753, val_loss: 0.029423377525649573\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.022307415247747774, val_loss: 0.03409934416413307\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01927844449681671, val_loss: 0.016730058742196935\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01697219116613269, val_loss: 0.023284236214270716\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02141442591030347, val_loss: 0.04306715236682641\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020708981901407242, val_loss: 0.01773907307927546\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016168764375738408, val_loss: 0.019236437535207523\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020472222743065732, val_loss: 0.045714366984994786\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.0216661655393086, val_loss: 0.018334316972054933\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.017262268934006755, val_loss: 0.025271994797022718\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01986661935715299, val_loss: 0.031053954049160604\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018877803642106682, val_loss: 0.019235525084169286\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016146545028804166, val_loss: 0.01650001878212941\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018110173117173344, val_loss: 0.036435691443713086\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.024793918587659534, val_loss: 0.04096051932949769\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020985785553133803, val_loss: 0.01917908796550412\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01839871020791562, val_loss: 0.03315819456781212\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02054196515267617, val_loss: 0.03086249491101817\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018595207433559392, val_loss: 0.016265519576049166\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01718107655056213, val_loss: 0.02108259920619036\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02331704077752013, val_loss: 0.040686445996949545\n",
      "epochs 43/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020933754251975762, val_loss: 0.017984049786862574\n",
      "epochs 44/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016445777329959367, val_loss: 0.030108000102796052\n",
      "epochs 45/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.021516290125682166, val_loss: 0.028481033287550275\n",
      "epochs 46/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018771629682496974, val_loss: 0.01750849270702977\n",
      "epochs 47/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.016679974358626885, val_loss: 0.016680566888106495\n",
      "epochs 48/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019588900068284648, val_loss: 0.045446032168049565\n",
      "epochs 49/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.025275932649444593, val_loss: 0.017844328950894505\n",
      "epochs 50/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01695679218851422, val_loss: 0.023113963782395188\n",
      "Fold: 3/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.13832799609946578, val_loss: 0.03522952136240507\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.04003207466418022, val_loss: 0.04067947205744291\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.03174126459481685, val_loss: 0.04105280790674059\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.025026679823273106, val_loss: 0.03856354932251729\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.023929567468401632, val_loss: 0.042169074969072094\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02347126438919651, val_loss: 0.038138920441269875\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.021982716249400062, val_loss: 0.03915396852320746\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020791671998602778, val_loss: 0.034861054614578425\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02469586046706689, val_loss: 0.0491901387117411\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.027491558980392784, val_loss: 0.0427480822331027\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.024132537753566316, val_loss: 0.03322687985277489\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018947464487466373, val_loss: 0.030013476498425007\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01946853772786103, val_loss: 0.028154588843646803\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.023256781952161538, val_loss: 0.03576576386235262\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.025449230874839582, val_loss: 0.044806676475625286\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.025470572327704805, val_loss: 0.038358074171762714\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.023611130585011682, val_loss: 0.03842409567809418\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02053362484041013, val_loss: 0.03769948931508943\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.021544622428911298, val_loss: 0.03552958133973574\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.023805364623273675, val_loss: 0.04248501399629995\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.023635501336110264, val_loss: 0.04778435140063888\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.024986963277976764, val_loss: 0.046874000249724636\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.024282789151919514, val_loss: 0.03233283885607594\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02031402809447364, val_loss: 0.030744070579346857\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01928291021307048, val_loss: 0.03067405678723988\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019970851582720092, val_loss: 0.02453116766226135\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.024642976834193656, val_loss: 0.0427726758153815\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.030526036788758478, val_loss: 0.03563228499536451\n",
      "epochs 29/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.022380150021298936, val_loss: 0.03009105297295671\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019173234676648127, val_loss: 0.029164799047928108\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.020099080241236248, val_loss: 0.028421084053422276\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.022894953941240123, val_loss: 0.034527646426699664\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.028162155594480664, val_loss: 0.05849306216757549\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02801566208271604, val_loss: 0.047474276470510585\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02395476592018416, val_loss: 0.05028671908535456\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.023729301743993635, val_loss: 0.040819369109445496\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.01968820741106021, val_loss: 0.030504832044243813\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.018468294733841168, val_loss: 0.02919920710356612\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.019318531831040195, val_loss: 0.02481191286719159\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.022541377399312824, val_loss: 0.036650755050543105\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.027728334069252014, val_loss: 0.04873424327295078\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02791404572168463, val_loss: 0.04270191020087192\n",
      "epochs 43/50\n",
      "Epoch 00043: reducing learning rate of group 0 to 2.7068e-04.\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.02934378941886519, val_loss: 0.038851114284051094\n",
      "epochs 44/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.021969605041177648, val_loss: 0.02110178508844815\n",
      "epochs 45/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.018843537098483035, val_loss: 0.0224643183234883\n",
      "epochs 46/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.018516141087993196, val_loss: 0.0220549711712489\n",
      "epochs 47/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.018475909888940423, val_loss: 0.02253400946134015\n",
      "epochs 48/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.017517889800824617, val_loss: 0.0227485254504963\n",
      "epochs 49/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.017829884284813153, val_loss: 0.02300472856548272\n",
      "epochs 50/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.01747898331010028, val_loss: 0.024011815800086447\n",
      "Fold: 4/6\n",
      "epochs 1/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.09441918521923454, val_loss: 0.02662200407174073\n",
      "epochs 2/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.05108158975398462, val_loss: 0.04092800244688988\n",
      "epochs 3/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.03797927593787838, val_loss: 0.024260218647357664\n",
      "epochs 4/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.027239384472762283, val_loss: 0.027896153495499964\n",
      "epochs 5/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.03284193220009145, val_loss: 0.037712285785298595\n",
      "epochs 6/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.03561403896463545, val_loss: 0.03089083976259357\n",
      "epochs 7/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.030572676344921713, val_loss: 0.027202434494699303\n",
      "epochs 8/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.029678779433628445, val_loss: 0.02713649063126037\n",
      "epochs 9/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.028063698144825667, val_loss: 0.026278246586260042\n",
      "epochs 10/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.028874210621181288, val_loss: 0.023747022116654797\n",
      "epochs 11/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.030049622218173585, val_loss: 0.02465647797247297\n",
      "epochs 12/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02867237012833357, val_loss: 0.021774282727978732\n",
      "epochs 13/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.026940976502373815, val_loss: 0.024440534755979713\n",
      "epochs 14/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.029842106539658027, val_loss: 0.02190693492363942\n",
      "epochs 15/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02632782009650806, val_loss: 0.02199980949884967\n",
      "epochs 16/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02883523543928995, val_loss: 0.022312940147362258\n",
      "epochs 17/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.027094735702695816, val_loss: 0.02203960530459881\n",
      "epochs 18/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02595107884783494, val_loss: 0.022125301882624626\n",
      "epochs 19/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.027736499219348554, val_loss: 0.023294444759621433\n",
      "epochs 20/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.026515050055949313, val_loss: 0.022742644246471554\n",
      "epochs 21/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.027264273749958528, val_loss: 0.025132347412995602\n",
      "epochs 22/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.027291808767538322, val_loss: 0.02344749769882152\n",
      "epochs 23/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02600035690212328, val_loss: 0.02232264820486307\n",
      "epochs 24/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.026024537779831963, val_loss: 0.023387886779872996\n",
      "epochs 25/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.025387605462272308, val_loss: 0.022304566773144824\n",
      "epochs 26/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02748101867588335, val_loss: 0.026356066846729892\n",
      "epochs 27/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.02551386086270213, val_loss: 0.02355494536459446\n",
      "epochs 28/50\n",
      "Current Learning Rate: 0.002706950839792254\n",
      "train_loss: 0.025770320213939015, val_loss: 0.02244463214944852\n",
      "epochs 29/50\n",
      "Epoch 00029: reducing learning rate of group 0 to 2.7068e-04.\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.026199397851565952, val_loss: 0.028691714997158238\n",
      "epochs 30/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.022204484649639773, val_loss: 0.029374011865767994\n",
      "epochs 31/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.021596261428816144, val_loss: 0.03024141456147558\n",
      "epochs 32/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.021609017037247356, val_loss: 0.028613641931626358\n",
      "epochs 33/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.02074314379684725, val_loss: 0.030598204337844725\n",
      "epochs 34/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.020480953564401716, val_loss: 0.028344262636413698\n",
      "epochs 35/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.02007183164211088, val_loss: 0.02958608769174469\n",
      "epochs 36/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.0207103000475878, val_loss: 0.02871000997133945\n",
      "epochs 37/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.02029568965448753, val_loss: 0.029099793565508566\n",
      "epochs 38/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.019921719660296253, val_loss: 0.028641263635731058\n",
      "epochs 39/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.019786407647188753, val_loss: 0.02884096191509774\n",
      "epochs 40/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.01977808261902905, val_loss: 0.028569786809384823\n",
      "epochs 41/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.019836650511838104, val_loss: 0.02856700999760314\n",
      "epochs 42/50\n",
      "Current Learning Rate: 0.00027067749007463456\n",
      "train_loss: 0.01955700875872648, val_loss: 0.02863965859930766\n",
      "epochs 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:21,390] Trial 11 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/6\n",
      "epochs 1/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.44815462023804065, val_loss: 0.062334946797866574\n",
      "epochs 2/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.09617220115308699, val_loss: 0.1807998080590838\n",
      "epochs 3/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.046961695544029534, val_loss: 0.0380129610972577\n",
      "epochs 4/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.040806196654509554, val_loss: 0.06899328250437975\n",
      "epochs 5/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02805354632437229, val_loss: 0.030458849217546612\n",
      "epochs 6/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.031007086220932633, val_loss: 0.0448083192428672\n",
      "epochs 7/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.0236488522373532, val_loss: 0.02545752980452227\n",
      "epochs 8/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.023126151437233938, val_loss: 0.022174294278221696\n",
      "epochs 9/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.027163105502136443, val_loss: 0.02442871993652692\n",
      "epochs 10/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.027727867795252485, val_loss: 0.034128339569035326\n",
      "epochs 11/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.022077008383348584, val_loss: 0.0256007889555277\n",
      "epochs 12/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.01642013720457295, val_loss: 0.020094062397746665\n",
      "epochs 13/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.020903908696613814, val_loss: 0.01951026183700091\n",
      "epochs 14/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02522422383098226, val_loss: 0.02263553314352114\n",
      "epochs 15/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.026633583891548608, val_loss: 0.034636312335925665\n",
      "epochs 16/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.022180740991117137, val_loss: 0.022726272894559724\n",
      "epochs 17/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.017450408691442328, val_loss: 0.01956582718871926\n",
      "epochs 18/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.01700616143888941, val_loss: 0.020845155133620688\n",
      "epochs 19/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.025055905883690638, val_loss: 0.020063513485518724\n",
      "epochs 20/53\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.8827e-04.\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.027145610460521358, val_loss: 0.039076706394553185\n",
      "epochs 21/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.02285776334496117, val_loss: 0.024905640313303785\n",
      "epochs 22/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01825628656354782, val_loss: 0.020518857189209053\n",
      "epochs 23/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.018268113833312925, val_loss: 0.020317198537094027\n",
      "epochs 24/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.017532402196114783, val_loss: 0.02130578514678698\n",
      "epochs 25/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.017414048350857275, val_loss: 0.021702007966508206\n",
      "epochs 26/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.016841274018301385, val_loss: 0.020067998183597075\n",
      "epochs 27/53\n",
      "Epoch 00027: reducing learning rate of group 0 to 3.8415e-05.\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.016877023380641874, val_loss: 0.019586866721510887\n",
      "epochs 28/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.01676035976944197, val_loss: 0.01959041285475618\n",
      "epochs 29/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.017310632368255603, val_loss: 0.01962155394109064\n",
      "epochs 30/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.017119765826991123, val_loss: 0.019654390965833476\n",
      "epochs 31/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.01677239609661659, val_loss: 0.019687038331635688\n",
      "epochs 32/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.015593153203984625, val_loss: 0.019700042915677552\n",
      "epochs 33/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.017161837252052992, val_loss: 0.019704158552677223\n",
      "epochs 34/53\n",
      "Epoch 00034: reducing learning rate of group 0 to 3.8008e-06.\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.0162935368226547, val_loss: 0.019735363679692933\n",
      "epochs 35/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.01680590267489223, val_loss: 0.01973660188531013\n",
      "epochs 36/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.016541096314444746, val_loss: 0.019736861381189602\n",
      "epochs 37/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.017631974003865923, val_loss: 0.019737349406472947\n",
      "epochs 38/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.01691081036053794, val_loss: 0.019737746064110023\n",
      "epochs 39/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.01618160535336325, val_loss: 0.019736996239149256\n",
      "epochs 40/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.0167485415616906, val_loss: 0.01973559827494778\n",
      "epochs 41/53\n",
      "Epoch 00041: reducing learning rate of group 0 to 3.7605e-07.\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016653089128483674, val_loss: 0.019732742190459056\n",
      "epochs 42/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016731734013822126, val_loss: 0.019732635811363395\n",
      "epochs 43/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.017513387634320872, val_loss: 0.01973234283688821\n",
      "epochs 44/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.017395597858060347, val_loss: 0.019731850583890553\n",
      "epochs 45/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016628293663655456, val_loss: 0.01973156056268827\n",
      "epochs 46/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016165020232284933, val_loss: 0.01973132470524625\n",
      "epochs 47/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016912170612302265, val_loss: 0.019731130144607863\n",
      "epochs 48/53\n",
      "Epoch 00048: reducing learning rate of group 0 to 3.7206e-08.\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016447619620808644, val_loss: 0.019730824658549147\n",
      "epochs 49/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016666542812201538, val_loss: 0.019730801951434267\n",
      "epochs 50/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.017151415911748222, val_loss: 0.01973078145008338\n",
      "epochs 51/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016628699879603165, val_loss: 0.0197307886310706\n",
      "epochs 52/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.01669735478016695, val_loss: 0.019730803605757262\n",
      "epochs 53/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016464931436961417, val_loss: 0.01973080802953949\n",
      "Fold: 2/6\n",
      "epochs 1/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.27824020715381365, val_loss: 0.035670506743420115\n",
      "epochs 2/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.047067949425821244, val_loss: 0.033276143777919445\n",
      "epochs 3/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.04315500461349362, val_loss: 0.028292939164920858\n",
      "epochs 4/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.03737132762264656, val_loss: 0.027428001753593747\n",
      "epochs 5/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.03294590067142915, val_loss: 0.03016236730802216\n",
      "epochs 6/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.029290883955055552, val_loss: 0.037176775549979585\n",
      "epochs 7/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02509015273780709, val_loss: 0.03841847816090051\n",
      "epochs 8/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.022491696260036213, val_loss: 0.03758274768724253\n",
      "epochs 9/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.019708954196080174, val_loss: 0.02698319598934368\n",
      "epochs 10/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.0181403052502949, val_loss: 0.02574995484840321\n",
      "epochs 11/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02052036756772156, val_loss: 0.03989210469942344\n",
      "epochs 12/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.019285362232231387, val_loss: 0.02502665765534498\n",
      "epochs 13/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.017880337545648217, val_loss: 0.02127026048439898\n",
      "epochs 14/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.025710526149180766, val_loss: 0.06789968329432763\n",
      "epochs 15/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02654324226559287, val_loss: 0.020697499760181495\n",
      "epochs 16/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.017924889885379297, val_loss: 0.027410481174133326\n",
      "epochs 17/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.022015531357426784, val_loss: 0.04775725089405712\n",
      "epochs 18/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02150175844527487, val_loss: 0.024411077071961603\n",
      "epochs 19/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.01651876621977671, val_loss: 0.01970648838476719\n",
      "epochs 20/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.0174374578790249, val_loss: 0.02099635509627038\n",
      "epochs 21/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.0260732186419007, val_loss: 0.06759548363716979\n",
      "epochs 22/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02485950723118884, val_loss: 0.02305047440734741\n",
      "epochs 23/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.01757111042541893, val_loss: 0.026814743570101104\n",
      "epochs 24/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.019275789565749858, val_loss: 0.024972495512644712\n",
      "epochs 25/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02653379988317427, val_loss: 0.04973027659089942\n",
      "epochs 26/53\n",
      "Epoch 00026: reducing learning rate of group 0 to 3.8827e-04.\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.02314886648060852, val_loss: 0.020586243315942977\n",
      "epochs 27/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.017614814988950168, val_loss: 0.017547255633153805\n",
      "epochs 28/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.016431681719902707, val_loss: 0.01921243013470973\n",
      "epochs 29/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.0167831237011246, val_loss: 0.01762787313935788\n",
      "epochs 30/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.0161009741652953, val_loss: 0.017727676680997798\n",
      "epochs 31/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.016052886476054004, val_loss: 0.01824313645422655\n",
      "epochs 32/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.016799044341927295, val_loss: 0.01742493829346801\n",
      "epochs 33/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.0156792794005014, val_loss: 0.01841793107604118\n",
      "epochs 34/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01615020342011887, val_loss: 0.0174949203225735\n",
      "epochs 35/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.016216099789170057, val_loss: 0.018173627837217952\n",
      "epochs 36/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.016535090910589422, val_loss: 0.01715341985446254\n",
      "epochs 37/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01604102974708535, val_loss: 0.018481033884822147\n",
      "epochs 38/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01720397252785532, val_loss: 0.01730877417139709\n",
      "epochs 39/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01633160137939022, val_loss: 0.017687592159123404\n",
      "epochs 40/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01610170647281369, val_loss: 0.017486739793400232\n",
      "epochs 41/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.016288332500200915, val_loss: 0.01803325236725964\n",
      "epochs 42/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.015387497802485564, val_loss: 0.01770534484622706\n",
      "epochs 43/53\n",
      "Epoch 00043: reducing learning rate of group 0 to 3.8415e-05.\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.015807484120351115, val_loss: 0.017562061162224335\n",
      "epochs 44/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.015758959175161038, val_loss: 0.017481256325386073\n",
      "epochs 45/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.015822130457596166, val_loss: 0.017462452830697753\n",
      "epochs 46/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.014829205551282748, val_loss: 0.01741330765530859\n",
      "epochs 47/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.015793768018116487, val_loss: 0.01741484880422879\n",
      "epochs 48/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.01509146555326879, val_loss: 0.017436588323969198\n",
      "epochs 49/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.015432647460042253, val_loss: 0.01741902998305465\n",
      "epochs 50/53\n",
      "Epoch 00050: reducing learning rate of group 0 to 3.8008e-06.\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.015923366287576134, val_loss: 0.01749197928553545\n",
      "epochs 51/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.015472493400968807, val_loss: 0.017496745089853282\n",
      "epochs 52/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.015642121299415043, val_loss: 0.017502900053697982\n",
      "epochs 53/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.015987981530519103, val_loss: 0.017506484977753933\n",
      "Fold: 3/6\n",
      "epochs 1/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.2494421148705378, val_loss: 0.09211148914733999\n",
      "epochs 2/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.05203407731830355, val_loss: 0.030895298077283723\n",
      "epochs 3/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.031146551216870808, val_loss: 0.054956448215403055\n",
      "epochs 4/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.04027563981556644, val_loss: 0.05974320219339509\n",
      "epochs 5/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.032985862712130734, val_loss: 0.03206560058568261\n",
      "epochs 6/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02251892207443649, val_loss: 0.02929262202968331\n",
      "epochs 7/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.026409728856953352, val_loss: 0.04480062581990894\n",
      "epochs 8/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.03445339785365943, val_loss: 0.04999854498983998\n",
      "epochs 9/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.027271526961652727, val_loss: 0.04224899943035684\n",
      "epochs 10/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02430305591776248, val_loss: 0.04014976011393102\n",
      "epochs 11/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.023745469391558385, val_loss: 0.03821833847139619\n",
      "epochs 12/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.026279246009755553, val_loss: 0.04461477405244583\n",
      "epochs 13/53\n",
      "Epoch 00013: reducing learning rate of group 0 to 3.8827e-04.\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.0282634214882069, val_loss: 0.04814659505102195\n",
      "epochs 14/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.021154563204107576, val_loss: 0.021208788427573285\n",
      "epochs 15/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.017959535444612828, val_loss: 0.024379955359587546\n",
      "epochs 16/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01811092938990904, val_loss: 0.022801814367994666\n",
      "epochs 17/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.017788649448847168, val_loss: 0.023331030296455873\n",
      "epochs 18/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01776981788070611, val_loss: 0.023927388939467307\n",
      "epochs 19/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.017645860010178007, val_loss: 0.023196874094489766\n",
      "epochs 20/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.017279644020865754, val_loss: 0.02398330223820123\n",
      "epochs 21/53\n",
      "Epoch 00021: reducing learning rate of group 0 to 3.8415e-05.\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.017337873634236882, val_loss: 0.024044548731150205\n",
      "epochs 22/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.017622685258015337, val_loss: 0.023581131343043557\n",
      "epochs 23/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.016628201078754245, val_loss: 0.02366276679707593\n",
      "epochs 24/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.0168847830949776, val_loss: 0.023572311663117847\n",
      "epochs 25/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.01704669471936333, val_loss: 0.02354597866437153\n",
      "epochs 26/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.016250786839232763, val_loss: 0.023524591993344456\n",
      "epochs 27/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.01681485071087975, val_loss: 0.023479935553807178\n",
      "epochs 28/53\n",
      "Epoch 00028: reducing learning rate of group 0 to 3.8008e-06.\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.01664301654397461, val_loss: 0.02346056937485149\n",
      "epochs 29/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.016938940235903782, val_loss: 0.023459980650314766\n",
      "epochs 30/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.01724504606120223, val_loss: 0.023466616917989756\n",
      "epochs 31/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.016994238993471588, val_loss: 0.023481992410348828\n",
      "epochs 32/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.01692862196400631, val_loss: 0.02349151994444822\n",
      "epochs 33/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.017038709817968897, val_loss: 0.023481291032543306\n",
      "epochs 34/53\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.016843281477983845, val_loss: 0.02347086199044593\n",
      "epochs 35/53\n",
      "Epoch 00035: reducing learning rate of group 0 to 3.7605e-07.\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016775144331500326, val_loss: 0.023487078957259655\n",
      "epochs 36/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.01723689843263281, val_loss: 0.023485687150815993\n",
      "epochs 37/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016194756553312272, val_loss: 0.023485971749515125\n",
      "epochs 38/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016627794440035103, val_loss: 0.023485503190099018\n",
      "epochs 39/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.017037247017581473, val_loss: 0.023484458350331375\n",
      "epochs 40/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016448533756805484, val_loss: 0.023483351050680012\n",
      "epochs 41/53\n",
      "Current Learning Rate: 3.760459809357447e-07\n",
      "train_loss: 0.016912260058074537, val_loss: 0.023486305285539282\n",
      "epochs 42/53\n",
      "Epoch 00042: reducing learning rate of group 0 to 3.7206e-08.\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016701743657202377, val_loss: 0.023487740937669418\n",
      "epochs 43/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.01652736747392306, val_loss: 0.023487802901256242\n",
      "epochs 44/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016692545830696952, val_loss: 0.023487635110014754\n",
      "epochs 45/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016812422916317654, val_loss: 0.023487743829671098\n",
      "epochs 46/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016791984040633236, val_loss: 0.023487925076072936\n",
      "epochs 47/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.016564351868031447, val_loss: 0.023487618291064313\n",
      "epochs 48/53\n",
      "Current Learning Rate: 3.720585289967061e-08\n",
      "train_loss: 0.01683935808148562, val_loss: 0.023487557540647686\n",
      "epochs 49/53\n",
      "Epoch 00049: reducing learning rate of group 0 to 3.6811e-09.\n",
      "Current Learning Rate: 3.681133585173086e-09\n",
      "train_loss: 0.017781759430490957, val_loss: 0.02348766328864976\n",
      "epochs 50/53\n",
      "Current Learning Rate: 3.681133585173086e-09\n",
      "train_loss: 0.017664892272206776, val_loss: 0.023487651837058365\n",
      "epochs 51/53\n",
      "Current Learning Rate: 3.681133585173086e-09\n",
      "train_loss: 0.01736088953462936, val_loss: 0.023487638620855778\n",
      "epochs 52/53\n",
      "Current Learning Rate: 3.681133585173086e-09\n",
      "train_loss: 0.017101305965824348, val_loss: 0.023487642836315853\n",
      "epochs 53/53\n",
      "Current Learning Rate: 3.681133585173086e-09\n",
      "train_loss: 0.01721133027029665, val_loss: 0.02348764408624878\n",
      "Fold: 4/6\n",
      "epochs 1/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.19171896100497657, val_loss: 0.026093431246025783\n",
      "epochs 2/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.04554947358127193, val_loss: 0.030127388768290218\n",
      "epochs 3/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.046019183658676126, val_loss: 0.02725797854854088\n",
      "epochs 4/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.0425804813276045, val_loss: 0.03719758842826674\n",
      "epochs 5/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.041637892330886404, val_loss: 0.040891644667441905\n",
      "epochs 6/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.040105768286402485, val_loss: 0.03155530665658022\n",
      "epochs 7/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.03740903997554836, val_loss: 0.027903167800487655\n",
      "epochs 8/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.03367184951601207, val_loss: 0.025181791046634316\n",
      "epochs 9/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.03256548761610726, val_loss: 0.025316529196539993\n",
      "epochs 10/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.03312262638141156, val_loss: 0.02247265082980065\n",
      "epochs 11/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.030595263280893528, val_loss: 0.02210264762030228\n",
      "epochs 12/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.029800694457279814, val_loss: 0.02219108727417494\n",
      "epochs 13/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02810290950583294, val_loss: 0.02346418730571474\n",
      "epochs 14/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02897407106387331, val_loss: 0.022451713423881876\n",
      "epochs 15/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.028127506589762083, val_loss: 0.022690734188807637\n",
      "epochs 16/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02827484251496284, val_loss: 0.022160070029234414\n",
      "epochs 17/53\n",
      "Current Learning Rate: 0.003924277392847092\n",
      "train_loss: 0.02922097274476025, val_loss: 0.022833426801585836\n",
      "epochs 18/53\n",
      "Epoch 00018: reducing learning rate of group 0 to 3.8827e-04.\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.027214239524542598, val_loss: 0.02244292639539038\n",
      "epochs 19/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.02267563981187873, val_loss: 0.030139954460441674\n",
      "epochs 20/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.021041844073854583, val_loss: 0.031517544902223894\n",
      "epochs 21/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.02103718182402908, val_loss: 0.02908478177912337\n",
      "epochs 22/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.02008173053802334, val_loss: 0.029200920969934055\n",
      "epochs 23/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.01992265972474247, val_loss: 0.029902533364580257\n",
      "epochs 24/53\n",
      "Current Learning Rate: 0.00038826658126342266\n",
      "train_loss: 0.020178976029120877, val_loss: 0.029746342640321114\n",
      "epochs 25/53\n",
      "Epoch 00025: reducing learning rate of group 0 to 3.8415e-05.\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.019297827338481222, val_loss: 0.02903430219617133\n",
      "epochs 26/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.019353184415529922, val_loss: 0.0304554602204773\n",
      "epochs 27/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.019538425351799418, val_loss: 0.03138069450659187\n",
      "epochs 28/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.018711630039002846, val_loss: 0.031242522833819845\n",
      "epochs 29/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.019245691330931885, val_loss: 0.03111694696800489\n",
      "epochs 30/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.019137370230103107, val_loss: 0.031225917180755028\n",
      "epochs 31/53\n",
      "Current Learning Rate: 3.841495466165685e-05\n",
      "train_loss: 0.01942332670870727, val_loss: 0.03120833105946842\n",
      "epochs 32/53\n",
      "Epoch 00032: reducing learning rate of group 0 to 3.8008e-06.\n",
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.019015729177657415, val_loss: 0.03132539570919777\n",
      "epochs 33/53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:39,921] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 3.800761674762693e-06\n",
      "train_loss: 0.01911122493268187, val_loss: 0.031298511704479004\n",
      "epochs 34/53\n",
      "Fold: 1/6\n",
      "epochs 1/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 4.643006339669228, val_loss: 1.7432169318199158\n",
      "epochs 2/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.33907116018235683, val_loss: 0.4408363774418831\n",
      "epochs 3/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.5718695223331451, val_loss: 0.40182628482580185\n",
      "epochs 4/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.532300315797329, val_loss: 1.2015313357114792\n",
      "epochs 5/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 2.103867508471012, val_loss: 1.6405108571052551\n",
      "epochs 6/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.6883716993033886, val_loss: 0.39284637570381165\n",
      "epochs 7/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 2.753586992621422, val_loss: 1.545867383480072\n",
      "epochs 8/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.36952350847423077, val_loss: 0.38317739963531494\n",
      "epochs 9/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.7825610414147377, val_loss: 0.27190590277314186\n",
      "epochs 10/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.753754660487175, val_loss: 0.22388054803013802\n",
      "epochs 11/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.32413744553923607, val_loss: 0.1526648961007595\n",
      "epochs 12/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 3.1232311725616455, val_loss: 1.752699226140976\n",
      "epochs 13/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.5882995538413525, val_loss: 0.3371649496257305\n",
      "epochs 14/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 1.0737711414694786, val_loss: 1.2337801456451416\n",
      "epochs 15/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.968400452286005, val_loss: 0.1399446204304695\n",
      "epochs 16/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 4.294794347137213, val_loss: 0.44593704491853714\n",
      "epochs 17/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 1.7132127285003662, val_loss: 0.19600043073296547\n",
      "epochs 18/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 1.3156160786747932, val_loss: 0.0674580279737711\n",
      "epochs 19/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 2.046469137072563, val_loss: 0.5485318899154663\n",
      "epochs 20/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 1.8889241367578506, val_loss: 0.2521192654967308\n",
      "epochs 21/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 1.254294015467167, val_loss: 0.10724371485412121\n",
      "epochs 22/341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:41,440] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 1.0846241042017937, val_loss: 0.49444014579057693\n",
      "epochs 23/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.7838925532996655, val_loss: 0.6309393346309662\n",
      "epochs 24/341\n",
      "Current Learning Rate: 0.022648836007613225\n",
      "train_loss: 0.7484219931066036, val_loss: 0.2230118364095688\n",
      "epochs 25/341\n",
      "Fold: 1/6\n",
      "epochs 1/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 33.98466523021067, val_loss: 26.205520956139814\n",
      "epochs 2/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 9.12381706818154, val_loss: 13.619576529452676\n",
      "epochs 3/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 8.865108709586295, val_loss: 1.1065402870115482\n",
      "epochs 4/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 4.06815074148931, val_loss: 1.716805603943373\n",
      "epochs 5/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 1.4089197500755912, val_loss: 2.20302414894104\n",
      "epochs 6/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 1.3530951844234216, val_loss: 1.0122679401385157\n",
      "epochs 7/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 1.0721111352506436, val_loss: 0.05882445543906406\n",
      "epochs 8/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.46961594529842077, val_loss: 0.0667774375822199\n",
      "epochs 9/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.36150423711852026, val_loss: 0.18127882794329994\n",
      "epochs 10/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.36604452956663935, val_loss: 0.05744427937622133\n",
      "epochs 11/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.34017968471897275, val_loss: 0.05876698246911952\n",
      "epochs 12/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.3752950316196994, val_loss: 0.136787940502951\n",
      "epochs 13/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.3477996575989221, val_loss: 0.05099273487729462\n",
      "epochs 14/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.2917096203095035, val_loss: 0.06607421504725751\n",
      "epochs 15/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.28461861571199015, val_loss: 0.050666775995571366\n",
      "epochs 16/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.33311526889079496, val_loss: 3.8091294522348202\n",
      "epochs 17/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.310100837561645, val_loss: 8.224379165490207\n",
      "epochs 18/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.3113685689474407, val_loss: 4.365370875910709\n",
      "epochs 19/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.2736508469832571, val_loss: 5.600607323019128\n",
      "epochs 20/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.27324692120677546, val_loss: 0.05035123023155488\n",
      "epochs 21/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.27840283767957436, val_loss: 0.05199660463748794\n",
      "epochs 22/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.25561379935396344, val_loss: 0.09131645930832938\n",
      "epochs 23/289\n",
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.20361346535776792, val_loss: 0.0503093110593526\n",
      "epochs 24/289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:44,080] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.01542206101960752\n",
      "train_loss: 0.2720444672986081, val_loss: 0.0505968488654808\n",
      "epochs 25/289\n",
      "Fold: 1/6\n",
      "epochs 1/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 5.0437179122512275, val_loss: 0.2096669390601547\n",
      "epochs 2/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.3899046811030099, val_loss: 0.2029728953187403\n",
      "epochs 3/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.25558856913917943, val_loss: 0.0796914927563385\n",
      "epochs 4/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.8820300564954155, val_loss: 1.3299525207594822\n",
      "epochs 5/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 1.3476422978074927, val_loss: 1.4020597377890034\n",
      "epochs 6/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 1.0697244223403304, val_loss: 1.232391758184684\n",
      "epochs 7/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.7748198053358417, val_loss: 0.17583327615437538\n",
      "epochs 8/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.6746997789136673, val_loss: 0.9647303309879804\n",
      "epochs 9/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.5479224680579806, val_loss: 0.32657239948840516\n",
      "epochs 10/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.7841775359487847, val_loss: 0.6275680906286365\n",
      "epochs 11/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.5657449535241252, val_loss: 0.34420687822919144\n",
      "epochs 12/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.38149218986693184, val_loss: 0.48616992133228404\n",
      "epochs 13/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.3594022411853075, val_loss: 0.31485641247739915\n",
      "epochs 14/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.327236523386091, val_loss: 0.40462377726247434\n",
      "epochs 15/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.44701296350869696, val_loss: 0.3512844386461534\n",
      "epochs 16/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.3819520219082111, val_loss: 0.3320732694119215\n",
      "epochs 17/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.36704021697177697, val_loss: 0.26434722758437457\n",
      "epochs 18/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.2779286297920503, val_loss: 0.15350633021444082\n",
      "epochs 19/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.29349037838217457, val_loss: 0.09682806752818196\n",
      "epochs 20/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.17469005612656474, val_loss: 0.038633880695622215\n",
      "epochs 21/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.12938306277225675, val_loss: 0.050953604133897705\n",
      "epochs 22/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.05917556262820175, val_loss: 0.10488112476703368\n",
      "epochs 23/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.04670377232526478, val_loss: 0.05525271873921156\n",
      "epochs 24/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.060836953870756064, val_loss: 0.051746834878270566\n",
      "epochs 25/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.036233873878556644, val_loss: 0.041548878603957985\n",
      "epochs 26/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.030574895478294867, val_loss: 0.032328090802031126\n",
      "epochs 27/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.03445011975341722, val_loss: 0.03778645419515669\n",
      "epochs 28/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.028163173297224075, val_loss: 0.03170220634752983\n",
      "epochs 29/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.025770430783986262, val_loss: 0.02619750278168603\n",
      "epochs 30/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.02503383727008967, val_loss: 0.028264577194166026\n",
      "epochs 31/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.021547124071634914, val_loss: 0.02566699215554093\n",
      "epochs 32/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.022357062767504862, val_loss: 0.02338788755189039\n",
      "epochs 33/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.02303160556690081, val_loss: 0.024886815894493146\n",
      "epochs 34/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01963599353391481, val_loss: 0.022048763979814555\n",
      "epochs 35/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.02038418953797143, val_loss: 0.02249270726583506\n",
      "epochs 36/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01930198393222925, val_loss: 0.02169623105835758\n",
      "epochs 37/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01899235217684978, val_loss: 0.021855061849285113\n",
      "epochs 38/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01865513514923422, val_loss: 0.02129785536396268\n",
      "epochs 39/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.019753904437254135, val_loss: 0.021887238370254636\n",
      "epochs 40/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.018213548648514245, val_loss: 0.02102200700411279\n",
      "epochs 41/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.018295322704177937, val_loss: 0.020991455677798705\n",
      "epochs 42/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.018833396686731202, val_loss: 0.021102074484683965\n",
      "epochs 43/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.019732583203892175, val_loss: 0.020349119508050774\n",
      "epochs 44/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01903115703087104, val_loss: 0.020439475860544724\n",
      "epochs 45/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.018699357800773884, val_loss: 0.020023111450044734\n",
      "epochs 46/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01863037168636526, val_loss: 0.020105908134658085\n",
      "epochs 47/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01832654866340913, val_loss: 0.01982586795317107\n",
      "epochs 48/99\n",
      "Current Learning Rate: 0.01925978855889573\n",
      "train_loss: 0.01844169992316318, val_loss: 0.019814062165096402\n",
      "epochs 49/99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:46,501] Trial 15 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/6\n",
      "epochs 1/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 69.91546855214983, val_loss: 16.16906123412283\n",
      "epochs 2/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 7.139836342711198, val_loss: 1.0719472311044995\n",
      "epochs 3/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 4.207407940375178, val_loss: 1.2855397564800162\n",
      "epochs 4/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 2.844600308882563, val_loss: 0.19205542085202118\n",
      "epochs 5/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.0171080588510162, val_loss: 0.7507650609079161\n",
      "epochs 6/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.2621837007372003, val_loss: 1.8551111440909536\n",
      "epochs 7/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 2.725872284487674, val_loss: 0.23022862190478727\n",
      "epochs 8/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.2604122232449682, val_loss: 0.05961792773910259\n",
      "epochs 9/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.2073669515942271, val_loss: 0.06780258850439598\n",
      "epochs 10/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 0.8224803154405794, val_loss: 0.3003596522306141\n",
      "epochs 11/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 0.9725542966472475, val_loss: 0.09632002655416727\n",
      "epochs 12/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.1247181617899944, val_loss: 0.1044140691427808\n",
      "epochs 13/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.0089407614187191, val_loss: 0.34740109192697627\n",
      "epochs 14/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 0.9009033206262087, val_loss: 0.1007832364228211\n",
      "epochs 15/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.039327553620464, val_loss: 0.1255975580333095\n",
      "epochs 16/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 1.016941159571472, val_loss: 0.08371672867552231\n",
      "epochs 17/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 0.8182658678606937, val_loss: 0.20045953204757289\n",
      "epochs 18/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 0.9474502641119456, val_loss: 0.3459456339478493\n",
      "epochs 19/91\n",
      "Current Learning Rate: 0.035349554086512094\n",
      "train_loss: 0.8272377401590347, val_loss: 0.5368453628922764\n",
      "epochs 20/91\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.2197e-03.\n",
      "Current Learning Rate: 0.0032196514376434983\n",
      "train_loss: 0.8323782741239196, val_loss: 0.12801304783083892\n",
      "epochs 21/91\n",
      "Current Learning Rate: 0.0032196514376434983\n",
      "train_loss: 0.7634976729750633, val_loss: 0.06619984808524973\n",
      "epochs 22/91\n",
      "Current Learning Rate: 0.0032196514376434983\n",
      "train_loss: 0.7521003371006564, val_loss: 0.08949978384924562\n",
      "epochs 23/91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:48,389] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.0032196514376434983\n",
      "train_loss: 0.858069463387916, val_loss: 0.07569586005257933\n",
      "epochs 24/91\n",
      "Current Learning Rate: 0.0032196514376434983\n",
      "train_loss: 0.6377549147919604, val_loss: 0.05651290146143813\n",
      "epochs 25/91\n",
      "Fold: 1/6\n",
      "epochs 1/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.07546948474268184, val_loss: 0.04569171743251469\n",
      "epochs 2/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.03268071215011572, val_loss: 0.0391965477829217\n",
      "epochs 3/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.029267299071714086, val_loss: 0.02765293289466124\n",
      "epochs 4/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.022354193822186636, val_loss: 0.030871433365207753\n",
      "epochs 5/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.020845476974500343, val_loss: 0.022617730838981897\n",
      "epochs 6/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.019563123125791254, val_loss: 0.02160677618935312\n",
      "epochs 7/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.018366442276409975, val_loss: 0.02066428134659011\n",
      "epochs 8/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01746857377216465, val_loss: 0.02014618103490456\n",
      "epochs 9/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017544854447662243, val_loss: 0.019918792758529123\n",
      "epochs 10/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01697482188893972, val_loss: 0.019555340239189957\n",
      "epochs 11/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017051786162865984, val_loss: 0.01927460997218364\n",
      "epochs 12/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01615286338461661, val_loss: 0.018956523976827924\n",
      "epochs 13/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017224847417693173, val_loss: 0.01874536703536777\n",
      "epochs 14/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.016060689180066173, val_loss: 0.018410179800795096\n",
      "epochs 15/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01573262995158362, val_loss: 0.018780454993247986\n",
      "epochs 16/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015177639582696812, val_loss: 0.018164226295132386\n",
      "epochs 17/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015457108854254904, val_loss: 0.018095999772317316\n",
      "epochs 18/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015063808846446735, val_loss: 0.01852317301458434\n",
      "epochs 19/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015029078778369646, val_loss: 0.01813133856232621\n",
      "epochs 20/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014421233463891488, val_loss: 0.018151015459902976\n",
      "epochs 21/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014951596951026372, val_loss: 0.018122477224096656\n",
      "epochs 22/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015078315054875259, val_loss: 0.018116277642548084\n",
      "epochs 23/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015056177945409314, val_loss: 0.021969601585480728\n",
      "epochs 24/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017760689725707237, val_loss: 0.018355863438429015\n",
      "epochs 25/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01747504530450035, val_loss: 0.021975796832090343\n",
      "epochs 26/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01710946588335853, val_loss: 0.01977500978759245\n",
      "epochs 27/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015349620171228276, val_loss: 0.018583766147984487\n",
      "epochs 28/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014963075407266794, val_loss: 0.019657106702461055\n",
      "epochs 29/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015112621872073137, val_loss: 0.018639412880139917\n",
      "epochs 30/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014539194892246374, val_loss: 0.018080242762440128\n",
      "epochs 31/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01509412767719416, val_loss: 0.017947680819289463\n",
      "epochs 32/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014552815390312676, val_loss: 0.018215398892368142\n",
      "epochs 33/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014033166217957745, val_loss: 0.01797503295452579\n",
      "epochs 34/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014487173104977333, val_loss: 0.01860628519697409\n",
      "epochs 35/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015285550169401654, val_loss: 0.019385541384843618\n",
      "epochs 36/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.013976346260230792, val_loss: 0.01811245551921035\n",
      "epochs 37/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015219929119523027, val_loss: 0.020787364080254184\n",
      "epochs 38/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.016255275413737093, val_loss: 0.019117004875289768\n",
      "epochs 39/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015666956866258068, val_loss: 0.02206070608410396\n",
      "epochs 40/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015124841323612552, val_loss: 0.019368008109986\n",
      "epochs 41/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.013910109266458005, val_loss: 0.018653662483158865\n",
      "epochs 42/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014023772790862591, val_loss: 0.019710281839300143\n",
      "epochs 43/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014962333720177412, val_loss: 0.018224431263086826\n",
      "epochs 44/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015005578385745665, val_loss: 0.020659200344724876\n",
      "epochs 45/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01620998914892736, val_loss: 0.018586647745810057\n",
      "epochs 46/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014567291004871214, val_loss: 0.020140654064322774\n",
      "epochs 47/61\n",
      "Epoch 00047: reducing learning rate of group 0 to 3.2818e-05.\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.014276205850029854, val_loss: 0.019564243891325436\n",
      "epochs 48/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.014726380530766236, val_loss: 0.018718810752034187\n",
      "epochs 49/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.01363697651111962, val_loss: 0.018518558617583232\n",
      "epochs 50/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.01355151006052773, val_loss: 0.018598509033357625\n",
      "epochs 51/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013461075149519419, val_loss: 0.0184672706956534\n",
      "epochs 52/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.014101110008165338, val_loss: 0.01854188697363593\n",
      "epochs 53/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013328029349826178, val_loss: 0.018454872696709475\n",
      "epochs 54/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013769485315271212, val_loss: 0.018404195416032484\n",
      "epochs 55/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013121521575636794, val_loss: 0.018390415983862783\n",
      "epochs 56/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013579111474284315, val_loss: 0.018375755765622382\n",
      "epochs 57/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.01448720508970406, val_loss: 0.018355940100982\n",
      "epochs 58/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013241065991582888, val_loss: 0.018358921301306078\n",
      "epochs 59/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013631349885958786, val_loss: 0.018362392070948294\n",
      "epochs 60/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.01350404266811797, val_loss: 0.018331068960067472\n",
      "epochs 61/61\n",
      "Current Learning Rate: 3.281811294608486e-05\n",
      "train_loss: 0.013667868667701106, val_loss: 0.018347999068761344\n",
      "Fold: 2/6\n",
      "epochs 1/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.0810845726978426, val_loss: 0.0654270881112959\n",
      "epochs 2/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.03886160195658082, val_loss: 0.02933845137196936\n",
      "epochs 3/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.028105184285729928, val_loss: 0.024061588178339758\n",
      "epochs 4/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.024242497790653846, val_loss: 0.0213485733042226\n",
      "epochs 5/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.020683875240042414, val_loss: 0.020665806656899422\n",
      "epochs 6/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01989589850602083, val_loss: 0.01962788090550978\n",
      "epochs 7/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.019592435264616813, val_loss: 0.01819552089038648\n",
      "epochs 8/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017878673529546512, val_loss: 0.017921367745944543\n",
      "epochs 9/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01789940364879409, val_loss: 0.01849951892846117\n",
      "epochs 10/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.018137423009121495, val_loss: 0.018811308864602132\n",
      "epochs 11/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017474985914304852, val_loss: 0.021111890943230766\n",
      "epochs 12/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017919952552275437, val_loss: 0.022967272658685322\n",
      "epochs 13/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017307039944602077, val_loss: 0.022114755953417012\n",
      "epochs 14/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.018395000728656, val_loss: 0.02314915326669028\n",
      "epochs 15/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.018290034976273187, val_loss: 0.020329199204417437\n",
      "epochs 16/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01810245500497618, val_loss: 0.017479181154876164\n",
      "epochs 17/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.016804168322491216, val_loss: 0.01678817320958172\n",
      "epochs 18/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01669994063099454, val_loss: 0.016434067860245705\n",
      "epochs 19/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017016811032877548, val_loss: 0.016543768376945274\n",
      "epochs 20/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.016515689166752917, val_loss: 0.016242627471097205\n",
      "epochs 21/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017102127649674292, val_loss: 0.016847202321514487\n",
      "epochs 22/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017750224453936283, val_loss: 0.022012145184960804\n",
      "epochs 23/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.019085996931320744, val_loss: 0.029838415034311384\n",
      "epochs 24/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.0212395507474675, val_loss: 0.022296985537794075\n",
      "epochs 25/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.020403450616868213, val_loss: 0.01702338507349946\n",
      "epochs 26/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.018535741688825318, val_loss: 0.01645933981298616\n",
      "epochs 27/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01593169984162638, val_loss: 0.016739113973208555\n",
      "epochs 28/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01606292928274917, val_loss: 0.017852009335336715\n",
      "epochs 29/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01669365811578341, val_loss: 0.020601182024141674\n",
      "epochs 30/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.016991618843013912, val_loss: 0.02371086930169871\n",
      "epochs 31/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.018371132435277104, val_loss: 0.020191792386436935\n",
      "epochs 32/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.017381847723491342, val_loss: 0.01747058812332781\n",
      "epochs 33/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.01668543426785618, val_loss: 0.016708829329888288\n",
      "epochs 34/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.014734862099622228, val_loss: 0.01666063860083293\n",
      "epochs 35/61\n",
      "Current Learning Rate: 0.00041905444825679917\n",
      "train_loss: 0.015794056504474657, val_loss: 0.01676992333101991\n",
      "epochs 36/61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:55,727] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00036: reducing learning rate of group 0 to 3.2818e-05.\n",
      "Fold: 1/6\n",
      "epochs 1/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.4449478379989925, val_loss: 2.920959772248017\n",
      "epochs 2/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 4.364244451648311, val_loss: 2.4559339347638582\n",
      "epochs 3/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 1.5473964286870079, val_loss: 0.37011785883652537\n",
      "epochs 4/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 1.0699910772474188, val_loss: 0.05159016306463041\n",
      "epochs 5/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.32231035514881734, val_loss: 0.24921160308938278\n",
      "epochs 6/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.0778033866694099, val_loss: 0.10330189058655187\n",
      "epochs 7/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.24616500950957598, val_loss: 0.30340938505373505\n",
      "epochs 8/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.3379897161533958, val_loss: 0.2893031843398747\n",
      "epochs 9/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.5831423433203446, val_loss: 0.845982175124319\n",
      "epochs 10/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 1.1273975387999886, val_loss: 0.8657797480884352\n",
      "epochs 11/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 1.0973449161178188, val_loss: 0.056509126958094145\n",
      "epochs 12/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.3032040337198659, val_loss: 0.38098309228294774\n",
      "epochs 13/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.0950817191287091, val_loss: 0.15226768231705615\n",
      "epochs 14/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.21578103147054972, val_loss: 0.09897320756786748\n",
      "epochs 15/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.3054282390757611, val_loss: 0.1847065828348461\n",
      "epochs 16/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.20578471218284808, val_loss: 0.0559154748916626\n",
      "epochs 17/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.16456113285139987, val_loss: 0.050391130541500295\n",
      "epochs 18/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.05576740685654314, val_loss: 0.07233294256423649\n",
      "epochs 19/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.08393645208132894, val_loss: 0.18866592175082156\n",
      "epochs 20/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.06873528649540324, val_loss: 0.05136727816180179\n",
      "epochs 21/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.05801116116344929, val_loss: 0.08633130161385787\n",
      "epochs 22/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.050354385454403724, val_loss: 0.08397156117778075\n",
      "epochs 23/388\n",
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.04172007347408094, val_loss: 0.05377834839256186\n",
      "epochs 24/388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:57,146] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.012513035234038234\n",
      "train_loss: 0.04732192582205722, val_loss: 0.05799440686639987\n",
      "epochs 25/388\n",
      "Fold: 1/6\n",
      "epochs 1/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 18.894613821059465, val_loss: 1.719362603990655\n",
      "epochs 2/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.5138292802791846, val_loss: 0.5162089467048645\n",
      "epochs 3/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.48345204246671575, val_loss: 0.13168473580950185\n",
      "epochs 4/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.6591292047186902, val_loss: 0.10457637229640233\n",
      "epochs 5/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.6752940957483492, val_loss: 0.26502158179094915\n",
      "epochs 6/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.6093067166052366, val_loss: 0.051506744011452325\n",
      "epochs 7/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.5683963502708235, val_loss: 0.11173442633528459\n",
      "epochs 8/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.5410211439195433, val_loss: 0.1289813273439282\n",
      "epochs 9/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.48530176595637675, val_loss: 0.05608429975415531\n",
      "epochs 10/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.4172137493365689, val_loss: 0.0626248043814772\n",
      "epochs 11/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.431630192618621, val_loss: 0.0906655247274198\n",
      "epochs 12/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.5097564419633464, val_loss: 0.10716318378323003\n",
      "epochs 13/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.35148598175299794, val_loss: 0.05137406333692764\n",
      "epochs 14/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.40113165229558945, val_loss: 0.052799256175364316\n",
      "epochs 15/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.4247465537566888, val_loss: 0.15470666026598529\n",
      "epochs 16/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.44861039284028503, val_loss: 0.2744913446275811\n",
      "epochs 17/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.3947678686756837, val_loss: 0.10277345345208519\n",
      "epochs 18/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.25752706080675125, val_loss: 0.12070825509727001\n",
      "epochs 19/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.27241226207268865, val_loss: 0.19677008433561577\n",
      "epochs 20/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.19340902253201134, val_loss: 0.10792171572776217\n",
      "epochs 21/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.23648480365150853, val_loss: 0.0819520790031866\n",
      "epochs 22/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.25284117891600255, val_loss: 0.10383479336374685\n",
      "epochs 23/140\n",
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.25753805511876154, val_loss: 0.072861452292847\n",
      "epochs 24/140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-08 02:18:58,656] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Learning Rate: 0.027899132162781483\n",
      "train_loss: 0.23099572231110774, val_loss: 0.05473709101543615\n",
      "epochs 25/140\n",
      "Number of finished trials: 20\n",
      "Best trial:\n",
      "Value: 0.027111990626374455\n",
      "Params:\n",
      "batch_size: 35\n",
      "epochs: 78\n",
      "hidden_size: 478\n",
      "layer_size: 1\n",
      "learning_rate: 0.004681379806469059\n",
      "dropout_prob: 0.24681607903179412\n",
      "weight_decay: 0.008333678497820345\n",
      "lr_step_size: 7\n",
      "gamma: 0.09623020131719237\n"
     ]
    }
   ],
   "source": [
    "study_name = \"Vanilla-LSTM-Tunner\"\n",
    "storage_url = \"sqlite:///db.sqlite3\"\n",
    "\n",
    "storage = optuna.storages.RDBStorage(url=storage_url)\n",
    "\n",
    "# Check if the study exists\n",
    "study_names = [study.study_name for study in optuna.study.get_all_study_summaries(storage=storage)]\n",
    "if study_name in study_names:\n",
    "    # Delete the study if it exists\n",
    "    print(f\"Deleting study '{study_name}'\")\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_url)\n",
    "else:\n",
    "    print(f\"Study '{study_name}' does not exist in the storage.\")\n",
    "    \n",
    "study = optuna.create_study(direction='minimize', \n",
    "                            storage=storage_url, \n",
    "                            sampler=TPESampler(),\n",
    "                            pruner=optuna.pruners.SuccessiveHalvingPruner(\n",
    "                            min_resource=3,  # Minimum amount of resource allocated to a trial\n",
    "                            reduction_factor=2,  # Reduction factor for pruning\n",
    "                            min_early_stopping_rate=3 # Minimum early-stopping rate\n",
    "                            ),\n",
    "                            study_name=study_name,\n",
    "                            load_if_exists=False)\n",
    "\n",
    "pbar = tqdm(total=20, desc='Optimizing', unit='trial')\n",
    "\n",
    "def callback(study, trial):\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "    pbar.set_postfix_str(f\"Best Value: {study.best_value:.4f}\")\n",
    "\n",
    "study.optimize(objective, n_trials=20, callbacks=[callback])\n",
    "pbar.close()\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Value:', trial.value)\n",
    "print('Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.20135455115355158\n",
      "epochs 2/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.0461604810440341\n",
      "epochs 3/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.0415962211789743\n",
      "epochs 4/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.040789665454148985\n",
      "epochs 5/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03635562783344077\n",
      "epochs 6/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.035776267083476115\n",
      "epochs 7/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03406718010879367\n",
      "epochs 8/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03283667247211397\n",
      "epochs 9/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03262831285195589\n",
      "epochs 10/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029974719722910475\n",
      "epochs 11/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02756373228693152\n",
      "epochs 12/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.028180839225298546\n",
      "epochs 13/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.02912689689975722\n",
      "epochs 14/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.027675527384864132\n",
      "epochs 15/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.027768554768459706\n",
      "epochs 16/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029894913902114097\n",
      "epochs 17/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03102682611962225\n",
      "epochs 18/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029686801818714133\n",
      "epochs 19/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.030238541457883752\n",
      "epochs 20/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.031990649288912355\n",
      "epochs 21/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.029972113000041104\n",
      "epochs 22/78\n",
      "Current Learning Rate: 0.004681379806469059\n",
      "train_loss: 0.03349126369402368\n",
      "epochs 23/78\n",
      "Epoch 00023: reducing learning rate of group 0 to 4.5049e-04.\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.03581727825905801\n",
      "epochs 24/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.025400100083890976\n",
      "epochs 25/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.023071072641310787\n",
      "epochs 26/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02277726604166163\n",
      "epochs 27/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.023194788738660384\n",
      "epochs 28/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02327017830848171\n",
      "epochs 29/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022703036133878\n",
      "epochs 30/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02279050175658133\n",
      "epochs 31/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.023109574017437632\n",
      "epochs 32/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.023116175388598715\n",
      "epochs 33/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022919618629793262\n",
      "epochs 34/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02250817815084909\n",
      "epochs 35/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022469162180705257\n",
      "epochs 36/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022006138504621686\n",
      "epochs 37/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022318413140039826\n",
      "epochs 38/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021788511930233716\n",
      "epochs 39/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02203180354492935\n",
      "epochs 40/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021744735833061368\n",
      "epochs 41/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021910247180545538\n",
      "epochs 42/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.0220001204023056\n",
      "epochs 43/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022535512872293526\n",
      "epochs 44/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02145544249970842\n",
      "epochs 45/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.0218907331725125\n",
      "epochs 46/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021916297914382692\n",
      "epochs 47/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021148512227738506\n",
      "epochs 48/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021840815625374058\n",
      "epochs 49/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021853628324415217\n",
      "epochs 50/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021793796770221537\n",
      "epochs 51/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.0219865150212539\n",
      "epochs 52/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02152265306805228\n",
      "epochs 53/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021708260243860678\n",
      "epochs 54/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021472052880358604\n",
      "epochs 55/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021466737333276684\n",
      "epochs 56/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022338376930412396\n",
      "epochs 57/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021318329056508413\n",
      "epochs 58/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021058299553707208\n",
      "epochs 59/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021262609098604963\n",
      "epochs 60/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02162806325583931\n",
      "epochs 61/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.0220643869497029\n",
      "epochs 62/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02054073849362076\n",
      "epochs 63/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.022065667690823607\n",
      "epochs 64/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021293813273363764\n",
      "epochs 65/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02144238402201026\n",
      "epochs 66/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021400750725501495\n",
      "epochs 67/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02141311196732874\n",
      "epochs 68/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021213200446238676\n",
      "epochs 69/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02143254721273766\n",
      "epochs 70/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.02157637608960565\n",
      "epochs 71/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021044596220905844\n",
      "epochs 72/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021337388378972384\n",
      "epochs 73/78\n",
      "Current Learning Rate: 0.00045049012121875666\n",
      "train_loss: 0.021999959137470444\n",
      "epochs 74/78\n",
      "Epoch 00074: reducing learning rate of group 0 to 4.3351e-05.\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.02134308415738735\n",
      "epochs 75/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.02051885481035386\n",
      "epochs 76/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.019789763159535238\n",
      "epochs 77/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.020038503116474635\n",
      "epochs 78/78\n",
      "Current Learning Rate: 4.335075505628735e-05\n",
      "train_loss: 0.019631469107065515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VanillaLSTMModel(\n",
       "  (gru): GRU(57, 478, batch_first=True)\n",
       "  (dropout): Dropout(p=0.24681607903179412, inplace=False)\n",
       "  (fc): Linear(in_features=478, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelActioner(train_data=train_data,test_data=test_data,device=device)\n",
    "model.train(trial.params)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.10666667],\n",
       "       [1.02666667],\n",
       "       [1.        ],\n",
       "       [1.26666667]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test.reshape(-1,1)\n",
    "print(len(y_test))\n",
    "y_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.06013654662590278\n"
     ]
    }
   ],
   "source": [
    "preds = model.test(trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8770607 ],\n",
       "       [0.9928646 ],\n",
       "       [1.0300233 ],\n",
       "       [1.0440997 ],\n",
       "       [1.1328907 ],\n",
       "       [1.1712093 ],\n",
       "       [1.2221115 ],\n",
       "       [0.942498  ],\n",
       "       [0.8703237 ],\n",
       "       [0.77939963],\n",
       "       [0.8447579 ],\n",
       "       [1.0676484 ],\n",
       "       [0.9912495 ],\n",
       "       [0.916434  ],\n",
       "       [0.8441156 ],\n",
       "       [0.92176217],\n",
       "       [0.9936601 ],\n",
       "       [0.9826658 ],\n",
       "       [1.0334864 ],\n",
       "       [1.0912741 ],\n",
       "       [1.1377591 ],\n",
       "       [0.66936994],\n",
       "       [0.6177618 ],\n",
       "       [0.737152  ],\n",
       "       [0.4892471 ],\n",
       "       [0.5164326 ],\n",
       "       [0.49339485],\n",
       "       [0.45327842],\n",
       "       [1.0236351 ],\n",
       "       [1.0419848 ],\n",
       "       [1.0683169 ],\n",
       "       [1.0429603 ],\n",
       "       [1.0142554 ],\n",
       "       [1.0159181 ],\n",
       "       [1.0596945 ],\n",
       "       [1.0136206 ],\n",
       "       [1.0445017 ],\n",
       "       [1.1367655 ],\n",
       "       [1.0621662 ],\n",
       "       [1.0698514 ],\n",
       "       [1.0643939 ],\n",
       "       [1.1345257 ],\n",
       "       [0.8797064 ],\n",
       "       [0.9403995 ],\n",
       "       [0.95486355],\n",
       "       [0.8326812 ],\n",
       "       [0.86810577],\n",
       "       [0.8905056 ],\n",
       "       [0.9393138 ],\n",
       "       [0.9041526 ],\n",
       "       [0.873683  ],\n",
       "       [0.85668343],\n",
       "       [0.9467225 ],\n",
       "       [0.8843799 ],\n",
       "       [0.8843843 ],\n",
       "       [0.94815356],\n",
       "       [1.05021   ],\n",
       "       [1.1792392 ],\n",
       "       [1.2902771 ],\n",
       "       [1.3343425 ],\n",
       "       [1.3963952 ],\n",
       "       [1.3718879 ],\n",
       "       [1.3695223 ],\n",
       "       [0.9126171 ],\n",
       "       [0.9841253 ],\n",
       "       [0.9845748 ],\n",
       "       [1.0731711 ],\n",
       "       [1.0764077 ],\n",
       "       [1.1432791 ],\n",
       "       [1.2478055 ],\n",
       "       [1.025935  ],\n",
       "       [1.1793675 ],\n",
       "       [1.2211308 ],\n",
       "       [1.0899183 ],\n",
       "       [1.1539714 ],\n",
       "       [1.1820923 ],\n",
       "       [1.2649802 ],\n",
       "       [0.9704554 ],\n",
       "       [1.0475293 ],\n",
       "       [1.1341643 ],\n",
       "       [1.1362567 ],\n",
       "       [1.1650898 ],\n",
       "       [1.2287748 ],\n",
       "       [1.3015125 ],\n",
       "       [1.056891  ],\n",
       "       [1.1020138 ],\n",
       "       [1.1075644 ],\n",
       "       [1.0684361 ],\n",
       "       [1.0716815 ],\n",
       "       [1.0816946 ],\n",
       "       [1.1053494 ],\n",
       "       [0.9126255 ],\n",
       "       [0.9637727 ],\n",
       "       [1.0187212 ],\n",
       "       [0.9627683 ],\n",
       "       [1.038136  ],\n",
       "       [1.073452  ],\n",
       "       [1.1514206 ],\n",
       "       [1.0710009 ],\n",
       "       [1.1295974 ],\n",
       "       [1.1440037 ],\n",
       "       [1.1771888 ],\n",
       "       [1.1818347 ],\n",
       "       [1.208665  ],\n",
       "       [1.3005672 ],\n",
       "       [1.011112  ],\n",
       "       [1.0080774 ],\n",
       "       [0.9929923 ],\n",
       "       [0.73687893],\n",
       "       [0.73814857],\n",
       "       [0.8443032 ],\n",
       "       [1.0338658 ],\n",
       "       [1.0093853 ],\n",
       "       [1.1419083 ],\n",
       "       [1.250931  ],\n",
       "       [1.3195114 ],\n",
       "       [1.2756376 ],\n",
       "       [1.2347721 ],\n",
       "       [1.340912  ],\n",
       "       [1.0813282 ],\n",
       "       [1.1996702 ],\n",
       "       [1.2939095 ],\n",
       "       [1.2593713 ],\n",
       "       [1.3464586 ],\n",
       "       [1.4213021 ],\n",
       "       [1.5466688 ],\n",
       "       [1.0687131 ],\n",
       "       [1.1493185 ],\n",
       "       [1.1229568 ],\n",
       "       [1.0592523 ],\n",
       "       [1.1400394 ],\n",
       "       [1.1909869 ],\n",
       "       [1.2707541 ],\n",
       "       [1.0298362 ],\n",
       "       [1.0645225 ],\n",
       "       [1.0721707 ],\n",
       "       [1.1130538 ],\n",
       "       [1.2129602 ],\n",
       "       [1.2664832 ],\n",
       "       [1.3781095 ],\n",
       "       [1.0097567 ],\n",
       "       [1.0251073 ],\n",
       "       [0.9572244 ],\n",
       "       [0.9680992 ],\n",
       "       [1.1267525 ],\n",
       "       [1.123708  ],\n",
       "       [1.0423656 ],\n",
       "       [1.07587   ],\n",
       "       [1.1734351 ],\n",
       "       [1.242649  ],\n",
       "       [1.2491515 ],\n",
       "       [1.3557576 ],\n",
       "       [1.4077754 ],\n",
       "       [1.5136411 ],\n",
       "       [1.0214779 ],\n",
       "       [1.163512  ],\n",
       "       [1.3961537 ],\n",
       "       [1.5211959 ],\n",
       "       [1.591736  ],\n",
       "       [1.6099796 ],\n",
       "       [1.6150572 ],\n",
       "       [1.0636975 ],\n",
       "       [1.1108234 ],\n",
       "       [1.1224272 ],\n",
       "       [1.1778723 ],\n",
       "       [1.2093419 ],\n",
       "       [1.2281982 ],\n",
       "       [1.3011278 ],\n",
       "       [0.9747198 ],\n",
       "       [1.0907725 ],\n",
       "       [1.1403213 ],\n",
       "       [1.1570218 ],\n",
       "       [1.1544042 ],\n",
       "       [1.2099184 ],\n",
       "       [1.2564446 ],\n",
       "       [0.9713872 ],\n",
       "       [1.002331  ],\n",
       "       [1.1122743 ],\n",
       "       [0.99596095],\n",
       "       [1.0378387 ],\n",
       "       [1.0788753 ],\n",
       "       [1.0381787 ],\n",
       "       [0.93244785],\n",
       "       [0.98461336],\n",
       "       [1.067952  ],\n",
       "       [1.0481715 ],\n",
       "       [1.1456982 ],\n",
       "       [1.1911342 ],\n",
       "       [1.183099  ],\n",
       "       [1.0031465 ],\n",
       "       [1.1161581 ],\n",
       "       [1.217376  ],\n",
       "       [1.2301736 ],\n",
       "       [1.2158883 ],\n",
       "       [1.2556196 ],\n",
       "       [1.3311305 ],\n",
       "       [1.0039922 ],\n",
       "       [1.039055  ],\n",
       "       [1.0838648 ],\n",
       "       [0.9983591 ],\n",
       "       [1.0366749 ],\n",
       "       [1.05972   ],\n",
       "       [1.1049556 ],\n",
       "       [1.052647  ],\n",
       "       [1.0738574 ],\n",
       "       [1.1483234 ],\n",
       "       [1.1467048 ],\n",
       "       [1.1698513 ],\n",
       "       [1.1796175 ],\n",
       "       [1.2292337 ],\n",
       "       [1.0873281 ],\n",
       "       [1.1277505 ],\n",
       "       [1.1816854 ],\n",
       "       [1.2007006 ],\n",
       "       [1.2449677 ],\n",
       "       [1.2794333 ],\n",
       "       [1.3431463 ],\n",
       "       [1.070952  ],\n",
       "       [1.1793308 ],\n",
       "       [1.2936301 ],\n",
       "       [1.3327793 ],\n",
       "       [1.425656  ],\n",
       "       [1.4438877 ],\n",
       "       [1.524021  ],\n",
       "       [0.98410547],\n",
       "       [1.1307338 ],\n",
       "       [1.1604295 ],\n",
       "       [1.0920298 ],\n",
       "       [0.9593653 ],\n",
       "       [0.92160743],\n",
       "       [0.9650287 ],\n",
       "       [1.0569553 ],\n",
       "       [1.1114991 ],\n",
       "       [1.2474703 ],\n",
       "       [1.027569  ],\n",
       "       [1.157553  ],\n",
       "       [1.2071222 ],\n",
       "       [1.2585415 ],\n",
       "       [0.9234003 ],\n",
       "       [0.93681514],\n",
       "       [0.99575007],\n",
       "       [0.99315524],\n",
       "       [1.0461308 ],\n",
       "       [1.0307995 ],\n",
       "       [1.0108826 ],\n",
       "       [0.8888445 ],\n",
       "       [0.89491826],\n",
       "       [0.9591256 ],\n",
       "       [0.87631005],\n",
       "       [0.8196806 ],\n",
       "       [0.95144   ],\n",
       "       [0.894804  ],\n",
       "       [0.8586162 ],\n",
       "       [0.9304741 ],\n",
       "       [1.0436382 ],\n",
       "       [1.0194333 ],\n",
       "       [1.054219  ],\n",
       "       [1.1763614 ],\n",
       "       [1.262161  ],\n",
       "       [1.003616  ],\n",
       "       [1.0320443 ],\n",
       "       [1.1060665 ],\n",
       "       [1.1256402 ],\n",
       "       [1.1264473 ],\n",
       "       [1.1193461 ],\n",
       "       [1.1875058 ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.array(preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(266,)\n",
      "(266,)\n",
      "MAPE Score: %25.85\n",
      "MSE Score: 0.25\n",
      "MAE Score: 0.27\n",
      "R_2 Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "preds_inverse = []\n",
    "y_true_inverse = []\n",
    "\n",
    "idx = 0\n",
    "data_size = len(y_test)//len(company_list)\n",
    "print(data_size)\n",
    "\n",
    "preds_comp_data = []\n",
    "y_true_comp_data = pd.DataFrame()\n",
    "\n",
    "for company in company_list:\n",
    "    scaler = company_dict[company][\"scaler_y\"]\n",
    "    preds_inverse.append(scaler.inverse_transform(preds[idx:idx+data_size]).flatten())\n",
    "    y_true_inverse.append(scaler.inverse_transform(y_test[idx:idx+data_size]).flatten())\n",
    "\n",
    "    for pred in preds_inverse:\n",
    "        preds_comp_data.append({\"Prediction\": pred, \"Company\": company})\n",
    "\n",
    "    idx += data_size\n",
    "\n",
    "\n",
    "\n",
    "# for company in company_list:\n",
    "#     preds_inverse.append(np.array(preds).flatten())\n",
    "#     y_true_inverse.append(np.array(y_test).flatten())\n",
    "\n",
    "comp_preds_inverse = np.array(preds_inverse)\n",
    "comp_true_inverse = np.array(y_true_inverse)\n",
    "\n",
    "preds_inverse = np.array(preds_inverse).flatten()\n",
    "y_true_inverse = np.array(y_true_inverse).flatten()\n",
    "print(y_true_inverse.shape)\n",
    "print(preds_inverse.shape)\n",
    "\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_true_inverse, preds_inverse)\n",
    "mape = mean_absolute_percentage_error(y_true_inverse, preds_inverse)*100\n",
    "mae = mean_absolute_error(y_true_inverse, preds_inverse)\n",
    "r2 = r2_score(y_true_inverse,preds_inverse)\n",
    "\n",
    "print(f\"MAPE Score: %{mape:.2f}\")\n",
    "print(f\"MSE Score: {mse:.2f}\")\n",
    "print(f\"MAE Score: {mae:.2f}\")\n",
    "print(f\"R_2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06  , 1.03  , 1.02  , 1.12  , 1.13  , 1.16  , 1.03  , 1.71  ,\n",
       "       1.3   , 2.16  , 3.31  , 1.87  , 1.45  , 2.53  , 1.84  , 1.88  ,\n",
       "       1.74  , 2.01  , 2.07  , 2.08  , 2.03  , 0.75  , 1.13  , 0.54  ,\n",
       "       0.72  , 0.65  , 0.61  , 0.7   , 1.03  , 1.06  , 0.99  , 0.94  ,\n",
       "       1.01  , 1.07  , 1.01  , 6.66  , 7.52  , 6.08  , 6.61  , 6.41  ,\n",
       "       7.15  , 8.34  , 0.9   , 0.76  , 0.05  , 0.44  , 0.57  , 0.58  ,\n",
       "       0.72  , 1.05  , 1.06  , 1.54  , 1.11  , 1.13  , 1.29  , 1.21  ,\n",
       "       1.87  , 1.95  , 1.88  , 2.11  , 1.94  , 1.96  , 1.85  , 1.74  ,\n",
       "       1.45  , 1.77  , 1.62  , 1.76  , 1.9   , 1.52  , 3.16  , 2.99  ,\n",
       "       1.87  , 2.9   , 3.07  , 3.17  , 2.25  , 1.91  , 2.05  , 2.03  ,\n",
       "       2.15  , 2.32  , 2.36  , 2.25  , 5.98  , 6.28  , 6.04  , 5.71  ,\n",
       "       5.81  , 4.79  , 4.69  , 1.13  , 1.15  , 0.78  , 1.39  , 1.33  ,\n",
       "       1.5   , 1.43  , 0.9   , 0.85  , 0.94  , 0.92  , 0.94  , 1.06  ,\n",
       "       0.95  , 0.66  , 0.66  , 0.44  , 0.56  , 0.65  , 0.7   , 0.64  ,\n",
       "       0.44  , 0.45  , 0.48  , 0.4   , 0.4   , 0.48  , 0.48  , 1.66  ,\n",
       "       1.78  , 1.55  , 1.78  , 1.89  , 2.15  , 1.96  , 2.04  , 1.69  ,\n",
       "       1.63  , 2.07  , 2.07  , 2.15  , 2.    , 1.07  , 1.02  , 1.21  ,\n",
       "       1.39  , 1.27  , 1.42  , 1.32  , 1.1   , 0.78  , 1.09  , 1.52  ,\n",
       "       1.18  , 0.77  , 1.19  , 1.3   , 1.35  , 1.31  , 1.55  , 1.54  ,\n",
       "       1.68  , 1.67  , 3.26  , 3.53  , 3.76  , 3.35  , 3.61  , 3.47  ,\n",
       "       3.6   , 0.3933, 0.3833, 0.42  , 0.4067, 0.4067, 0.4233, 0.43  ,\n",
       "       1.68  , 1.58  , 1.67  , 1.48  , 1.75  , 1.69  , 1.8   , 1.35  ,\n",
       "       1.67  , 1.11  , 1.43  , 1.52  , 1.23  , 1.41  , 1.15  , 1.33  ,\n",
       "       1.24  , 1.33  , 1.42  , 1.33  , 1.18  , 2.72  , 2.82  , 2.75  ,\n",
       "       2.61  , 2.88  , 2.94  , 2.97  , 3.01  , 3.15  , 2.44  , 3.    ,\n",
       "       3.14  , 3.22  , 2.33  , 0.32  , 0.37  , 0.37  , 0.37  , 0.37  ,\n",
       "       0.39  , 0.39  , 1.0333, 1.12  , 1.1933, 1.2067, 1.2   , 1.3333,\n",
       "       1.26  , 0.6   , 0.65  , 0.65  , 0.69  , 0.66  , 0.74  , 0.63  ,\n",
       "       2.05  , 1.87  , 1.68  , 1.24  , 1.45  , 1.51  , 1.98  , 1.77  ,\n",
       "       2.3   , 1.41  , 2.09  , 2.03  , 2.13  , 2.03  , 1.81  , 2.54  ,\n",
       "       2.13  , 2.83  , 2.02  , 1.43  , 3.32  , 1.08  , 1.16  , 1.09  ,\n",
       "       1.03  , 1.3   , 1.07  , 0.93  , 0.4311, 0.4978, 0.4178, 0.44  ,\n",
       "       0.5467, 0.5667, 0.4733, 0.89  , 1.04  , 1.08  , 1.04  , 0.99  ,\n",
       "       1.17  , 0.97  ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97389776, 1.0173242 , 1.0312588 , 1.0365374 , 1.069834  ,\n",
       "       1.0842035 , 1.1032917 , 2.8388956 , 2.674338  , 2.4670312 ,\n",
       "       2.616048  , 3.1242385 , 2.950049  , 2.7794695 , 1.909821  ,\n",
       "       2.00455   , 2.0922654 , 2.0788522 , 2.1408534 , 2.2113545 ,\n",
       "       2.2680662 , 0.8778291 , 0.85563755, 0.9069754 , 0.8003763 ,\n",
       "       0.812066  , 0.8021598 , 0.7849097 , 1.1160719 , 1.1285497 ,\n",
       "       1.1464555 , 1.129213  , 1.1096936 , 1.1108243 , 1.1405922 ,\n",
       "       6.752712  , 6.8722215 , 7.2292824 , 6.940583  , 6.970325  ,\n",
       "       6.9492044 , 7.2206144 , 1.163095  , 1.2674872 , 1.2923653 ,\n",
       "       1.0822116 , 1.143142  , 1.1816697 , 1.2656199 , 1.2822356 ,\n",
       "       1.2511566 , 1.2338171 , 1.3256569 , 1.2620676 , 1.262072  ,\n",
       "       1.3271166 , 1.6422603 , 1.8022565 , 1.9399436 , 1.9945847 ,\n",
       "       2.07153   , 2.041141  , 2.0382078 , 1.752519  , 1.8404742 ,\n",
       "       1.841027  , 1.9500005 , 1.9539814 , 2.0362332 , 2.1648006 ,\n",
       "       2.6666832 , 2.9428616 , 3.0180357 , 2.781853  , 2.8971486 ,\n",
       "       2.947766  , 3.0969644 , 1.7807178 , 1.8831942 , 1.9983827 ,\n",
       "       2.0011647 , 2.0395007 , 2.1241753 , 2.2208862 , 7.393181  ,\n",
       "       7.744688  , 7.787927  , 7.4831176 , 7.508399  , 7.586401  ,\n",
       "       7.7706723 , 1.1590344 , 1.2239914 , 1.2937759 , 1.2227157 ,\n",
       "       1.3184327 , 1.363284  , 1.4623041 , 0.93990254, 0.97283375,\n",
       "       0.9809301 , 0.9995801 , 1.0021911 , 1.0172697 , 1.0689187 ,\n",
       "       0.73355585, 0.7325848 , 0.7277576 , 0.64580125, 0.6462076 ,\n",
       "       0.680177  , 0.7408371 , 0.38178322, 0.40696254, 0.4276769 ,\n",
       "       0.44070718, 0.43237114, 0.4246067 , 0.44477326, 1.5892171 ,\n",
       "       1.7190382 , 1.8224188 , 1.7845304 , 1.8800651 , 1.9621685 ,\n",
       "       2.0996957 , 2.1292536 , 2.2574162 , 2.215501  , 2.114211  ,\n",
       "       2.2426627 , 2.3236692 , 2.450499  , 1.2989411 , 1.3325869 ,\n",
       "       1.3400056 , 1.3796622 , 1.4765714 , 1.5284888 , 1.6367662 ,\n",
       "       1.3911227 , 1.4086223 , 1.3312358 , 1.343633  , 1.5244979 ,\n",
       "       1.5210272 , 1.4282968 , 1.3760068 , 1.4608885 , 1.5211046 ,\n",
       "       1.5267618 , 1.6195091 , 1.6647645 , 1.7568678 , 3.0012376 ,\n",
       "       3.273943  , 3.720615  , 3.9606962 , 4.0961328 , 4.131161  ,\n",
       "       4.14091   , 0.42667645, 0.43657294, 0.43900973, 0.45065317,\n",
       "       0.45726177, 0.4612216 , 0.4765368 , 1.5277535 , 1.6298798 ,\n",
       "       1.6734827 , 1.6881791 , 1.6858757 , 1.7347282 , 1.7756712 ,\n",
       "       1.3728179 , 1.4022144 , 1.5066606 , 1.3961629 , 1.4359467 ,\n",
       "       1.4749315 , 1.4362698 , 1.2163682 , 1.2732286 , 1.3640677 ,\n",
       "       1.3425069 , 1.448811  , 1.4983363 , 1.4895779 , 2.4340277 ,\n",
       "       2.5786822 , 2.7082412 , 2.7246222 , 2.706337  , 2.757193  ,\n",
       "       2.853847  , 3.0880241 , 3.1585004 , 3.248568  , 3.0767016 ,\n",
       "       3.1537163 , 3.2000372 , 3.2909606 , 0.37632057, 0.3828958 ,\n",
       "       0.40598026, 0.40547848, 0.4126539 , 0.41568142, 0.43106246,\n",
       "       1.1600555 , 1.1940104 , 1.2393157 , 1.2552885 , 1.2924728 ,\n",
       "       1.3214239 , 1.3749429 , 0.5797999 , 0.62531894, 0.67332464,\n",
       "       0.6897673 , 0.7287755 , 0.73643285, 0.77008885, 1.8142644 ,\n",
       "       1.9594265 , 1.9888251 , 1.9211096 , 1.7897717 , 1.7523913 ,\n",
       "       1.7953783 , 1.8303727 , 1.8881891 , 2.0323186 , 1.7992233 ,\n",
       "       1.9370062 , 1.9895496 , 2.044054  , 2.8851752 , 2.9403102 ,\n",
       "       3.1825328 , 3.171868  , 3.3895977 , 3.326586  , 3.2447276 ,\n",
       "       1.0733147 , 1.0758656 , 1.1028328 , 1.0680501 , 1.0442659 ,\n",
       "       1.0996047 , 1.0758177 , 0.42523506, 0.44854578, 0.48525625,\n",
       "       0.47740415, 0.48868865, 0.52831167, 0.556145  , 1.0933628 ,\n",
       "       1.1198012 , 1.1886418 , 1.2068453 , 1.207596  , 1.2009919 ,\n",
       "       1.2643803 ], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_inverse = np.array(preds_inverse).flatten()\n",
    "preds_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8770607 ],\n",
       "       [0.9928646 ],\n",
       "       [1.0300233 ],\n",
       "       [1.0440997 ],\n",
       "       [1.1328907 ],\n",
       "       [1.1712093 ],\n",
       "       [1.2221115 ],\n",
       "       [0.942498  ],\n",
       "       [0.8703237 ],\n",
       "       [0.77939963],\n",
       "       [0.8447579 ],\n",
       "       [1.0676484 ],\n",
       "       [0.9912495 ],\n",
       "       [0.916434  ],\n",
       "       [0.8441156 ],\n",
       "       [0.92176217],\n",
       "       [0.9936601 ],\n",
       "       [0.9826658 ],\n",
       "       [1.0334864 ],\n",
       "       [1.0912741 ],\n",
       "       [1.1377591 ],\n",
       "       [0.66936994],\n",
       "       [0.6177618 ],\n",
       "       [0.737152  ],\n",
       "       [0.4892471 ],\n",
       "       [0.5164326 ],\n",
       "       [0.49339485],\n",
       "       [0.45327842],\n",
       "       [1.0236351 ],\n",
       "       [1.0419848 ],\n",
       "       [1.0683169 ],\n",
       "       [1.0429603 ],\n",
       "       [1.0142554 ],\n",
       "       [1.0159181 ],\n",
       "       [1.0596945 ],\n",
       "       [1.0136206 ],\n",
       "       [1.0445017 ],\n",
       "       [1.1367655 ],\n",
       "       [1.0621662 ],\n",
       "       [1.0698514 ],\n",
       "       [1.0643939 ],\n",
       "       [1.1345257 ],\n",
       "       [0.8797064 ],\n",
       "       [0.9403995 ],\n",
       "       [0.95486355],\n",
       "       [0.8326812 ],\n",
       "       [0.86810577],\n",
       "       [0.8905056 ],\n",
       "       [0.9393138 ],\n",
       "       [0.9041526 ],\n",
       "       [0.873683  ],\n",
       "       [0.85668343],\n",
       "       [0.9467225 ],\n",
       "       [0.8843799 ],\n",
       "       [0.8843843 ],\n",
       "       [0.94815356],\n",
       "       [1.05021   ],\n",
       "       [1.1792392 ],\n",
       "       [1.2902771 ],\n",
       "       [1.3343425 ],\n",
       "       [1.3963952 ],\n",
       "       [1.3718879 ],\n",
       "       [1.3695223 ],\n",
       "       [0.9126171 ],\n",
       "       [0.9841253 ],\n",
       "       [0.9845748 ],\n",
       "       [1.0731711 ],\n",
       "       [1.0764077 ],\n",
       "       [1.1432791 ],\n",
       "       [1.2478055 ],\n",
       "       [1.025935  ],\n",
       "       [1.1793675 ],\n",
       "       [1.2211308 ],\n",
       "       [1.0899183 ],\n",
       "       [1.1539714 ],\n",
       "       [1.1820923 ],\n",
       "       [1.2649802 ],\n",
       "       [0.9704554 ],\n",
       "       [1.0475293 ],\n",
       "       [1.1341643 ],\n",
       "       [1.1362567 ],\n",
       "       [1.1650898 ],\n",
       "       [1.2287748 ],\n",
       "       [1.3015125 ],\n",
       "       [1.056891  ],\n",
       "       [1.1020138 ],\n",
       "       [1.1075644 ],\n",
       "       [1.0684361 ],\n",
       "       [1.0716815 ],\n",
       "       [1.0816946 ],\n",
       "       [1.1053494 ],\n",
       "       [0.9126255 ],\n",
       "       [0.9637727 ],\n",
       "       [1.0187212 ],\n",
       "       [0.9627683 ],\n",
       "       [1.038136  ],\n",
       "       [1.073452  ],\n",
       "       [1.1514206 ],\n",
       "       [1.0710009 ],\n",
       "       [1.1295974 ],\n",
       "       [1.1440037 ],\n",
       "       [1.1771888 ],\n",
       "       [1.1818347 ],\n",
       "       [1.208665  ],\n",
       "       [1.3005672 ],\n",
       "       [1.011112  ],\n",
       "       [1.0080774 ],\n",
       "       [0.9929923 ],\n",
       "       [0.73687893],\n",
       "       [0.73814857],\n",
       "       [0.8443032 ],\n",
       "       [1.0338658 ],\n",
       "       [1.0093853 ],\n",
       "       [1.1419083 ],\n",
       "       [1.250931  ],\n",
       "       [1.3195114 ],\n",
       "       [1.2756376 ],\n",
       "       [1.2347721 ],\n",
       "       [1.340912  ],\n",
       "       [1.0813282 ],\n",
       "       [1.1996702 ],\n",
       "       [1.2939095 ],\n",
       "       [1.2593713 ],\n",
       "       [1.3464586 ],\n",
       "       [1.4213021 ],\n",
       "       [1.5466688 ],\n",
       "       [1.0687131 ],\n",
       "       [1.1493185 ],\n",
       "       [1.1229568 ],\n",
       "       [1.0592523 ],\n",
       "       [1.1400394 ],\n",
       "       [1.1909869 ],\n",
       "       [1.2707541 ],\n",
       "       [1.0298362 ],\n",
       "       [1.0645225 ],\n",
       "       [1.0721707 ],\n",
       "       [1.1130538 ],\n",
       "       [1.2129602 ],\n",
       "       [1.2664832 ],\n",
       "       [1.3781095 ],\n",
       "       [1.0097567 ],\n",
       "       [1.0251073 ],\n",
       "       [0.9572244 ],\n",
       "       [0.9680992 ],\n",
       "       [1.1267525 ],\n",
       "       [1.123708  ],\n",
       "       [1.0423656 ],\n",
       "       [1.07587   ],\n",
       "       [1.1734351 ],\n",
       "       [1.242649  ],\n",
       "       [1.2491515 ],\n",
       "       [1.3557576 ],\n",
       "       [1.4077754 ],\n",
       "       [1.5136411 ],\n",
       "       [1.0214779 ],\n",
       "       [1.163512  ],\n",
       "       [1.3961537 ],\n",
       "       [1.5211959 ],\n",
       "       [1.591736  ],\n",
       "       [1.6099796 ],\n",
       "       [1.6150572 ],\n",
       "       [1.0636975 ],\n",
       "       [1.1108234 ],\n",
       "       [1.1224272 ],\n",
       "       [1.1778723 ],\n",
       "       [1.2093419 ],\n",
       "       [1.2281982 ],\n",
       "       [1.3011278 ],\n",
       "       [0.9747198 ],\n",
       "       [1.0907725 ],\n",
       "       [1.1403213 ],\n",
       "       [1.1570218 ],\n",
       "       [1.1544042 ],\n",
       "       [1.2099184 ],\n",
       "       [1.2564446 ],\n",
       "       [0.9713872 ],\n",
       "       [1.002331  ],\n",
       "       [1.1122743 ],\n",
       "       [0.99596095],\n",
       "       [1.0378387 ],\n",
       "       [1.0788753 ],\n",
       "       [1.0381787 ],\n",
       "       [0.93244785],\n",
       "       [0.98461336],\n",
       "       [1.067952  ],\n",
       "       [1.0481715 ],\n",
       "       [1.1456982 ],\n",
       "       [1.1911342 ],\n",
       "       [1.183099  ],\n",
       "       [1.0031465 ],\n",
       "       [1.1161581 ],\n",
       "       [1.217376  ],\n",
       "       [1.2301736 ],\n",
       "       [1.2158883 ],\n",
       "       [1.2556196 ],\n",
       "       [1.3311305 ],\n",
       "       [1.0039922 ],\n",
       "       [1.039055  ],\n",
       "       [1.0838648 ],\n",
       "       [0.9983591 ],\n",
       "       [1.0366749 ],\n",
       "       [1.05972   ],\n",
       "       [1.1049556 ],\n",
       "       [1.052647  ],\n",
       "       [1.0738574 ],\n",
       "       [1.1483234 ],\n",
       "       [1.1467048 ],\n",
       "       [1.1698513 ],\n",
       "       [1.1796175 ],\n",
       "       [1.2292337 ],\n",
       "       [1.0873281 ],\n",
       "       [1.1277505 ],\n",
       "       [1.1816854 ],\n",
       "       [1.2007006 ],\n",
       "       [1.2449677 ],\n",
       "       [1.2794333 ],\n",
       "       [1.3431463 ],\n",
       "       [1.070952  ],\n",
       "       [1.1793308 ],\n",
       "       [1.2936301 ],\n",
       "       [1.3327793 ],\n",
       "       [1.425656  ],\n",
       "       [1.4438877 ],\n",
       "       [1.524021  ],\n",
       "       [0.98410547],\n",
       "       [1.1307338 ],\n",
       "       [1.1604295 ],\n",
       "       [1.0920298 ],\n",
       "       [0.9593653 ],\n",
       "       [0.92160743],\n",
       "       [0.9650287 ],\n",
       "       [1.0569553 ],\n",
       "       [1.1114991 ],\n",
       "       [1.2474703 ],\n",
       "       [1.027569  ],\n",
       "       [1.157553  ],\n",
       "       [1.2071222 ],\n",
       "       [1.2585415 ],\n",
       "       [0.9234003 ],\n",
       "       [0.93681514],\n",
       "       [0.99575007],\n",
       "       [0.99315524],\n",
       "       [1.0461308 ],\n",
       "       [1.0307995 ],\n",
       "       [1.0108826 ],\n",
       "       [0.8888445 ],\n",
       "       [0.89491826],\n",
       "       [0.9591256 ],\n",
       "       [0.87631005],\n",
       "       [0.8196806 ],\n",
       "       [0.95144   ],\n",
       "       [0.894804  ],\n",
       "       [0.8586162 ],\n",
       "       [0.9304741 ],\n",
       "       [1.0436382 ],\n",
       "       [1.0194333 ],\n",
       "       [1.054219  ],\n",
       "       [1.1763614 ],\n",
       "       [1.262161  ],\n",
       "       [1.003616  ],\n",
       "       [1.0320443 ],\n",
       "       [1.1060665 ],\n",
       "       [1.1256402 ],\n",
       "       [1.1264473 ],\n",
       "       [1.1193461 ],\n",
       "       [1.1875058 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.flatten()\n",
    "x_indices = np.arange(len(y_true_inverse))\n",
    "preds = np.array(preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "All Stocks True: EPS - PENDS",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265
         ],
         "y": [
          1.06,
          1.03,
          1.02,
          1.12,
          1.13,
          1.16,
          1.03,
          1.7099999999999997,
          1.3,
          2.16,
          3.31,
          1.87,
          1.45,
          2.53,
          1.84,
          1.88,
          1.74,
          2.01,
          2.07,
          2.08,
          2.03,
          0.7500000000000001,
          1.13,
          0.54,
          0.72,
          0.65,
          0.61,
          0.7000000000000001,
          1.03,
          1.06,
          0.99,
          0.94,
          1.01,
          1.07,
          1.01,
          6.66,
          7.519999999999999,
          6.08,
          6.61,
          6.41,
          7.15,
          8.34,
          0.9000000000000002,
          0.76,
          0.049999999999999906,
          0.44,
          0.5699999999999998,
          0.5799999999999998,
          0.72,
          1.05,
          1.06,
          1.54,
          1.11,
          1.13,
          1.2899999999999998,
          1.21,
          1.87,
          1.95,
          1.88,
          2.11,
          1.9399999999999997,
          1.9599999999999997,
          1.85,
          1.74,
          1.45,
          1.77,
          1.62,
          1.7600000000000002,
          1.9,
          1.52,
          3.16,
          2.99,
          1.87,
          2.9,
          3.0700000000000007,
          3.17,
          2.2500000000000004,
          1.9099999999999997,
          2.05,
          2.0299999999999994,
          2.15,
          2.32,
          2.3599999999999994,
          2.25,
          5.980000000000001,
          6.28,
          6.04,
          5.71,
          5.809999999999999,
          4.79,
          4.6899999999999995,
          1.13,
          1.15,
          0.7800000000000001,
          1.39,
          1.33,
          1.5,
          1.43,
          0.8999999999999999,
          0.8500000000000001,
          0.9399999999999997,
          0.9200000000000002,
          0.9399999999999997,
          1.06,
          0.95,
          0.66,
          0.66,
          0.44,
          0.56,
          0.65,
          0.7,
          0.64,
          0.44,
          0.45,
          0.48000000000000004,
          0.4,
          0.4,
          0.48000000000000004,
          0.48000000000000004,
          1.6600000000000001,
          1.7800000000000002,
          1.55,
          1.7800000000000002,
          1.89,
          2.15,
          1.9600000000000004,
          2.04,
          1.69,
          1.6300000000000001,
          2.07,
          2.07,
          2.15,
          2,
          1.07,
          1.02,
          1.21,
          1.39,
          1.27,
          1.42,
          1.32,
          1.1000000000000003,
          0.7799999999999999,
          1.0900000000000003,
          1.52,
          1.18,
          0.77,
          1.1900000000000002,
          1.3,
          1.3499999999999999,
          1.3099999999999998,
          1.55,
          1.54,
          1.68,
          1.67,
          3.26,
          3.53,
          3.76,
          3.35,
          3.61,
          3.4700000000000006,
          3.6,
          0.3933,
          0.3833,
          0.42,
          0.4067,
          0.4067,
          0.42329999999999995,
          0.43,
          1.68,
          1.58,
          1.6700000000000002,
          1.48,
          1.75,
          1.69,
          1.7999999999999996,
          1.35,
          1.67,
          1.11,
          1.4300000000000002,
          1.52,
          1.23,
          1.41,
          1.15,
          1.33,
          1.24,
          1.33,
          1.4199999999999997,
          1.33,
          1.18,
          2.72,
          2.8200000000000003,
          2.75,
          2.6099999999999994,
          2.88,
          2.94,
          2.9700000000000006,
          3.01,
          3.1500000000000004,
          2.4400000000000004,
          3.0000000000000004,
          3.14,
          3.22,
          2.33,
          0.32,
          0.37,
          0.37,
          0.37,
          0.37,
          0.39,
          0.39,
          1.0333,
          1.12,
          1.1933,
          1.2067,
          1.2,
          1.3333,
          1.2599999999999998,
          0.6,
          0.65,
          0.65,
          0.69,
          0.66,
          0.74,
          0.63,
          2.05,
          1.8700000000000003,
          1.68,
          1.24,
          1.4500000000000002,
          1.51,
          1.9799999999999998,
          1.77,
          2.2999999999999994,
          1.41,
          2.09,
          2.03,
          2.1299999999999994,
          2.03,
          1.8100000000000003,
          2.54,
          2.13,
          2.83,
          2.02,
          1.4300000000000002,
          3.3199999999999994,
          1.08,
          1.16,
          1.09,
          1.0300000000000002,
          1.3,
          1.07,
          0.93,
          0.4310999999999999,
          0.49779999999999996,
          0.4178,
          0.44,
          0.5466999999999999,
          0.5666999999999999,
          0.4733,
          0.89,
          1.04,
          1.08,
          1.04,
          0.99,
          1.17,
          0.97
         ]
        },
        {
         "mode": "lines+markers",
         "name": "All Stocks Preds: EPS - PENDS",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265
         ],
         "y": [
          0.9738977551460266,
          1.0173242092132568,
          1.0312588214874268,
          1.0365374088287354,
          1.0698339939117432,
          1.0842034816741943,
          1.1032917499542236,
          2.838895559310913,
          2.6743381023406982,
          2.467031240463257,
          2.6160480976104736,
          3.1242384910583496,
          2.9500489234924316,
          2.7794694900512695,
          1.9098210334777832,
          2.004549980163574,
          2.0922653675079346,
          2.0788521766662598,
          2.1408534049987793,
          2.2113544940948486,
          2.268066167831421,
          0.8778290748596191,
          0.8556375503540039,
          0.9069753885269165,
          0.800376296043396,
          0.8120660185813904,
          0.8021597862243652,
          0.784909725189209,
          1.1160719394683838,
          1.1285496950149536,
          1.1464555263519287,
          1.1292129755020142,
          1.1096936464309692,
          1.1108243465423584,
          1.1405922174453735,
          6.752711772918701,
          6.87222146987915,
          7.229282379150391,
          6.940583229064941,
          6.970324993133545,
          6.949204444885254,
          7.220614433288574,
          1.1630949974060059,
          1.2674871683120728,
          1.292365312576294,
          1.0822116136550903,
          1.1431419849395752,
          1.1816697120666504,
          1.2656198740005493,
          1.2822356224060059,
          1.2511565685272217,
          1.2338171005249023,
          1.3256568908691406,
          1.2620675563812256,
          1.262071967124939,
          1.32711660861969,
          1.6422603130340576,
          1.802256464958191,
          1.939943552017212,
          1.9945846796035767,
          2.0715301036834717,
          2.0411410331726074,
          2.038207769393921,
          1.7525190114974976,
          1.840474247932434,
          1.841027021408081,
          1.950000524520874,
          1.9539813995361328,
          2.0362331867218018,
          2.1648006439208984,
          2.6666831970214844,
          2.942861557006836,
          3.018035650253296,
          2.781852960586548,
          2.897148609161377,
          2.947766065597534,
          3.0969643592834473,
          1.7807178497314453,
          1.8831942081451416,
          1.9983826875686646,
          2.001164674758911,
          2.0395007133483887,
          2.1241753101348877,
          2.22088623046875,
          7.393180847167969,
          7.744688034057617,
          7.787927150726318,
          7.483117580413818,
          7.50839900970459,
          7.586400985717773,
          7.77067232131958,
          1.1590343713760376,
          1.2239913940429688,
          1.2937759160995483,
          1.2227157354354858,
          1.3184326887130737,
          1.3632839918136597,
          1.4623041152954102,
          0.9399025440216064,
          0.9728337526321411,
          0.9809300899505615,
          0.9995800852775574,
          1.0021910667419434,
          1.0172697305679321,
          1.0689187049865723,
          0.7335558533668518,
          0.7325847744941711,
          0.7277575731277466,
          0.6458012461662292,
          0.6462075710296631,
          0.6801769733428955,
          0.7408370971679688,
          0.38178321719169617,
          0.4069625437259674,
          0.42767688632011414,
          0.44070717692375183,
          0.4323711395263672,
          0.42460671067237854,
          0.44477325677871704,
          1.5892170667648315,
          1.7190382480621338,
          1.8224188089370728,
          1.7845304012298584,
          1.8800650835037231,
          1.9621684551239014,
          2.0996956825256348,
          2.129253625869751,
          2.257416248321533,
          2.215501070022583,
          2.114211082458496,
          2.2426626682281494,
          2.323669195175171,
          2.4504990577697754,
          1.2989411354064941,
          1.3325868844985962,
          1.34000563621521,
          1.3796621561050415,
          1.4765714406967163,
          1.5284887552261353,
          1.6367661952972412,
          1.3911226987838745,
          1.4086222648620605,
          1.3312357664108276,
          1.3436330556869507,
          1.5244978666305542,
          1.5210272073745728,
          1.4282968044281006,
          1.376006841659546,
          1.4608885049819946,
          1.5211045742034912,
          1.526761770248413,
          1.6195091009140015,
          1.6647645235061646,
          1.75686776638031,
          3.001237630844116,
          3.2739429473876953,
          3.7206149101257324,
          3.960696220397949,
          4.096132755279541,
          4.131161212921143,
          4.1409101486206055,
          0.4266764521598816,
          0.43657293915748596,
          0.43900972604751587,
          0.45065316557884216,
          0.4572617709636688,
          0.4612216055393219,
          0.4765368103981018,
          1.527753472328186,
          1.6298798322677612,
          1.6734826564788818,
          1.6881791353225708,
          1.685875654220581,
          1.734728217124939,
          1.7756712436676025,
          1.372817873954773,
          1.4022144079208374,
          1.5066605806350708,
          1.3961628675460815,
          1.4359467029571533,
          1.4749314785003662,
          1.436269760131836,
          1.2163681983947754,
          1.273228645324707,
          1.3640676736831665,
          1.3425068855285645,
          1.4488110542297363,
          1.4983363151550293,
          1.4895778894424438,
          2.434027671813965,
          2.5786821842193604,
          2.7082412242889404,
          2.7246222496032715,
          2.7063369750976562,
          2.757193088531494,
          2.853847026824951,
          3.088024139404297,
          3.1585004329681396,
          3.248568058013916,
          3.0767016410827637,
          3.1537163257598877,
          3.2000372409820557,
          3.2909605503082275,
          0.37632057070732117,
          0.3828957974910736,
          0.40598025918006897,
          0.40547847747802734,
          0.4126538932323456,
          0.4156814217567444,
          0.4310624599456787,
          1.1600555181503296,
          1.1940103769302368,
          1.2393157482147217,
          1.2552884817123413,
          1.2924728393554688,
          1.321423888206482,
          1.3749428987503052,
          0.5797998905181885,
          0.6253189444541931,
          0.6733246445655823,
          0.6897673010826111,
          0.7287755012512207,
          0.7364328503608704,
          0.7700888514518738,
          1.8142644166946411,
          1.9594265222549438,
          1.9888250827789307,
          1.9211095571517944,
          1.7897716760635376,
          1.7523913383483887,
          1.79537832736969,
          1.83037269115448,
          1.8881890773773193,
          2.032318592071533,
          1.7992233037948608,
          1.9370062351226807,
          1.9895496368408203,
          2.0440540313720703,
          2.8851752281188965,
          2.94031023979187,
          3.182532787322998,
          3.171868085861206,
          3.3895976543426514,
          3.3265860080718994,
          3.244727611541748,
          1.0733146667480469,
          1.075865626335144,
          1.1028327941894531,
          1.0680501461029053,
          1.044265866279602,
          1.0996047258377075,
          1.0758177042007446,
          0.4252350628376007,
          0.44854578375816345,
          0.48525625467300415,
          0.4774041473865509,
          0.4886886477470398,
          0.5283116698265076,
          0.5561450123786926,
          1.093362808227539,
          1.1198011636734009,
          1.1886417865753174,
          1.2068452835083008,
          1.207595944404602,
          1.2009918689727783,
          1.2643803358078003
         ]
        }
       ],
       "layout": {
        "height": 600,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "PGR: EPS - PENDS"
        },
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "rangemode": "tozero",
         "side": "left",
         "title": {
          "text": "EPS"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace_true = go.Scatter(x=x_indices, y=y_true_inverse, mode=\"lines+markers\", name=f\"All Stocks True: EPS - PENDS\")\n",
    "trace_preds = go.Scatter(x=x_indices, y=preds_inverse, mode=\"lines+markers\", name=f\"All Stocks Preds: EPS - PENDS\")\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = f\"{stock_symbol}: EPS - PENDS\",\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='EPS', side='left', rangemode='tozero'),\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace_true, trace_preds], layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "x_indices = [[\"2022-Q2\", \"2022-Q3\" ,\"2022-Q4\", '2023-Q1', '2023-Q2', '2023-Q3']] * len(company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines+markers",
         "name": "All Companies True: EPS - PENDS",
         "type": "scatter",
         "x": [
          "2022-Q2 AFL",
          "2022-Q3 AFL",
          "2022-Q4 AFL",
          "2023-Q1 AFL",
          "2023-Q2 AFL",
          "2023-Q3 AFL",
          "2022-Q2 AON",
          "2022-Q3 AON",
          "2022-Q4 AON",
          "2023-Q1 AON",
          "2023-Q2 AON",
          "2023-Q3 AON",
          "2022-Q2 AXP",
          "2022-Q3 AXP",
          "2022-Q4 AXP",
          "2023-Q1 AXP",
          "2023-Q2 AXP",
          "2023-Q3 AXP",
          "2022-Q2 BEN",
          "2022-Q3 BEN",
          "2022-Q4 BEN",
          "2023-Q1 BEN",
          "2023-Q2 BEN",
          "2023-Q3 BEN",
          "2022-Q2 BK",
          "2022-Q3 BK",
          "2022-Q4 BK",
          "2023-Q1 BK",
          "2023-Q2 BK",
          "2023-Q3 BK",
          "2022-Q2 BLK",
          "2022-Q3 BLK",
          "2022-Q4 BLK",
          "2023-Q1 BLK",
          "2023-Q2 BLK",
          "2023-Q3 BLK",
          "2022-Q2 BX",
          "2022-Q3 BX",
          "2022-Q4 BX",
          "2023-Q1 BX",
          "2023-Q2 BX",
          "2023-Q3 BX",
          "2022-Q2 CBOE",
          "2022-Q3 CBOE",
          "2022-Q4 CBOE",
          "2023-Q1 CBOE",
          "2023-Q2 CBOE",
          "2023-Q3 CBOE",
          "2022-Q2 CMA",
          "2022-Q3 CMA",
          "2022-Q4 CMA",
          "2023-Q1 CMA",
          "2023-Q2 CMA",
          "2023-Q3 CMA",
          "2022-Q2 CME",
          "2022-Q3 CME",
          "2022-Q4 CME",
          "2023-Q1 CME",
          "2023-Q2 CME",
          "2023-Q3 CME",
          "2022-Q2 COF",
          "2022-Q3 COF",
          "2022-Q4 COF",
          "2023-Q1 COF",
          "2023-Q2 COF",
          "2023-Q3 COF",
          "2022-Q2 DFS",
          "2022-Q3 DFS",
          "2022-Q4 DFS",
          "2023-Q1 DFS",
          "2023-Q2 DFS",
          "2023-Q3 DFS",
          "2022-Q2 GS",
          "2022-Q3 GS",
          "2022-Q4 GS",
          "2023-Q1 GS",
          "2023-Q2 GS",
          "2023-Q3 GS",
          "2022-Q2 HIG",
          "2022-Q3 HIG",
          "2022-Q4 HIG",
          "2023-Q1 HIG",
          "2023-Q2 HIG",
          "2023-Q3 HIG",
          "2022-Q2 ICE",
          "2022-Q3 ICE",
          "2022-Q4 ICE",
          "2023-Q1 ICE",
          "2023-Q2 ICE",
          "2023-Q3 ICE",
          "2022-Q2 IVZ",
          "2022-Q3 IVZ",
          "2022-Q4 IVZ",
          "2023-Q1 IVZ",
          "2023-Q2 IVZ",
          "2023-Q3 IVZ",
          "2022-Q2 KEY",
          "2022-Q3 KEY",
          "2022-Q4 KEY",
          "2023-Q1 KEY",
          "2023-Q2 KEY",
          "2023-Q3 KEY",
          "2022-Q2 MA",
          "2022-Q3 MA",
          "2022-Q4 MA",
          "2023-Q1 MA",
          "2023-Q2 MA",
          "2023-Q3 MA",
          "2022-Q2 MCO",
          "2022-Q3 MCO",
          "2022-Q4 MCO",
          "2023-Q1 MCO",
          "2023-Q2 MCO",
          "2023-Q3 MCO",
          "2022-Q2 MKTX",
          "2022-Q3 MKTX",
          "2022-Q4 MKTX",
          "2023-Q1 MKTX",
          "2023-Q2 MKTX",
          "2023-Q3 MKTX",
          "2022-Q2 MMC",
          "2022-Q3 MMC",
          "2022-Q4 MMC",
          "2023-Q1 MMC",
          "2023-Q2 MMC",
          "2023-Q3 MMC",
          "2022-Q2 MSCI",
          "2022-Q3 MSCI",
          "2022-Q4 MSCI",
          "2023-Q1 MSCI",
          "2023-Q2 MSCI",
          "2023-Q3 MSCI",
          "2022-Q2 MTB",
          "2022-Q3 MTB",
          "2022-Q4 MTB",
          "2023-Q1 MTB",
          "2023-Q2 MTB",
          "2023-Q3 MTB",
          "2022-Q2 NDAQ",
          "2022-Q3 NDAQ",
          "2022-Q4 NDAQ",
          "2023-Q1 NDAQ",
          "2023-Q2 NDAQ",
          "2023-Q3 NDAQ",
          "2022-Q2 NTRS",
          "2022-Q3 NTRS",
          "2022-Q4 NTRS",
          "2023-Q1 NTRS",
          "2023-Q2 NTRS",
          "2023-Q3 NTRS",
          "2022-Q2 PFG",
          "2022-Q3 PFG",
          "2022-Q4 PFG",
          "2023-Q1 PFG",
          "2023-Q2 PFG",
          "2023-Q3 PFG",
          "2022-Q2 PGR",
          "2022-Q3 PGR",
          "2022-Q4 PGR",
          "2023-Q1 PGR",
          "2023-Q2 PGR",
          "2023-Q3 PGR",
          "2022-Q2 PNC",
          "2022-Q3 PNC",
          "2022-Q4 PNC",
          "2023-Q1 PNC",
          "2023-Q2 PNC",
          "2023-Q3 PNC",
          "2022-Q2 PRU",
          "2022-Q3 PRU",
          "2022-Q4 PRU",
          "2023-Q1 PRU",
          "2023-Q2 PRU",
          "2023-Q3 PRU",
          "2022-Q2 RF",
          "2022-Q3 RF",
          "2022-Q4 RF",
          "2023-Q1 RF",
          "2023-Q2 RF",
          "2023-Q3 RF",
          "2022-Q2 RJF",
          "2022-Q3 RJF",
          "2022-Q4 RJF",
          "2023-Q1 RJF",
          "2023-Q2 RJF",
          "2023-Q3 RJF",
          "2022-Q2 SCHW",
          "2022-Q3 SCHW",
          "2022-Q4 SCHW",
          "2023-Q1 SCHW",
          "2023-Q2 SCHW",
          "2023-Q3 SCHW",
          "2022-Q2 STT",
          "2022-Q3 STT",
          "2022-Q4 STT",
          "2023-Q1 STT",
          "2023-Q2 STT",
          "2023-Q3 STT",
          "2022-Q2 TROW",
          "2022-Q3 TROW",
          "2022-Q4 TROW",
          "2023-Q1 TROW",
          "2023-Q2 TROW",
          "2023-Q3 TROW",
          "2022-Q2 TRV",
          "2022-Q3 TRV",
          "2022-Q4 TRV",
          "2023-Q1 TRV",
          "2023-Q2 TRV",
          "2023-Q3 TRV",
          "2022-Q2 WFC",
          "2022-Q3 WFC",
          "2022-Q4 WFC",
          "2023-Q1 WFC",
          "2023-Q2 WFC",
          "2023-Q3 WFC",
          "2022-Q2 WRB",
          "2022-Q3 WRB",
          "2022-Q4 WRB",
          "2023-Q1 WRB",
          "2023-Q2 WRB",
          "2023-Q3 WRB",
          "2022-Q2 ZION",
          "2022-Q3 ZION",
          "2022-Q4 ZION",
          "2023-Q1 ZION",
          "2023-Q2 ZION",
          "2023-Q3 ZION"
         ],
         "y": [
          1.06,
          1.03,
          1.02,
          1.12,
          1.13,
          1.16,
          1.03,
          1.7099999999999997,
          1.3,
          2.16,
          3.31,
          1.87,
          1.45,
          2.53,
          1.84,
          1.88,
          1.74,
          2.01,
          2.07,
          2.08,
          2.03,
          0.7500000000000001,
          1.13,
          0.54,
          0.72,
          0.65,
          0.61,
          0.7000000000000001,
          1.03,
          1.06,
          0.99,
          0.94,
          1.01,
          1.07,
          1.01,
          6.66,
          7.519999999999999,
          6.08,
          6.61,
          6.41,
          7.15,
          8.34,
          0.9000000000000002,
          0.76,
          0.049999999999999906,
          0.44,
          0.5699999999999998,
          0.5799999999999998,
          0.72,
          1.05,
          1.06,
          1.54,
          1.11,
          1.13,
          1.2899999999999998,
          1.21,
          1.87,
          1.95,
          1.88,
          2.11,
          1.9399999999999997,
          1.9599999999999997,
          1.85,
          1.74,
          1.45,
          1.77,
          1.62,
          1.7600000000000002,
          1.9,
          1.52,
          3.16,
          2.99,
          1.87,
          2.9,
          3.0700000000000007,
          3.17,
          2.2500000000000004,
          1.9099999999999997,
          2.05,
          2.0299999999999994,
          2.15,
          2.32,
          2.3599999999999994,
          2.25,
          5.980000000000001,
          6.28,
          6.04,
          5.71,
          5.809999999999999,
          4.79,
          4.6899999999999995,
          1.13,
          1.15,
          0.7800000000000001,
          1.39,
          1.33,
          1.5,
          1.43,
          0.8999999999999999,
          0.8500000000000001,
          0.9399999999999997,
          0.9200000000000002,
          0.9399999999999997,
          1.06,
          0.95,
          0.66,
          0.66,
          0.44,
          0.56,
          0.65,
          0.7,
          0.64,
          0.44,
          0.45,
          0.48000000000000004,
          0.4,
          0.4,
          0.48000000000000004,
          0.48000000000000004,
          1.6600000000000001,
          1.7800000000000002,
          1.55,
          1.7800000000000002,
          1.89,
          2.15,
          1.9600000000000004,
          2.04,
          1.69,
          1.6300000000000001,
          2.07,
          2.07,
          2.15,
          2,
          1.07,
          1.02,
          1.21,
          1.39,
          1.27,
          1.42,
          1.32,
          1.1000000000000003,
          0.7799999999999999,
          1.0900000000000003,
          1.52,
          1.18,
          0.77,
          1.1900000000000002,
          1.3,
          1.3499999999999999,
          1.3099999999999998,
          1.55,
          1.54,
          1.68,
          1.67,
          3.26,
          3.53,
          3.76,
          3.35,
          3.61,
          3.4700000000000006,
          3.6,
          0.3933,
          0.3833,
          0.42,
          0.4067,
          0.4067,
          0.42329999999999995,
          0.43,
          1.68,
          1.58,
          1.6700000000000002,
          1.48,
          1.75,
          1.69,
          1.7999999999999996,
          1.35,
          1.67,
          1.11,
          1.4300000000000002,
          1.52,
          1.23,
          1.41,
          1.15,
          1.33,
          1.24,
          1.33,
          1.4199999999999997,
          1.33,
          1.18,
          2.72,
          2.8200000000000003,
          2.75,
          2.6099999999999994,
          2.88,
          2.94,
          2.9700000000000006,
          3.01,
          3.1500000000000004,
          2.4400000000000004,
          3.0000000000000004,
          3.14,
          3.22,
          2.33,
          0.32,
          0.37,
          0.37,
          0.37,
          0.37,
          0.39,
          0.39,
          1.0333,
          1.12,
          1.1933,
          1.2067,
          1.2,
          1.3333,
          1.2599999999999998,
          0.6,
          0.65,
          0.65,
          0.69,
          0.66,
          0.74,
          0.63,
          2.05,
          1.8700000000000003,
          1.68,
          1.24,
          1.4500000000000002,
          1.51,
          1.9799999999999998,
          1.77,
          2.2999999999999994,
          1.41,
          2.09,
          2.03,
          2.1299999999999994,
          2.03,
          1.8100000000000003,
          2.54,
          2.13,
          2.83,
          2.02,
          1.4300000000000002,
          3.3199999999999994,
          1.08,
          1.16,
          1.09,
          1.0300000000000002,
          1.3,
          1.07,
          0.93,
          0.4310999999999999,
          0.49779999999999996,
          0.4178,
          0.44,
          0.5466999999999999,
          0.5666999999999999,
          0.4733,
          0.89,
          1.04,
          1.08,
          1.04,
          0.99,
          1.17,
          0.97
         ]
        },
        {
         "mode": "lines+markers",
         "name": "All Companies Preds: EPS - PENDS",
         "type": "scatter",
         "x": [
          "2022-Q2 AFL",
          "2022-Q3 AFL",
          "2022-Q4 AFL",
          "2023-Q1 AFL",
          "2023-Q2 AFL",
          "2023-Q3 AFL",
          "2022-Q2 AON",
          "2022-Q3 AON",
          "2022-Q4 AON",
          "2023-Q1 AON",
          "2023-Q2 AON",
          "2023-Q3 AON",
          "2022-Q2 AXP",
          "2022-Q3 AXP",
          "2022-Q4 AXP",
          "2023-Q1 AXP",
          "2023-Q2 AXP",
          "2023-Q3 AXP",
          "2022-Q2 BEN",
          "2022-Q3 BEN",
          "2022-Q4 BEN",
          "2023-Q1 BEN",
          "2023-Q2 BEN",
          "2023-Q3 BEN",
          "2022-Q2 BK",
          "2022-Q3 BK",
          "2022-Q4 BK",
          "2023-Q1 BK",
          "2023-Q2 BK",
          "2023-Q3 BK",
          "2022-Q2 BLK",
          "2022-Q3 BLK",
          "2022-Q4 BLK",
          "2023-Q1 BLK",
          "2023-Q2 BLK",
          "2023-Q3 BLK",
          "2022-Q2 BX",
          "2022-Q3 BX",
          "2022-Q4 BX",
          "2023-Q1 BX",
          "2023-Q2 BX",
          "2023-Q3 BX",
          "2022-Q2 CBOE",
          "2022-Q3 CBOE",
          "2022-Q4 CBOE",
          "2023-Q1 CBOE",
          "2023-Q2 CBOE",
          "2023-Q3 CBOE",
          "2022-Q2 CMA",
          "2022-Q3 CMA",
          "2022-Q4 CMA",
          "2023-Q1 CMA",
          "2023-Q2 CMA",
          "2023-Q3 CMA",
          "2022-Q2 CME",
          "2022-Q3 CME",
          "2022-Q4 CME",
          "2023-Q1 CME",
          "2023-Q2 CME",
          "2023-Q3 CME",
          "2022-Q2 COF",
          "2022-Q3 COF",
          "2022-Q4 COF",
          "2023-Q1 COF",
          "2023-Q2 COF",
          "2023-Q3 COF",
          "2022-Q2 DFS",
          "2022-Q3 DFS",
          "2022-Q4 DFS",
          "2023-Q1 DFS",
          "2023-Q2 DFS",
          "2023-Q3 DFS",
          "2022-Q2 GS",
          "2022-Q3 GS",
          "2022-Q4 GS",
          "2023-Q1 GS",
          "2023-Q2 GS",
          "2023-Q3 GS",
          "2022-Q2 HIG",
          "2022-Q3 HIG",
          "2022-Q4 HIG",
          "2023-Q1 HIG",
          "2023-Q2 HIG",
          "2023-Q3 HIG",
          "2022-Q2 ICE",
          "2022-Q3 ICE",
          "2022-Q4 ICE",
          "2023-Q1 ICE",
          "2023-Q2 ICE",
          "2023-Q3 ICE",
          "2022-Q2 IVZ",
          "2022-Q3 IVZ",
          "2022-Q4 IVZ",
          "2023-Q1 IVZ",
          "2023-Q2 IVZ",
          "2023-Q3 IVZ",
          "2022-Q2 KEY",
          "2022-Q3 KEY",
          "2022-Q4 KEY",
          "2023-Q1 KEY",
          "2023-Q2 KEY",
          "2023-Q3 KEY",
          "2022-Q2 MA",
          "2022-Q3 MA",
          "2022-Q4 MA",
          "2023-Q1 MA",
          "2023-Q2 MA",
          "2023-Q3 MA",
          "2022-Q2 MCO",
          "2022-Q3 MCO",
          "2022-Q4 MCO",
          "2023-Q1 MCO",
          "2023-Q2 MCO",
          "2023-Q3 MCO",
          "2022-Q2 MKTX",
          "2022-Q3 MKTX",
          "2022-Q4 MKTX",
          "2023-Q1 MKTX",
          "2023-Q2 MKTX",
          "2023-Q3 MKTX",
          "2022-Q2 MMC",
          "2022-Q3 MMC",
          "2022-Q4 MMC",
          "2023-Q1 MMC",
          "2023-Q2 MMC",
          "2023-Q3 MMC",
          "2022-Q2 MSCI",
          "2022-Q3 MSCI",
          "2022-Q4 MSCI",
          "2023-Q1 MSCI",
          "2023-Q2 MSCI",
          "2023-Q3 MSCI",
          "2022-Q2 MTB",
          "2022-Q3 MTB",
          "2022-Q4 MTB",
          "2023-Q1 MTB",
          "2023-Q2 MTB",
          "2023-Q3 MTB",
          "2022-Q2 NDAQ",
          "2022-Q3 NDAQ",
          "2022-Q4 NDAQ",
          "2023-Q1 NDAQ",
          "2023-Q2 NDAQ",
          "2023-Q3 NDAQ",
          "2022-Q2 NTRS",
          "2022-Q3 NTRS",
          "2022-Q4 NTRS",
          "2023-Q1 NTRS",
          "2023-Q2 NTRS",
          "2023-Q3 NTRS",
          "2022-Q2 PFG",
          "2022-Q3 PFG",
          "2022-Q4 PFG",
          "2023-Q1 PFG",
          "2023-Q2 PFG",
          "2023-Q3 PFG",
          "2022-Q2 PGR",
          "2022-Q3 PGR",
          "2022-Q4 PGR",
          "2023-Q1 PGR",
          "2023-Q2 PGR",
          "2023-Q3 PGR",
          "2022-Q2 PNC",
          "2022-Q3 PNC",
          "2022-Q4 PNC",
          "2023-Q1 PNC",
          "2023-Q2 PNC",
          "2023-Q3 PNC",
          "2022-Q2 PRU",
          "2022-Q3 PRU",
          "2022-Q4 PRU",
          "2023-Q1 PRU",
          "2023-Q2 PRU",
          "2023-Q3 PRU",
          "2022-Q2 RF",
          "2022-Q3 RF",
          "2022-Q4 RF",
          "2023-Q1 RF",
          "2023-Q2 RF",
          "2023-Q3 RF",
          "2022-Q2 RJF",
          "2022-Q3 RJF",
          "2022-Q4 RJF",
          "2023-Q1 RJF",
          "2023-Q2 RJF",
          "2023-Q3 RJF",
          "2022-Q2 SCHW",
          "2022-Q3 SCHW",
          "2022-Q4 SCHW",
          "2023-Q1 SCHW",
          "2023-Q2 SCHW",
          "2023-Q3 SCHW",
          "2022-Q2 STT",
          "2022-Q3 STT",
          "2022-Q4 STT",
          "2023-Q1 STT",
          "2023-Q2 STT",
          "2023-Q3 STT",
          "2022-Q2 TROW",
          "2022-Q3 TROW",
          "2022-Q4 TROW",
          "2023-Q1 TROW",
          "2023-Q2 TROW",
          "2023-Q3 TROW",
          "2022-Q2 TRV",
          "2022-Q3 TRV",
          "2022-Q4 TRV",
          "2023-Q1 TRV",
          "2023-Q2 TRV",
          "2023-Q3 TRV",
          "2022-Q2 WFC",
          "2022-Q3 WFC",
          "2022-Q4 WFC",
          "2023-Q1 WFC",
          "2023-Q2 WFC",
          "2023-Q3 WFC",
          "2022-Q2 WRB",
          "2022-Q3 WRB",
          "2022-Q4 WRB",
          "2023-Q1 WRB",
          "2023-Q2 WRB",
          "2023-Q3 WRB",
          "2022-Q2 ZION",
          "2022-Q3 ZION",
          "2022-Q4 ZION",
          "2023-Q1 ZION",
          "2023-Q2 ZION",
          "2023-Q3 ZION"
         ],
         "y": [
          0.9738977551460266,
          1.0173242092132568,
          1.0312588214874268,
          1.0365374088287354,
          1.0698339939117432,
          1.0842034816741943,
          1.1032917499542236,
          2.838895559310913,
          2.6743381023406982,
          2.467031240463257,
          2.6160480976104736,
          3.1242384910583496,
          2.9500489234924316,
          2.7794694900512695,
          1.9098210334777832,
          2.004549980163574,
          2.0922653675079346,
          2.0788521766662598,
          2.1408534049987793,
          2.2113544940948486,
          2.268066167831421,
          0.8778290748596191,
          0.8556375503540039,
          0.9069753885269165,
          0.800376296043396,
          0.8120660185813904,
          0.8021597862243652,
          0.784909725189209,
          1.1160719394683838,
          1.1285496950149536,
          1.1464555263519287,
          1.1292129755020142,
          1.1096936464309692,
          1.1108243465423584,
          1.1405922174453735,
          6.752711772918701,
          6.87222146987915,
          7.229282379150391,
          6.940583229064941,
          6.970324993133545,
          6.949204444885254,
          7.220614433288574,
          1.1630949974060059,
          1.2674871683120728,
          1.292365312576294,
          1.0822116136550903,
          1.1431419849395752,
          1.1816697120666504,
          1.2656198740005493,
          1.2822356224060059,
          1.2511565685272217,
          1.2338171005249023,
          1.3256568908691406,
          1.2620675563812256,
          1.262071967124939,
          1.32711660861969,
          1.6422603130340576,
          1.802256464958191,
          1.939943552017212,
          1.9945846796035767,
          2.0715301036834717,
          2.0411410331726074,
          2.038207769393921,
          1.7525190114974976,
          1.840474247932434,
          1.841027021408081,
          1.950000524520874,
          1.9539813995361328,
          2.0362331867218018,
          2.1648006439208984,
          2.6666831970214844,
          2.942861557006836,
          3.018035650253296,
          2.781852960586548,
          2.897148609161377,
          2.947766065597534,
          3.0969643592834473,
          1.7807178497314453,
          1.8831942081451416,
          1.9983826875686646,
          2.001164674758911,
          2.0395007133483887,
          2.1241753101348877,
          2.22088623046875,
          7.393180847167969,
          7.744688034057617,
          7.787927150726318,
          7.483117580413818,
          7.50839900970459,
          7.586400985717773,
          7.77067232131958,
          1.1590343713760376,
          1.2239913940429688,
          1.2937759160995483,
          1.2227157354354858,
          1.3184326887130737,
          1.3632839918136597,
          1.4623041152954102,
          0.9399025440216064,
          0.9728337526321411,
          0.9809300899505615,
          0.9995800852775574,
          1.0021910667419434,
          1.0172697305679321,
          1.0689187049865723,
          0.7335558533668518,
          0.7325847744941711,
          0.7277575731277466,
          0.6458012461662292,
          0.6462075710296631,
          0.6801769733428955,
          0.7408370971679688,
          0.38178321719169617,
          0.4069625437259674,
          0.42767688632011414,
          0.44070717692375183,
          0.4323711395263672,
          0.42460671067237854,
          0.44477325677871704,
          1.5892170667648315,
          1.7190382480621338,
          1.8224188089370728,
          1.7845304012298584,
          1.8800650835037231,
          1.9621684551239014,
          2.0996956825256348,
          2.129253625869751,
          2.257416248321533,
          2.215501070022583,
          2.114211082458496,
          2.2426626682281494,
          2.323669195175171,
          2.4504990577697754,
          1.2989411354064941,
          1.3325868844985962,
          1.34000563621521,
          1.3796621561050415,
          1.4765714406967163,
          1.5284887552261353,
          1.6367661952972412,
          1.3911226987838745,
          1.4086222648620605,
          1.3312357664108276,
          1.3436330556869507,
          1.5244978666305542,
          1.5210272073745728,
          1.4282968044281006,
          1.376006841659546,
          1.4608885049819946,
          1.5211045742034912,
          1.526761770248413,
          1.6195091009140015,
          1.6647645235061646,
          1.75686776638031,
          3.001237630844116,
          3.2739429473876953,
          3.7206149101257324,
          3.960696220397949,
          4.096132755279541,
          4.131161212921143,
          4.1409101486206055,
          0.4266764521598816,
          0.43657293915748596,
          0.43900972604751587,
          0.45065316557884216,
          0.4572617709636688,
          0.4612216055393219,
          0.4765368103981018,
          1.527753472328186,
          1.6298798322677612,
          1.6734826564788818,
          1.6881791353225708,
          1.685875654220581,
          1.734728217124939,
          1.7756712436676025,
          1.372817873954773,
          1.4022144079208374,
          1.5066605806350708,
          1.3961628675460815,
          1.4359467029571533,
          1.4749314785003662,
          1.436269760131836,
          1.2163681983947754,
          1.273228645324707,
          1.3640676736831665,
          1.3425068855285645,
          1.4488110542297363,
          1.4983363151550293,
          1.4895778894424438,
          2.434027671813965,
          2.5786821842193604,
          2.7082412242889404,
          2.7246222496032715,
          2.7063369750976562,
          2.757193088531494,
          2.853847026824951,
          3.088024139404297,
          3.1585004329681396,
          3.248568058013916,
          3.0767016410827637,
          3.1537163257598877,
          3.2000372409820557,
          3.2909605503082275,
          0.37632057070732117,
          0.3828957974910736,
          0.40598025918006897,
          0.40547847747802734,
          0.4126538932323456,
          0.4156814217567444,
          0.4310624599456787,
          1.1600555181503296,
          1.1940103769302368,
          1.2393157482147217,
          1.2552884817123413,
          1.2924728393554688,
          1.321423888206482,
          1.3749428987503052,
          0.5797998905181885,
          0.6253189444541931,
          0.6733246445655823,
          0.6897673010826111,
          0.7287755012512207,
          0.7364328503608704,
          0.7700888514518738,
          1.8142644166946411,
          1.9594265222549438,
          1.9888250827789307,
          1.9211095571517944,
          1.7897716760635376,
          1.7523913383483887,
          1.79537832736969,
          1.83037269115448,
          1.8881890773773193,
          2.032318592071533,
          1.7992233037948608,
          1.9370062351226807,
          1.9895496368408203,
          2.0440540313720703,
          2.8851752281188965,
          2.94031023979187,
          3.182532787322998,
          3.171868085861206,
          3.3895976543426514,
          3.3265860080718994,
          3.244727611541748,
          1.0733146667480469,
          1.075865626335144,
          1.1028327941894531,
          1.0680501461029053,
          1.044265866279602,
          1.0996047258377075,
          1.0758177042007446,
          0.4252350628376007,
          0.44854578375816345,
          0.48525625467300415,
          0.4774041473865509,
          0.4886886477470398,
          0.5283116698265076,
          0.5561450123786926,
          1.093362808227539,
          1.1198011636734009,
          1.1886417865753174,
          1.2068452835083008,
          1.207595944404602,
          1.2009918689727783,
          1.2643803358078003
         ]
        }
       ],
       "layout": {
        "height": 700,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Multiple Companies: EPS - PENDS"
        },
        "width": 1400,
        "xaxis": {
         "title": {
          "text": "Date"
         }
        },
        "yaxis": {
         "rangemode": "tozero",
         "side": "left",
         "title": {
          "text": "EPS"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stock_symbol = \"Multiple Companies\"\n",
    "\n",
    "# Initialize the figure with layout\n",
    "layout = go.Layout(\n",
    "    title=f\"{stock_symbol}: EPS - PENDS\",\n",
    "    xaxis=dict(title='Date'),\n",
    "    yaxis=dict(title='EPS', side='left', rangemode='tozero'),\n",
    "    height=700,\n",
    "    width=1400\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "\n",
    "# Variables to keep track of the extended x-axis and y-axis data\n",
    "extended_x_indices = []\n",
    "all_true_y = []\n",
    "all_preds_y = []\n",
    "\n",
    "# Adjust x_indices and concatenate y data for each company\n",
    "for i, company in enumerate(company_list):\n",
    "    # Extend x_indices with company identifier to differentiate time points between companies\n",
    "    extended_x_indices += [f\"{x} {company}\" for x in x_indices[i]]\n",
    "    all_true_y += list(comp_true_inverse[i].flatten())\n",
    "    all_preds_y += list(comp_preds_inverse[i].flatten())\n",
    "\n",
    "# Add traces for the concatenated true and predicted EPS values\n",
    "trace_true = go.Scatter(x=extended_x_indices, y=all_true_y, mode=\"lines+markers\", name=\"All Companies True: EPS - PENDS\")\n",
    "trace_preds = go.Scatter(x=extended_x_indices, y=all_preds_y, mode=\"lines+markers\", name=\"All Companies Preds: EPS - PENDS\")\n",
    "\n",
    "# Add traces to the figure\n",
    "fig.add_trace(trace_true)\n",
    "fig.add_trace(trace_preds)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>OFTIC</th>\n",
       "      <th>PENDS</th>\n",
       "      <th>MEAN</th>\n",
       "      <th>STDEV</th>\n",
       "      <th>BPS</th>\n",
       "      <th>CPS</th>\n",
       "      <th>CPX</th>\n",
       "      <th>CSH</th>\n",
       "      <th>DPS</th>\n",
       "      <th>...</th>\n",
       "      <th>commodity_trade_Close_quarterly_return</th>\n",
       "      <th>C_Discretionary_Close_quarterly_return</th>\n",
       "      <th>C_Staples_Close_quarterly_return</th>\n",
       "      <th>Energy_Close_quarterly_return</th>\n",
       "      <th>Financials_Close_quarterly_return</th>\n",
       "      <th>Health_care_Close_quarterly_return</th>\n",
       "      <th>industrials_Close_quarterly_return</th>\n",
       "      <th>information_Close_quarterly_return</th>\n",
       "      <th>materials_Close_quarterly_return</th>\n",
       "      <th>utilities_Close_quarterly_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, OFTIC, PENDS, MEAN, STDEV, BPS, CPS, CPX, CSH, DPS, EBG, EBI, EBS, EBT, ENT, EPS, FFO, GPS, GRM, NAV, NDT, NET, OPR, PRE, ROA, ROE, SAL, Real_Estate_Index_Price, VIX_Close, Gold_Close, Three_Month_Yield, Brent_Close, Hrc_close, commodity_trade_Close, C_Discretionary_Close, C_Staples_Close, Energy_Close, Financials_Close, Health_care_Close, industrials_Close, information_Close, materials_Close, utilities_Close, Sector, numeric_sector, Real_Estate_Index_Price_quarterly_return, VIX_Close_quarterly_return, Gold_Close_quarterly_return, Three_Month_Yield_quarterly_return, Brent_Close_quarterly_return, Hrc_close_quarterly_return, commodity_trade_Close_quarterly_return, C_Discretionary_Close_quarterly_return, C_Staples_Close_quarterly_return, Energy_Close_quarterly_return, Financials_Close_quarterly_return, Health_care_Close_quarterly_return, industrials_Close_quarterly_return, information_Close_quarterly_return, materials_Close_quarterly_return, utilities_Close_quarterly_return]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 61 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_data[pivoted_data[\"OFTIC\"]==\"AAP\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
